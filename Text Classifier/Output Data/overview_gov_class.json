{
    "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\n \n\n \n\n \n \n\nOverview \nThis document is the first of four major consulta\ufffdons that Ofcom, as appointed regulator of the \nnew Online Safety Act (\u2018the Act\u2019), will publish as part of our work to establish the new regula\ufffdons \nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \ncontent (\u2018user-to-user\u2019 or \u2018U2U\u2019 services) and search services should approach their new du\ufffdes \nrela\ufffdng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \nservices should assess and mi\ufffdgate the risks of illegal harms; how services can iden\ufffdfy illegal \ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \nas informa\ufffdon and evidence gathered through extensive engagement with industry and other \nexperts. Causes and impacts of illegal harms \nWe are consul\ufffdng on our assessment of how the priority illegal harms covered by the Act manifest \nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \nanalysis underlines the need for services to take ac\ufffdon to combat online harms. It shows that a large \npropor\ufffdon of the UK popula\ufffdon has experienced harm online and that the impact of online harms \ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \nwith protected characteris\ufffdcs are especially likely to be exposed to harm online. Assessing risk \nWe are consul\ufffdng on the guidance we propose to give about how regulated stakeholders should \nassess the risk of illegal harm taking place on their services (the \u2018Risk Assessment Guidance\u2019), as well \nas on our proposals about the governance services should put in place to manage risks. This \ndocument also consults on guidance about how services should keep adequate records of their risk \nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \npropose that providers should follow the four-step process set out in Figure 1 below when assessing \nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \nprocedures in place to mi\ufffdgate the risks associated with illegal content. For example, we are \nproposing that senior governance bodies at large services review the service\u2019s risk management \nac\ufffdvi\ufffdes related to online safety at least annually and that all services iden\ufffdfy a named senior \nexecu\ufffdve who is accountable for compliance with the online safety du\ufffdes. 2 \n \n\nFigure 1: Proposed four-step process for illegal content risk assessment \n\n \n\nMitigating risk \nWe are consul\ufffdng on our illegal harms Codes of Prac\ufffdce, laying out recommended measures that \nregulated services can take to mi\ufffdgate the risk of illegal harm. While the Codes are not binding, and \nso services can choose to take a different approach to mee\ufffdng their du\ufffdes, they act as a \u2018safe \nharbour\u2019. This means any service that implements the recommenda\ufffdons in the Codes would be \ndeemed to be compliant with its related safety du\ufffdes. In the codes we propose to recommend that services put in place a series of measures which, taken \ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \ninclude: \n\n\u2022 ensuring content modera\ufffdon teams are appropriately resourced and trained;  \n\u2022 having easy-to-use systems for users to report poten\ufffdally illegal content and make \n\ncomplaints;  \n\u2022 allowing users to block other users or disable comments;  \n\u2022 conduc\ufffdng tests when they update their algorithms that recommend content to users \n\n(\u2018recommender systems\u2019) to assess the risk that the changes would increase the \ndissemina\ufffdon of illegal content; and \n\n\u2022 a series of recommended steps to make their terms and condi\ufffdons clear and accessible. In addi\ufffdon to these cross-cu\ufffdng measures, we propose that relevant services should take a series of \ntargeted steps to combat Child Sexual Exploita\ufffdon and Abuse (CSEA), fraud and terrorism. These \ntargeted steps include: \n\n\u2022 Using a technology called \u2018hash matching\u2019 to detect and remove known CSAM (Child Sexual \nAbuse Material). Consistent with the restric\ufffdons in the Act, this proposal does not apply to \nprivate communica\ufffdons or end-to-end encrypted communica\ufffdons. We are not making any \nproposals that would involve breaking encryp\ufffdon. However, end-to-end encrypted services \nare s\ufffdll subject to all the safety du\ufffdes set out in the Act and will s\ufffdll need to take steps to \nmi\ufffdgate risks of CSAM on their services; \n\n\u2022 Taking steps to make it harder for perpetrators to groom children online. For example, \nconfiguring default se\ufffdngs so that children do not appear in lists sugges\ufffdng other users \nconnect with them; \n \n\n\u2022 Deploying keyword detec\ufffdon systems to help find and remove posts linked to the sale of \nstolen creden\ufffdals.": [],
    "This should help prevent atempts to commit fraud online. We are also \nrecommending large and high-risk services have dedicated fraud repor\ufffdng channels; \n\n\n\n \n\n \n\n3 \n \n\n\u2022 Where services operate account verifica\ufffdon schemes, they should be transparent about the \nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \ndeceived by posts on fake accounts and should help address fraud and foreign interference in \nUK processes such as elec\ufffdons; and \n\n\u2022 Blocking accounts run by banned terrorist organisa\ufffdons. Our analysis suggests that, taken together, these measures will be an effec\ufffdve and propor\ufffdonate \nmeans of tackling the priority illegal harms in scope of the Act.": [],
    "Some of the proposals set out in this \nconsulta\ufffdon would apply to all services. However, we are targe\ufffdng many of the more onerous \nproposals only at services which are large and/or high risk. We provide a full list of all the measures \nwe are proposing and who they would apply to in our \u201cconsulta\ufffdon at a glance\u201d which is published \nalongside this consulta\ufffdon. Our proposals will make a contribu\ufffdon to comba\ufffdng violence against women and girls online, \nincluding by se\ufffdng out how services should assess their risk of coercive and controlling behaviour, \nstalking, harassment and threats, and in\ufffdmate image abuse. Our proposals for cross-cu\ufffdng \nmeasures and some harm specific measures will begin to mi\ufffdgate these risks. However, we recognise \nthat more work is needed in this area, and we will be publishing dra\ufffd guidance on how services can \ncombat violence against women and girls in early 2025. Identifying illegal content \nA new legal requirement of the Act is for all services to swi\ufffdly take down specific illegal content \nwhen they become aware of it. Today we are consul\ufffdng on our Illegal Content Judgements Guidance \n(\u2018ICJG\u2019). This will provide guidance to services on how they can iden\ufffdfy whether a piece of content is \nlikely to be illegal. Enforcement \nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \nto issue fines of up \u00a318m or up to 10% of the service\u2019s qualifying worldwide revenue (whichever is \ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \n \n\nNext steps \nWe welcome comments from stakeholders in response to the proposals set out in this document, \nincluding any further evidence and suppor\ufffdng informa\ufffdon to inform our final decisions.": [],
    "The \nconsulta\ufffdon closes on 23 February 2024, and we invite stakeholders to provide responses by this \ndate. This is also a statutory consulta\ufffdon under the Act, and we will engage with specific statutory \nconsultees through the consulta\ufffdon process. For further details of the process for making Codes and \nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta\ufffdon, we are also publishing a \u201cchapter summaries\u201d \ndocument. It explains what each chapter is about, what proposals we are making, why we are \nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc\ufffdon) provides further informa\ufffdon on how services can use and \nnavigate the document. Annexes 1-4 explain our approach to consulta\ufffdons and how stakeholders \ncan respond to this consulta\ufffdon. Once the consulta\ufffdon closes, we will consider stakeholder responses, review our proposals, and \npublish a statement se\ufffdng out our final decisions in rela\ufffdon to our consulta\ufffdon proposals, including \nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \nhave three months to conduct their risk assessment. The Codes will also be subject to a \nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \npoint the Codes will come into force. This is one of a number of major consulta\ufffdons we will publish as over the next 12-18 months as we \nput the new online safety regula\ufffdons in place. We provide more detail on our roadmap for \nimplemen\ufffdng the Online Safety Act at the following link: Crea\ufffdng a safer life online for people in the \nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\n\n\tOverview\n\tCauses and impacts of illegal harms\n\tAssessing risk\n\tMitigating risk\n\tIdentifying illegal content\n\tEnforcement\n\tNext steps\n\n\n": []
}