{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- What are git remotes, how do they work, how do they edit them, how do I fix mine\n",
    "- How do I push to my main branch of git\n",
    "- Follow up on CAPS question on tokenization section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract raw data\n",
    "### Expected Format: \n",
    "<font color='red'>[</font>{<font color='green'>'text':</font> 'All services should have a dedicated channel for reporting fraud. We see and hear about a lot of fraud on small platforms, especially dating sights and job boards. ',\n",
    "  <font color='green'>'label':</font> 0,\n",
    "  <font color='green'>'summary':</font> 'Every service should have a dedicated reporting channel for fraud, because there is lots of fraud on a range of platforms. '}, <font color='red'>...]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS: create training set, evaluation set and validation set\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "dataset_name = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/Llama_tutorial/data/consultation2.csv\"\n",
    "dataset = []\n",
    "\n",
    "with open(dataset_name, encoding='utf-8-sig') as FID:\n",
    "    csvReader = csv.DictReader(FID, delimiter=\"\\t\")\n",
    "    for key, row in enumerate(csvReader): \n",
    "        # print(key, row)\n",
    "        dataset.append({\n",
    "            \"label\" : row['Topic'].strip().lower().replace('\\n', ''),\n",
    "            \"text\" : row['Full summary of comment'],\n",
    "            \"summary\" : row['One-line summary']\n",
    "        })\n",
    "\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.drop(columns =['summary'], inplace=True)  # don't need this for now\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                                               468\n",
       "automated content moderation (user to user)                                         270\n",
       "governance and accountability                                                       228\n",
       "user reporting and complaints (u2u and search)                                      140\n",
       "content moderation (user to user)                                                   128\n",
       "user access to services (u2u)                                                       111\n",
       "enhanced user control (u2u)                                                         101\n",
       "default settings and user support (u2u)                                              89\n",
       "cumulative assessment                                                                76\n",
       "terms of service and publicly available statements                                   60\n",
       "recommender system testing (u2u)                                                     59\n",
       "content moderation (search)                                                          52\n",
       "service design and user support (search)                                             30\n",
       "automated content moderation (search)                                                25\n",
       "statutory tests                                                                      22\n",
       "                                                                                      6\n",
       "default settings and user support for child users (u2u)                               6\n",
       "n/a                                                                                   5\n",
       "our approach to the codes                                                             4\n",
       "record keeping and review guidance                                                    3\n",
       "default settings and user support (u2u) and user support (u2u)                        2\n",
       "acm automated content moderation (search)                                             2\n",
       "content moderation (user to user) and content moderation (search)                     2\n",
       "automated content moderation (user to user)automated content moderation (search)      1\n",
       "user reporting and complaints                                                         1\n",
       "ror                                                                                   1\n",
       "approach  to codes                                                                    1\n",
       "icjg                                                                                  1\n",
       "online safety enforcement guidance                                                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_count = df_dataset['label'].value_counts()\n",
    "df_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                 468\n",
       "automated content moderation (user to user)           270\n",
       "governance and accountability                         228\n",
       "user reporting and complaints (u2u and search)        140\n",
       "content moderation (user to user)                     128\n",
       "user access to services (u2u)                         111\n",
       "enhanced user control (u2u)                           101\n",
       "default settings and user support (u2u)                89\n",
       "cumulative assessment                                  76\n",
       "terms of service and publicly available statements     60\n",
       "recommender system testing (u2u)                       59\n",
       "content moderation (search)                            52\n",
       "service design and user support (search)               30\n",
       "automated content moderation (search)                  25\n",
       "statutory tests                                        22\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_rows = list(df_dataset_count[df_dataset_count > 10].index)\n",
    "df_dataset = df_dataset[df_dataset['label'].isin(keep_rows)]\n",
    "display(df_dataset['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert labels to binary classifier format\n",
    "- 'governance and accountability' = 1, everything else = 0\n",
    "\n",
    "Note data set heavily imbalanced so accuracy would be a poor metric of evaluating our model (in its current state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1631\n",
       "1     228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add in our classifier labels\n",
    "topic = 'governance and accountability'\n",
    "df_dataset['label'] = np.where(np.array(df_dataset['label']) == topic, 1, 0)\n",
    "df_dataset['label'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>0</td>\n",
       "      <td>Disadvantages of each signal ● Blocking by Dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0</td>\n",
       "      <td>The vast majority of file-storage and file-sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>0</td>\n",
       "      <td>Same answer as Question 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>0</td>\n",
       "      <td>creation of dedicated reporting mechanisms and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>0</td>\n",
       "      <td>5Rights state they are concerned that the draf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1</td>\n",
       "      <td>there may be some potential downsides to this:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>1</td>\n",
       "      <td>Tracking new illegal content – Any service sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>1</td>\n",
       "      <td>Response: To prevent stifling innovation and e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>1</td>\n",
       "      <td>Executive remuneration is often tied to delive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>All services should have to comply with these ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1882      0  Disadvantages of each signal ● Blocking by Dev...\n",
       "677       0  The vast majority of file-storage and file-sha...\n",
       "1571      0                         Same answer as Question 13\n",
       "1329      0  creation of dedicated reporting mechanisms and...\n",
       "1682      0  5Rights state they are concerned that the draf...\n",
       "...     ...                                                ...\n",
       "1257      1  there may be some potential downsides to this:...\n",
       "1268      1  Tracking new illegal content – Any service sho...\n",
       "895       1  Response: To prevent stifling innovation and e...\n",
       "470       1  Executive remuneration is often tied to delive...\n",
       "5         1  All services should have to comply with these ...\n",
       "\n",
       "[456 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>All services should have a dedicated channel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>When prioritising what content to review, rega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>(Disagree). As noted before, \"there should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>0</td>\n",
       "      <td>Risk of misuse and exploitation of verificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>0</td>\n",
       "      <td>It should be noted that Snap, as a U.S. compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>0</td>\n",
       "      <td>It is Snap’s practice to generally apply accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>0</td>\n",
       "      <td>Notwithstanding an appeal being successful and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>0</td>\n",
       "      <td>Snap and other platforms utilise hash matching...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  All services should have a dedicated channel f...\n",
       "7         0  When prioritising what content to review, rega...\n",
       "8         0  (Disagree). As noted before, \"there should be ...\n",
       "12        0  The RSPCA agrees that the most onerous measure...\n",
       "13        0  The RSPCA agrees that the most onerous measure...\n",
       "...     ...                                                ...\n",
       "1878      0  Risk of misuse and exploitation of verificatio...\n",
       "1879      0  It should be noted that Snap, as a U.S. compan...\n",
       "1881      0  It is Snap’s practice to generally apply accou...\n",
       "1884      0  Notwithstanding an appeal being successful and...\n",
       "1885      0  Snap and other platforms utilise hash matching...\n",
       "\n",
       "[1403 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test data\n",
    "class_len = len(df_dataset[df_dataset['label'] == 1])  # find how many values we can take and still have a balanced class\n",
    "class_0_data = df_dataset[df_dataset.label.eq(0)].sample(class_len) \n",
    "class_1_data = df_dataset[df_dataset.label.eq(1)].sample(class_len)\n",
    "train_test_data = pd.concat([class_0_data, class_1_data])  # 50/50 class split\n",
    "display(train_test_data)\n",
    "\n",
    "# evaluation data\n",
    "eval_data = df_dataset.drop(train_test_data.index)  # put the rest into an evaluation set we can play with\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data to usable version for huggingface model\n",
    "\n",
    "### Using distilbert base uncased model\n",
    "- BERT = is an LLM (large language model)\n",
    "- distil = smaller model with most of the same power as a normal bert model\n",
    "- base = not much tweaking or tuning done yet\n",
    "- uncased = captilised letters make no difference i.e. england = EnglaND (Note: probably want to still consider this in case we change to an cased model)\n",
    "\n",
    "### Tokenizing our data using our base model\n",
    "- Tokenizing is breaking our data up into smaller parts for our model to read (note that tokens are always just words but can be special chars or subwords)\n",
    "- Embedding (not used for this model) take individual token and turning it into a more computer friendly format through transforming it into a hidden multi-dimensional-numeric format e.g. cat -> (12, 45))\n",
    "- Work from huggingface dataset \n",
    "- Only want to tokenize our text\n",
    "- Pad using special characters (adds length up to the min tensor value) the values if necessary\n",
    "- Truncate (shortens to the max tensor value) the values if necessary (up to max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456/456 [00:00<00:00, 1941.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 387\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 69\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "huggingface_data = Dataset.from_pandas(train_test_data, preserve_index=False)  # don't include pandas index\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\"  # This is our base model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def proc_data(data):\n",
    "    return tokenizer(data['text'], max_length=512, padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = huggingface_data.map(proc_data, batched=True)  # advantage of \".map\" is we can parallel process data in batches (i think)\n",
    "print(tokenized_data['text'] == huggingface_data['text'])  # SHOULDN'T THIS NOT BE TRUE??? ESPECIALLY IF I CHANGE MAX_LENGTH TO BE 1 OR SOMETHING\n",
    "\n",
    "split_tokenized_hugginface_data = tokenized_data.train_test_split(test_size=0.15)  # 85/15 train/test split\n",
    "\n",
    "train, test = split_tokenized_hugginface_data\n",
    "print(split_tokenized_hugginface_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "- 1 = \"Positive\" -> the model is predicting that the input CAN be caterogirsed by our input topic\n",
    "- 0 = \"Negative\" -> the model is predicting that the input can NOT be caterogirsed by our input topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:37:58.567870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 11:38:00.530128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "evaluation_metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
