{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- What are git remotes, how do they work, how do they edit them, how do I fix mine\n",
    "- How do I push to my main branch of git\n",
    "- Follow up on CAPS question on tokenization section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract raw data\n",
    "### Expected Format: \n",
    "<font color='red'>[</font>{<font color='green'>'text':</font> 'All services should have a dedicated channel for reporting fraud. We see and hear about a lot of fraud on small platforms, especially dating sights and job boards. ',\n",
    "  <font color='green'>'label':</font> 0,\n",
    "  <font color='green'>'summary':</font> 'Every service should have a dedicated reporting channel for fraud, because there is lots of fraud on a range of platforms. '}, <font color='red'>...]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user reporting and complaints (u2u and search)</td>\n",
       "      <td>All services should have a dedicated channel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>This mitigation should apply to all services, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Evidence of new kinds of illegal content on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>A Code of Conduct or principles provided to al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Staff, in particular engineers, involved in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Snap asked that Ofcom consider how this measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>ror</td>\n",
       "      <td>Supports Ofcoms wide use of litrature to regog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>approach  to codes</td>\n",
       "      <td>They support the use of the STIM but alongside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>icjg</td>\n",
       "      <td>ASW should be expected to take more of a proac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>online safety enforcement guidance</td>\n",
       "      <td>The nature of the ASW sector means that privac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0     user reporting and complaints (u2u and search)   \n",
       "1                      governance and accountability   \n",
       "2                      governance and accountability   \n",
       "3                      governance and accountability   \n",
       "4                      governance and accountability   \n",
       "...                                              ...   \n",
       "1890                   governance and accountability   \n",
       "1891                                             ror   \n",
       "1892                              approach  to codes   \n",
       "1893                                            icjg   \n",
       "1894              online safety enforcement guidance   \n",
       "\n",
       "                                                   text  \n",
       "0     All services should have a dedicated channel f...  \n",
       "1     This mitigation should apply to all services, ...  \n",
       "2     Evidence of new kinds of illegal content on a ...  \n",
       "3     A Code of Conduct or principles provided to al...  \n",
       "4     Staff, in particular engineers, involved in th...  \n",
       "...                                                 ...  \n",
       "1890  Snap asked that Ofcom consider how this measur...  \n",
       "1891  Supports Ofcoms wide use of litrature to regog...  \n",
       "1892  They support the use of the STIM but alongside...  \n",
       "1893  ASW should be expected to take more of a proac...  \n",
       "1894  The nature of the ASW sector means that privac...  \n",
       "\n",
       "[1895 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MS: create training set, evaluation set and validation set\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "dataset_name = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/Llama_tutorial/data/consultation2.csv\"\n",
    "dataset = []\n",
    "\n",
    "with open(dataset_name, encoding='utf-8-sig') as FID:\n",
    "    csvReader = csv.DictReader(FID, delimiter=\"\\t\")\n",
    "    for key, row in enumerate(csvReader): \n",
    "        # print(key, row)\n",
    "        dataset.append({\n",
    "            \"label\" : row['Topic'].strip().lower().replace('\\n', ''),\n",
    "            \"text\" : row['Full summary of comment'],\n",
    "            \"summary\" : row['One-line summary']\n",
    "        })\n",
    "\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.drop(columns =['summary'], inplace=True)  # don't need this for now\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                                               468\n",
       "automated content moderation (user to user)                                         270\n",
       "governance and accountability                                                       228\n",
       "user reporting and complaints (u2u and search)                                      140\n",
       "content moderation (user to user)                                                   128\n",
       "user access to services (u2u)                                                       111\n",
       "enhanced user control (u2u)                                                         101\n",
       "default settings and user support (u2u)                                              89\n",
       "cumulative assessment                                                                76\n",
       "terms of service and publicly available statements                                   60\n",
       "recommender system testing (u2u)                                                     59\n",
       "content moderation (search)                                                          52\n",
       "service design and user support (search)                                             30\n",
       "automated content moderation (search)                                                25\n",
       "statutory tests                                                                      22\n",
       "                                                                                      6\n",
       "default settings and user support for child users (u2u)                               6\n",
       "n/a                                                                                   5\n",
       "our approach to the codes                                                             4\n",
       "record keeping and review guidance                                                    3\n",
       "default settings and user support (u2u) and user support (u2u)                        2\n",
       "acm automated content moderation (search)                                             2\n",
       "content moderation (user to user) and content moderation (search)                     2\n",
       "automated content moderation (user to user)automated content moderation (search)      1\n",
       "user reporting and complaints                                                         1\n",
       "ror                                                                                   1\n",
       "approach  to codes                                                                    1\n",
       "icjg                                                                                  1\n",
       "online safety enforcement guidance                                                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_count = df_dataset['label'].value_counts()\n",
    "df_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                 468\n",
       "automated content moderation (user to user)           270\n",
       "governance and accountability                         228\n",
       "user reporting and complaints (u2u and search)        140\n",
       "content moderation (user to user)                     128\n",
       "user access to services (u2u)                         111\n",
       "enhanced user control (u2u)                           101\n",
       "default settings and user support (u2u)                89\n",
       "cumulative assessment                                  76\n",
       "terms of service and publicly available statements     60\n",
       "recommender system testing (u2u)                       59\n",
       "content moderation (search)                            52\n",
       "service design and user support (search)               30\n",
       "automated content moderation (search)                  25\n",
       "statutory tests                                        22\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_rows = list(df_dataset_count[df_dataset_count > 10].index)\n",
    "df_dataset = df_dataset[df_dataset['label'].isin(keep_rows)]\n",
    "display(df_dataset['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert labels to binary classifier format\n",
    "- 'governance and accountability' = 1, everything else = 0\n",
    "\n",
    "Note data set heavily imbalanced so accuracy would be a poor metric of evaluating our model (in its current state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1631\n",
       "1     228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add in our classifier labels\n",
    "topic = 'governance and accountability'\n",
    "df_dataset['label'] = np.where(np.array(df_dataset['label']) == topic, 1, 0)\n",
    "df_dataset['label'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0</td>\n",
       "      <td>Spotify's response says that a large service s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>0</td>\n",
       "      <td>Evidence suggests that those aged 15-17 dislik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>0</td>\n",
       "      <td>5Rights argue that services should undertake r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>We are concerned that Ofcom’s definition of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>0</td>\n",
       "      <td>In 2022, eBay removed 773,000 items based on r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1</td>\n",
       "      <td>Spotify says it wants Ofcom to be proportionat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>1</td>\n",
       "      <td>We also believe there could be significant cos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>1</td>\n",
       "      <td>MSPG urge Ofcom to not mandate use of external...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1</td>\n",
       "      <td>CCDH is aware of the societal impacts caused ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>1</td>\n",
       "      <td>(a) internal audit functions frequently act in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1537      0  Spotify's response says that a large service s...\n",
       "1431      0  Evidence suggests that those aged 15-17 dislik...\n",
       "1705      0  5Rights argue that services should undertake r...\n",
       "332       0  We are concerned that Ofcom’s definition of a ...\n",
       "1171      0  In 2022, eBay removed 773,000 items based on r...\n",
       "...     ...                                                ...\n",
       "1531      1  Spotify says it wants Ofcom to be proportionat...\n",
       "779       1  We also believe there could be significant cos...\n",
       "920       1  MSPG urge Ofcom to not mandate use of external...\n",
       "983       1   CCDH is aware of the societal impacts caused ...\n",
       "1052      1  (a) internal audit functions frequently act in...\n",
       "\n",
       "[456 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>All services should have a dedicated channel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>When prioritising what content to review, rega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>(Disagree). As noted before, \"there should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>0</td>\n",
       "      <td>It is Snap’s practice to generally apply accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>0</td>\n",
       "      <td>Disadvantages of each signal ● Blocking by Dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>0</td>\n",
       "      <td>We would stress that this issue is not easily,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>0</td>\n",
       "      <td>Notwithstanding an appeal being successful and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>0</td>\n",
       "      <td>Snap and other platforms utilise hash matching...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  All services should have a dedicated channel f...\n",
       "7         0  When prioritising what content to review, rega...\n",
       "8         0  (Disagree). As noted before, \"there should be ...\n",
       "12        0  The RSPCA agrees that the most onerous measure...\n",
       "13        0  The RSPCA agrees that the most onerous measure...\n",
       "...     ...                                                ...\n",
       "1881      0  It is Snap’s practice to generally apply accou...\n",
       "1882      0  Disadvantages of each signal ● Blocking by Dev...\n",
       "1883      0  We would stress that this issue is not easily,...\n",
       "1884      0  Notwithstanding an appeal being successful and...\n",
       "1885      0  Snap and other platforms utilise hash matching...\n",
       "\n",
       "[1403 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test data\n",
    "class_len = len(df_dataset[df_dataset['label'] == 1])  # find how many values we can take and still have a balanced class\n",
    "class_0_data = df_dataset[df_dataset.label.eq(0)].sample(class_len) \n",
    "class_1_data = df_dataset[df_dataset.label.eq(1)].sample(class_len)\n",
    "train_test_data = pd.concat([class_0_data, class_1_data])  # 50/50 class split\n",
    "display(train_test_data)\n",
    "\n",
    "# evaluation data\n",
    "eval_data = df_dataset.drop(train_test_data.index)  # put the rest into an evaluation set we can play with\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data to usable version for huggingface model\n",
    "\n",
    "### Using distilbert base uncased model\n",
    "- BERT = is an LLM (large language model)\n",
    "- distil = smaller model with most of the same power as a normal bert model\n",
    "- base = not much tweaking or tuning done yet\n",
    "- uncased = captilised letters make no difference i.e. england = EnglaND (Note: probably want to still consider this in case we change to an cased model)\n",
    "\n",
    "### Tokenizing our data using our base model\n",
    "- Tokenizing is breaking our data up into smaller parts for our model to read (note that tokens are always just words but can be special chars or subwords)\n",
    "- Embedding (not used for this model) take individual token and turning it into a more computer friendly format through transforming it into a hidden multi-dimensional-numeric format e.g. cat -> (12, 45))\n",
    "- Work from huggingface dataset \n",
    "- Only want to tokenize our text\n",
    "- Pad using special characters (adds length up to the min tensor value) the values if necessary\n",
    "- Truncate (shortens to the max tensor value) the values if necessary (up to max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456/456 [00:00<00:00, 3958.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 410\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "huggingface_data = Dataset.from_pandas(train_test_data, preserve_index=False)  # don't include pandas index\n",
    "\n",
    "pretrained_model_name = \"distilbert/distilbert-base-uncased\"  # This is our base model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "def proc_data(data):\n",
    "    return tokenizer(data['text'], max_length=512, padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = huggingface_data.map(proc_data, batched=True)  # advantage of \".map\" is we can parallel process data in batches (i think)\n",
    "# print(tokenized_data['text'] == huggingface_data['text'])  # SHOULDN'T THIS NOT BE TRUE??? ESPECIALLY IF I CHANGE MAX_LENGTH TO BE 1 OR SOMETHING\n",
    "\n",
    "split_tokenized_hugginface_data = tokenized_data.train_test_split(test_size=0.10)  # 85/15 train/test split\n",
    "print(split_tokenized_hugginface_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "- 1 = \"Positive\" -> the model is predicting that the input CAN be caterogirsed by our input topic\n",
    "- 0 = \"Negative\" -> the model is predicting that the input can NOT be caterogirsed by our input topic\n",
    "\n",
    "Instead of doing evaluate.load(metric), can load several metrics using accuracy.combine([metric1, metric2, ...])\n",
    "\n",
    "Note:\n",
    "- Just reducing test set size from 15% to 10% had about 10% improvement on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/anaconda/envs/llm_parser/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 06:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.612856</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.437802</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.437469</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.475368</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.611315</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.699294</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.765226</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.816931</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.849058</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.868352</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.886609</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.911840</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.903606</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.900685</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.930874</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.939406</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.939437</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.944128</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.944569</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-26 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-52 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-78 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-104 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-130 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-156 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-182 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-208 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-234 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-260 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-286 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-312 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-338 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-364 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-390 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-416 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-442 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-468 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-494 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/checkpoint-520 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=520, training_loss=0.11145450839629541, metrics={'train_runtime': 405.7465, 'train_samples_per_second': 20.21, 'train_steps_per_second': 1.282, 'total_flos': 1086232668979200.0, 'train_loss': 0.11145450839629541, 'epoch': 20.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# Select accuracy metric\n",
    "evaluation_metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "accuracy = evaluate.combine(evaluation_metrics)# evaluate.load(\"accuracy\")\n",
    "\n",
    "# Use accuracy to determine which class is the most likely prediction\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # get predictions and labels\n",
    "    predictions = np.argmax(predictions, axis=1)  # calculate highest probability class (negative or positive sentiment)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,\n",
    "                                                            id2label=id2label,\n",
    "                                                            label2id=label2id)\n",
    "\n",
    "# How we input training arguments into the model\n",
    "model_output_path = \"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_path,  # path model stored\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=100\n",
    "    )\n",
    "\n",
    "\n",
    "# Model training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_tokenized_hugginface_data[\"train\"],\n",
    "    eval_dataset=split_tokenized_hugginface_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Stop training if no improvement after 3 consecutive epochs\n",
    "\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
