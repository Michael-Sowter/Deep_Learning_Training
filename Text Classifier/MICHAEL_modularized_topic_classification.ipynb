{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- What are git remotes, how do they work, how do they edit them, how do I fix mine\n",
    "- How do I push to my main branch of git\n",
    "- Follow up on CAPS question on tokenization section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract raw data\n",
    "### Expected Format: \n",
    "<font color='red'>[</font>{<font color='green'>'text':</font> 'All services should have a dedicated channel for reporting fraud. We see and hear about a lot of fraud on small platforms, especially dating sights and job boards. ',\n",
    "  <font color='green'>'label':</font> 0,\n",
    "  <font color='green'>'summary':</font> 'Every service should have a dedicated reporting channel for fraud, because there is lots of fraud on a range of platforms. '}, <font color='red'>...]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user reporting and complaints (u2u and search)</td>\n",
       "      <td>All services should have a dedicated channel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>This mitigation should apply to all services, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Evidence of new kinds of illegal content on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>A Code of Conduct or principles provided to al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Staff, in particular engineers, involved in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>governance and accountability</td>\n",
       "      <td>Snap asked that Ofcom consider how this measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>ror</td>\n",
       "      <td>Supports Ofcoms wide use of litrature to regog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>approach  to codes</td>\n",
       "      <td>They support the use of the STIM but alongside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>icjg</td>\n",
       "      <td>ASW should be expected to take more of a proac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>online safety enforcement guidance</td>\n",
       "      <td>The nature of the ASW sector means that privac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0     user reporting and complaints (u2u and search)   \n",
       "1                      governance and accountability   \n",
       "2                      governance and accountability   \n",
       "3                      governance and accountability   \n",
       "4                      governance and accountability   \n",
       "...                                              ...   \n",
       "1890                   governance and accountability   \n",
       "1891                                             ror   \n",
       "1892                              approach  to codes   \n",
       "1893                                            icjg   \n",
       "1894              online safety enforcement guidance   \n",
       "\n",
       "                                                   text  \n",
       "0     All services should have a dedicated channel f...  \n",
       "1     This mitigation should apply to all services, ...  \n",
       "2     Evidence of new kinds of illegal content on a ...  \n",
       "3     A Code of Conduct or principles provided to al...  \n",
       "4     Staff, in particular engineers, involved in th...  \n",
       "...                                                 ...  \n",
       "1890  Snap asked that Ofcom consider how this measur...  \n",
       "1891  Supports Ofcoms wide use of litrature to regog...  \n",
       "1892  They support the use of the STIM but alongside...  \n",
       "1893  ASW should be expected to take more of a proac...  \n",
       "1894  The nature of the ASW sector means that privac...  \n",
       "\n",
       "[1895 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MS: create training set, evaluation set and validation set\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "dataset_name = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/Llama_tutorial/data/consultation2.csv\"\n",
    "dataset = []\n",
    "\n",
    "with open(dataset_name, encoding='utf-8-sig') as FID:\n",
    "    csvReader = csv.DictReader(FID, delimiter=\"\\t\")\n",
    "    for key, row in enumerate(csvReader): \n",
    "        # print(key, row)\n",
    "        dataset.append({\n",
    "            \"label\" : row['Topic'].strip().lower().replace('\\n', ''),\n",
    "            \"text\" : row['Full summary of comment'],\n",
    "            \"summary\" : row['One-line summary']\n",
    "        })\n",
    "\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.drop(columns =['summary'], inplace=True)  # don't need this for now\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                                               468\n",
       "automated content moderation (user to user)                                         270\n",
       "governance and accountability                                                       228\n",
       "user reporting and complaints (u2u and search)                                      140\n",
       "content moderation (user to user)                                                   128\n",
       "user access to services (u2u)                                                       111\n",
       "enhanced user control (u2u)                                                         101\n",
       "default settings and user support (u2u)                                              89\n",
       "cumulative assessment                                                                76\n",
       "terms of service and publicly available statements                                   60\n",
       "recommender system testing (u2u)                                                     59\n",
       "content moderation (search)                                                          52\n",
       "service design and user support (search)                                             30\n",
       "automated content moderation (search)                                                25\n",
       "statutory tests                                                                      22\n",
       "default settings and user support for child users (u2u)                               6\n",
       "                                                                                      6\n",
       "n/a                                                                                   5\n",
       "our approach to the codes                                                             4\n",
       "record keeping and review guidance                                                    3\n",
       "default settings and user support (u2u) and user support (u2u)                        2\n",
       "acm automated content moderation (search)                                             2\n",
       "content moderation (user to user) and content moderation (search)                     2\n",
       "automated content moderation (user to user)automated content moderation (search)      1\n",
       "user reporting and complaints                                                         1\n",
       "ror                                                                                   1\n",
       "approach  to codes                                                                    1\n",
       "icjg                                                                                  1\n",
       "online safety enforcement guidance                                                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_count = df_dataset['label'].value_counts()\n",
    "df_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "approach to the codes                                 468\n",
       "automated content moderation (user to user)           270\n",
       "governance and accountability                         228\n",
       "user reporting and complaints (u2u and search)        140\n",
       "content moderation (user to user)                     128\n",
       "user access to services (u2u)                         111\n",
       "enhanced user control (u2u)                           101\n",
       "default settings and user support (u2u)                89\n",
       "cumulative assessment                                  76\n",
       "terms of service and publicly available statements     60\n",
       "recommender system testing (u2u)                       59\n",
       "content moderation (search)                            52\n",
       "service design and user support (search)               30\n",
       "automated content moderation (search)                  25\n",
       "statutory tests                                        22\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_rows = list(df_dataset_count[df_dataset_count > 10].index)\n",
    "df_dataset = df_dataset[df_dataset['label'].isin(keep_rows)]\n",
    "display(df_dataset['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert labels to binary classifier format\n",
    "- 'governance and accountability' = 1, everything else = 0\n",
    "\n",
    "Note data set heavily imbalanced so accuracy would be a poor metric of evaluating our model (in its current state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1631\n",
       "1     228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add in our classifier labels\n",
    "topic = 'governance and accountability'\n",
    "df_dataset['label'] = np.where(np.array(df_dataset['label']) == topic, 1, 0)\n",
    "df_dataset['label'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>Barnardo's agree with the CSAM hash matching a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>0</td>\n",
       "      <td>Mega state that measures 4D (prioritisation ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0</td>\n",
       "      <td>We also have concerns with some of Ofcom’s oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0</td>\n",
       "      <td>Number of areas where further measures should ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>Google systems work well because do not apply ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Staff, in particular engineers, involved in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1</td>\n",
       "      <td>The codes continual references to \"industry\" a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>1</td>\n",
       "      <td>Spotify says \"We suggest further consideration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>1</td>\n",
       "      <td>Services are required to creat codes of conduc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "154       0  Barnardo's agree with the CSAM hash matching a...\n",
       "1593      0  Mega state that measures 4D (prioritisation ba...\n",
       "676       0  We also have concerns with some of Ofcom’s oth...\n",
       "514       0  Number of areas where further measures should ...\n",
       "284       0  Google systems work well because do not apply ...\n",
       "...     ...                                                ...\n",
       "4         1  Staff, in particular engineers, involved in th...\n",
       "439       1  The codes continual references to \"industry\" a...\n",
       "1532      1  Spotify says \"We suggest further consideration...\n",
       "529       1                                                 No\n",
       "461       1  Services are required to creat codes of conduc...\n",
       "\n",
       "[456 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>All services should have a dedicated channel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>When prioritising what content to review, rega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>(Disagree). As noted before, \"there should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>The RSPCA agrees that the most onerous measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>0</td>\n",
       "      <td>It should be noted that Snap, as a U.S. compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>0</td>\n",
       "      <td>We apply account blocks based on signals inclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>0</td>\n",
       "      <td>It is Snap’s practice to generally apply accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>0</td>\n",
       "      <td>Disadvantages of each signal ● Blocking by Dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>0</td>\n",
       "      <td>We would stress that this issue is not easily,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  All services should have a dedicated channel f...\n",
       "7         0  When prioritising what content to review, rega...\n",
       "8         0  (Disagree). As noted before, \"there should be ...\n",
       "12        0  The RSPCA agrees that the most onerous measure...\n",
       "13        0  The RSPCA agrees that the most onerous measure...\n",
       "...     ...                                                ...\n",
       "1879      0  It should be noted that Snap, as a U.S. compan...\n",
       "1880      0  We apply account blocks based on signals inclu...\n",
       "1881      0  It is Snap’s practice to generally apply accou...\n",
       "1882      0  Disadvantages of each signal ● Blocking by Dev...\n",
       "1883      0  We would stress that this issue is not easily,...\n",
       "\n",
       "[1403 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test data\n",
    "class_len = len(df_dataset[df_dataset['label'] == 1])  # find how many values we can take and still have a balanced class\n",
    "class_0_data = df_dataset[df_dataset.label.eq(0)].sample(class_len) \n",
    "class_1_data = df_dataset[df_dataset.label.eq(1)].sample(class_len)\n",
    "train_test_data = pd.concat([class_0_data, class_1_data])  # 50/50 class split\n",
    "display(train_test_data)\n",
    "\n",
    "# evaluation data\n",
    "eval_data = df_dataset.drop(train_test_data.index)  # put the rest into an evaluation set we can play with\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data to usable version for huggingface model\n",
    "\n",
    "### Using distilbert base uncased model\n",
    "- BERT = is an LLM (large language model)\n",
    "- distil = smaller model with most of the same power as a normal bert model\n",
    "- base = not much tweaking or tuning done yet\n",
    "- uncased = captilised letters make no difference i.e. england = EnglaND (Note: probably want to still consider this in case we change to an cased model)\n",
    "\n",
    "### Tokenizing our data using our base model\n",
    "- Tokenizing is breaking our data up into smaller parts for our model to read (note that tokens are always just words but can be special chars or subwords)\n",
    "- Embedding (not used for this model) take individual token and turning it into a more computer friendly format through transforming it into a hidden multi-dimensional-numeric format e.g. cat -> (12, 45))\n",
    "- Work from huggingface dataset \n",
    "- Only want to tokenize our text\n",
    "- Pad using special characters (adds length up to the min tensor value) the values if necessary\n",
    "- Truncate (shortens to the max tensor value) the values if necessary (up to max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/llm_parser/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456/456 [00:00<00:00, 2075.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 410\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "huggingface_data = Dataset.from_pandas(train_test_data, preserve_index=False)  # don't include pandas index\n",
    "\n",
    "pretrained_model_name = \"distilbert/distilbert-base-uncased\"  # This is our base model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "def proc_data(data):\n",
    "    return tokenizer(data['text'], max_length=512, padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = huggingface_data.map(proc_data, batched=True)  # advantage of \".map\" is we can parallel process data in batches (i think)\n",
    "# print(tokenized_data['text'] == huggingface_data['text'])  # SHOULDN'T THIS NOT BE TRUE??? ESPECIALLY IF I CHANGE MAX_LENGTH TO BE 1 OR SOMETHING\n",
    "\n",
    "split_tokenized_hugginface_data = tokenized_data.train_test_split(test_size=0.10)  # 85/15 train/test split\n",
    "print(split_tokenized_hugginface_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "- 1 = \"Positive\" -> the model is predicting that the input CAN be caterogirsed by our input topic\n",
    "- 0 = \"Negative\" -> the model is predicting that the input can NOT be caterogirsed by our input topic\n",
    "\n",
    "Instead of doing evaluate.load(metric), can load several metrics using accuracy.combine([metric1, metric2, ...])\n",
    "\n",
    "Note:\n",
    "- Just reducing test set size from 15% to 10% had about 10% improvement on accuracy\n",
    "- Use <font color='green'>\".select(range(X))\"</font> in place of <font color='green'>\"[:X]\"</font>. Measurable increase in accuracy from just 50 more sample but still decent accuracy and improvements without much training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def clear_folder_except(folder_path, exclude_folder):\n",
    "    # Iterate over the items in the directory\n",
    "    for item_name in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item_name)\n",
    "        # Skip the exclude folder\n",
    "        if item_name == exclude_folder:\n",
    "            continue\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(item_path):\n",
    "            # If it is a directory, remove it and its contents\n",
    "            shutil.rmtree(item_path)\n",
    "        else:\n",
    "            # If it is a file, remove it\n",
    "            os.remove(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -HOT FIX TO STOP MODEL TRAINING BREAKING----------------------------------------------------------------------------------------------------------- #\n",
    "# model_output_path = \"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/Mod_1\"\n",
    "# # clear_folder_except(model_output_path, \"runs\")\n",
    "# # --------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# import evaluate\n",
    "\n",
    "\n",
    "# # Select accuracy metric\n",
    "# evaluation_metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "# accuracy = evaluate.combine(evaluation_metrics)# evaluate.load(\"accuracy\")\n",
    "\n",
    "# # Use accuracy to determine which class is the most likely prediction\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred  # get predictions and labels\n",
    "#     predictions = np.argmax(predictions, axis=1)  # calculate highest probability class (negative or positive sentiment)\n",
    "#     return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "# label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,\n",
    "#                                                             id2label=id2label,\n",
    "#                                                             label2id=label2id)\n",
    "\n",
    "# # How we input training arguments into the model\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=model_output_path,  # path model stored\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=20,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "#     logging_steps= 25 # logs made every X batches. So smaller log means more information recorded (see log in table but also more computational and memory requirements)\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Model training arguments\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=split_tokenized_hugginface_data[\"train\"],#.select(range(200)),\n",
    "#     eval_dataset=split_tokenized_hugginface_data[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Stop training if no improvement after 3 consecutive epochs\n",
    "\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.save_model(\"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/Mod_1/Best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Overview \n",
      "This document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \n",
      "new Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \n",
      "over the next 18 months.  \n",
      "\n",
      "It focuses on our proposals for how internet services which enable the sharing of user generated \n",
      "content (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \n",
      "rela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \n",
      "services should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \n",
      "content; and our approach to enforcement.  \n",
      "\n",
      "The proposals in this document reflect research we have conducted over the past three years as well \n",
      "as informa�on and evidence gathered through extensive engagement with industry and other \n",
      "experts. \n",
      "\n",
      "Causes and impacts of illegal harms \n",
      "We are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \n",
      "online, what factors give rise to a risk of these harms and what the impact of the harms is. This \n",
      "analysis underlines the need for services to take ac�on to combat online harms. It shows that a large \n",
      "propor�on of the UK popula�on has experienced harm online and that the impact of online harms \n",
      "can, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \n",
      "with protected characteris�cs are especially likely to be exposed to harm online. \n",
      "\n",
      "Assessing risk \n",
      "We are consul�ng on the guidance we propose to give about how regulated stakeholders should \n",
      "assess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \n",
      "as on our proposals about the governance services should put in place to manage risks. This \n",
      "document also consults on guidance about how services should keep adequate records of their risk \n",
      "assessments. \n",
      "\n",
      "Services should undertake a robust and comprehensive risk assessment. More specifically, we \n",
      "propose that providers should follow the four-step process set out in Figure 1 below when assessing \n",
      "the risk of illegal content on their services.  \n",
      "\n",
      "We are also proposing that services take several steps to ensure that they have strong governance \n",
      "procedures in place to mi�gate the risks associated with illegal content. For example, we are \n",
      "proposing that senior governance bodies at large services review the service’s risk management \n",
      "ac�vi�es related to online safety at least annually and that all services iden�fy a named senior \n",
      "execu�ve who is accountable for compliance with the online safety du�es.  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2 \n",
      " \n",
      "\n",
      "Figure 1: Proposed four-step process for illegal content risk assessment \n",
      "\n",
      " \n",
      "\n",
      "Mitigating risk \n",
      "We are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \n",
      "regulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \n",
      "so services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \n",
      "harbour’. This means any service that implements the recommenda�ons in the Codes would be \n",
      "deemed to be compliant with its related safety du�es. \n",
      "\n",
      "In the codes we propose to recommend that services put in place a series of measures which, taken \n",
      "together, will help combat all of the priority illegal harms in scope of the Act. These measures \n",
      "include: \n",
      "\n",
      "• ensuring content modera�on teams are appropriately resourced and trained;  \n",
      "• having easy-to-use systems for users to report poten�ally illegal content and make \n",
      "\n",
      "complaints;  \n",
      "• allowing users to block other users or disable comments;  \n",
      "• conduc�ng tests when they update their algorithms that recommend content to users \n",
      "\n",
      "(‘recommender systems’) to assess the risk that the changes would increase the \n",
      "dissemina�on of illegal content; and \n",
      "\n",
      "• a series of recommended steps to make their terms and condi�ons clear and accessible. \n",
      "\n",
      "In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \n",
      "targeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \n",
      "targeted steps include: \n",
      "\n",
      "• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \n",
      "Abuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \n",
      "private communica�ons or end-to-end encrypted communica�ons. We are not making any \n",
      "proposals that would involve breaking encryp�on. However, end-to-end encrypted services \n",
      "are s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \n",
      "mi�gate risks of CSAM on their services; \n",
      "\n",
      "• Taking steps to make it harder for perpetrators to groom children online. For example, \n",
      "configuring default se�ngs so that children do not appear in lists sugges�ng other users \n",
      "connect with them; \n",
      " \n",
      "\n",
      "• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \n",
      "stolen creden�als. This should help prevent atempts to commit fraud online. We are also \n",
      "recommending large and high-risk services have dedicated fraud repor�ng channels; \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3 \n",
      " \n",
      "\n",
      "• Where services operate account verifica�on schemes, they should be transparent about the \n",
      "steps they are taking to verify accounts. This is aimed at reducing the risk of users being \n",
      "deceived by posts on fake accounts and should help address fraud and foreign interference in \n",
      "UK processes such as elec�ons; and \n",
      "\n",
      "• Blocking accounts run by banned terrorist organisa�ons.  \n",
      "\n",
      "Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \n",
      "means of tackling the priority illegal harms in scope of the Act. Some of the proposals set out in this \n",
      "consulta�on would apply to all services. However, we are targe�ng many of the more onerous \n",
      "proposals only at services which are large and/or high risk. We provide a full list of all the measures \n",
      "we are proposing and who they would apply to in our “consulta�on at a glance” which is published \n",
      "alongside this consulta�on.  \n",
      "\n",
      "Our proposals will make a contribu�on to comba�ng violence against women and girls online, \n",
      "including by se�ng out how services should assess their risk of coercive and controlling behaviour, \n",
      "stalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \n",
      "measures and some harm specific measures will begin to mi�gate these risks. However, we recognise \n",
      "that more work is needed in this area, and we will be publishing dra� guidance on how services can \n",
      "combat violence against women and girls in early 2025. \n",
      "\n",
      "Identifying illegal content \n",
      "A new legal requirement of the Act is for all services to swi�ly take down specific illegal content \n",
      "when they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \n",
      "(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \n",
      "likely to be illegal. \n",
      "\n",
      "Enforcement \n",
      "The Act gives us significant enforcement powers in the event of non-compliance, including the ability \n",
      "to issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \n",
      "greater) and to apply for a court order requiring an internet service provider to withdraw access to \n",
      "the service to prevent a significant risk of harm to UK users as a result of its failure.     \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4 \n",
      " \n",
      "\n",
      "Next steps \n",
      "We welcome comments from stakeholders in response to the proposals set out in this document, \n",
      "including any further evidence and suppor�ng informa�on to inform our final decisions. The \n",
      "consulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \n",
      "date. \n",
      "\n",
      "This is also a statutory consulta�on under the Act, and we will engage with specific statutory \n",
      "consultees through the consulta�on process. For further details of the process for making Codes and \n",
      "guidance, please refer to our Legal Framework (Annex 12).  \n",
      "\n",
      "To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \n",
      "document. It explains what each chapter is about, what proposals we are making, why we are \n",
      "making them and what input we would appreciate from stakeholders. \n",
      "\n",
      "Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \n",
      "navigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \n",
      "can respond to this consulta�on.   \n",
      "\n",
      "Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \n",
      "publish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \n",
      "final versions of our guidance and Codes.  \n",
      "\n",
      "We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \n",
      "have three months to conduct their risk assessment. The Codes will also be subject to a \n",
      "parliamentary approval process. We expect this process to conclude by the end of 2024 at which \n",
      "point the Codes will come into force. \n",
      "\n",
      "This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \n",
      "put the new online safety regula�ons in place. We provide more detail on our roadmap for \n",
      "implemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \n",
      "UK.  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\n",
      "https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\n",
      "\n",
      "\tOverview\n",
      "\tCauses and impacts of illegal harms\n",
      "\tAssessing risk\n",
      "\tMitigating risk\n",
      "\tIdentifying illegal content\n",
      "\tEnforcement\n",
      "\tNext steps\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tika import parser\n",
    "pdf_filepath = \"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Overview.pdf\"\n",
    "parsed_file = parser.from_file(pdf_filepath)['content']\n",
    "print(parsed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01980757713317871"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split inference document\n",
    "- This embedding is better than the one we used for bert\n",
    "- Couldn't use this for bert tokenisation/embeddings because we needed a bert tokenizer since our model is BERT\n",
    "\n",
    "Now we have our document in a machine readable form we need to split the doc into parts we can apply the model on and tag. We could do this using delimeters like paragraphs but a semanic splitter will hopefully seperate the doc by content i.e. each now input should be a new tag\n",
    "\n",
    "Q: Do we not need to clean the chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"avsolatorio/GIST-small-Embedding-v0\")  # get embeddings model\n",
    "text_splitter = SemanticChunker(embedding)  # apply the model to the type of split we want to perform (a semantic split)\n",
    "chunks = text_splitter.create_documents([parsed_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store spliced pdf in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Output_Data/Inf_res.json\"\n",
    "# print(len(chunks), chunks[0])\n",
    "\n",
    "import json\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # If the output file doesn't exist then create one\n",
    "# if not os.path.isfile(outfile):\n",
    "#     dict_chunks = defaultdict(list)\n",
    "#     for par in chunks:\n",
    "#         dict_chunks[par.page_content]=[]\n",
    "#     with open(outfile, \"w\") as outfile: \n",
    "#         json.dump(dict_chunks, outfile, indent = 4)\n",
    "#     print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.'),\n",
       " Document(page_content='This should help prevent atempts to commit fraud online. We are also \\nrecommending large and high-risk services have dedicated fraud repor�ng channels; \\n\\n\\n\\n \\n\\n \\n\\n3 \\n \\n\\n• Where services operate account verifica�on schemes, they should be transparent about the \\nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \\ndeceived by posts on fake accounts and should help address fraud and foreign interference in \\nUK processes such as elec�ons; and \\n\\n• Blocking accounts run by banned terrorist organisa�ons. Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \\nmeans of tackling the priority illegal harms in scope of the Act.'),\n",
       " Document(page_content='Some of the proposals set out in this \\nconsulta�on would apply to all services. However, we are targe�ng many of the more onerous \\nproposals only at services which are large and/or high risk. We provide a full list of all the measures \\nwe are proposing and who they would apply to in our “consulta�on at a glance” which is published \\nalongside this consulta�on. Our proposals will make a contribu�on to comba�ng violence against women and girls online, \\nincluding by se�ng out how services should assess their risk of coercive and controlling behaviour, \\nstalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \\nmeasures and some harm specific measures will begin to mi�gate these risks. However, we recognise \\nthat more work is needed in this area, and we will be publishing dra� guidance on how services can \\ncombat violence against women and girls in early 2025. Identifying illegal content \\nA new legal requirement of the Act is for all services to swi�ly take down specific illegal content \\nwhen they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \\n(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \\nlikely to be illegal. Enforcement \\nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \\nto issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \\ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \\nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \\n \\n\\nNext steps \\nWe welcome comments from stakeholders in response to the proposals set out in this document, \\nincluding any further evidence and suppor�ng informa�on to inform our final decisions.'),\n",
       " Document(page_content='The \\nconsulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \\ndate. This is also a statutory consulta�on under the Act, and we will engage with specific statutory \\nconsultees through the consulta�on process. For further details of the process for making Codes and \\nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \\ndocument. It explains what each chapter is about, what proposals we are making, why we are \\nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \\nnavigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \\ncan respond to this consulta�on. Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \\npublish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \\nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \\nhave three months to conduct their risk assessment. The Codes will also be subject to a \\nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \\npoint the Codes will come into force. This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \\nput the new online safety regula�ons in place. We provide more detail on our roadmap for \\nimplemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \\nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\n\\n\\tOverview\\n\\tCauses and impacts of illegal harms\\n\\tAssessing risk\\n\\tMitigating risk\\n\\tIdentifying illegal content\\n\\tEnforcement\\n\\tNext steps\\n\\n\\n')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open file back into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.': [],\n",
       " 'This should help prevent atempts to commit fraud online. We are also \\nrecommending large and high-risk services have dedicated fraud repor�ng channels; \\n\\n\\n\\n \\n\\n \\n\\n3 \\n \\n\\n• Where services operate account verifica�on schemes, they should be transparent about the \\nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \\ndeceived by posts on fake accounts and should help address fraud and foreign interference in \\nUK processes such as elec�ons; and \\n\\n• Blocking accounts run by banned terrorist organisa�ons. Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \\nmeans of tackling the priority illegal harms in scope of the Act.': [],\n",
       " 'Some of the proposals set out in this \\nconsulta�on would apply to all services. However, we are targe�ng many of the more onerous \\nproposals only at services which are large and/or high risk. We provide a full list of all the measures \\nwe are proposing and who they would apply to in our “consulta�on at a glance” which is published \\nalongside this consulta�on. Our proposals will make a contribu�on to comba�ng violence against women and girls online, \\nincluding by se�ng out how services should assess their risk of coercive and controlling behaviour, \\nstalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \\nmeasures and some harm specific measures will begin to mi�gate these risks. However, we recognise \\nthat more work is needed in this area, and we will be publishing dra� guidance on how services can \\ncombat violence against women and girls in early 2025. Identifying illegal content \\nA new legal requirement of the Act is for all services to swi�ly take down specific illegal content \\nwhen they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \\n(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \\nlikely to be illegal. Enforcement \\nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \\nto issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \\ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \\nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \\n \\n\\nNext steps \\nWe welcome comments from stakeholders in response to the proposals set out in this document, \\nincluding any further evidence and suppor�ng informa�on to inform our final decisions.': [],\n",
       " 'The \\nconsulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \\ndate. This is also a statutory consulta�on under the Act, and we will engage with specific statutory \\nconsultees through the consulta�on process. For further details of the process for making Codes and \\nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \\ndocument. It explains what each chapter is about, what proposals we are making, why we are \\nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \\nnavigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \\ncan respond to this consulta�on. Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \\npublish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \\nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \\nhave three months to conduct their risk assessment. The Codes will also be subject to a \\nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \\npoint the Codes will come into force. This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \\nput the new online safety regula�ons in place. We provide more detail on our roadmap for \\nimplemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \\nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\n\\n\\tOverview\\n\\tCauses and impacts of illegal harms\\n\\tAssessing risk\\n\\tMitigating risk\\n\\tIdentifying illegal content\\n\\tEnforcement\\n\\tNext steps\\n\\n\\n': []}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(str(outfile), 'r') as empt_par:\n",
    "#     strored_chunk = json.load(empt_par)\n",
    "# strored_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use inferencing pipeline to tag data with model prediction\n",
    "\n",
    "- Can use our pipeline to determine a tag with pipeline(input_text) and it will output the label and score\n",
    "\n",
    "Q: Why don't we use a real_data pdf for the inferencing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:22:45.800572: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 13:22:47.389439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def inference_pipeline(model_path, max_length=512):\n",
    "   pipe = pipeline(\"text-classification\", model=model_path, max_length=max_length, truncation=True)\n",
    "   return pipe\n",
    "\n",
    "best_model_path = \"/home/azureuser/cloudfiles/code/Users/Michael.Sowter/Deep_Learning_Training/Text Classifier/Models/Mod_1/Best\"\n",
    "infer = inference_pipeline(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.8220359683036804}]\n",
      "[{'label': 'Negative', 'score': 0.716924786567688}]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(infer(\"Governance\"))\n",
    "print(infer(\"Bye\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Negative', 'score': 0.5996244549751282}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try on our data\n",
    "infer(str(chunks)[0].replace(\"\\n\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.': {'governance and accountability': 0.1501895785331726},\n",
       " 'This should help prevent atempts to commit fraud online. We are also \\nrecommending large and high-risk services have dedicated fraud repor�ng channels; \\n\\n\\n\\n \\n\\n \\n\\n3 \\n \\n\\n• Where services operate account verifica�on schemes, they should be transparent about the \\nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \\ndeceived by posts on fake accounts and should help address fraud and foreign interference in \\nUK processes such as elec�ons; and \\n\\n• Blocking accounts run by banned terrorist organisa�ons. Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \\nmeans of tackling the priority illegal harms in scope of the Act.': {'governance and accountability': 0.14618158340454102},\n",
       " 'Some of the proposals set out in this \\nconsulta�on would apply to all services. However, we are targe�ng many of the more onerous \\nproposals only at services which are large and/or high risk. We provide a full list of all the measures \\nwe are proposing and who they would apply to in our “consulta�on at a glance” which is published \\nalongside this consulta�on. Our proposals will make a contribu�on to comba�ng violence against women and girls online, \\nincluding by se�ng out how services should assess their risk of coercive and controlling behaviour, \\nstalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \\nmeasures and some harm specific measures will begin to mi�gate these risks. However, we recognise \\nthat more work is needed in this area, and we will be publishing dra� guidance on how services can \\ncombat violence against women and girls in early 2025. Identifying illegal content \\nA new legal requirement of the Act is for all services to swi�ly take down specific illegal content \\nwhen they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \\n(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \\nlikely to be illegal. Enforcement \\nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \\nto issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \\ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \\nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \\n \\n\\nNext steps \\nWe welcome comments from stakeholders in response to the proposals set out in this document, \\nincluding any further evidence and suppor�ng informa�on to inform our final decisions.': {'governance and accountability': 0.45066750049591064},\n",
       " 'The \\nconsulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \\ndate. This is also a statutory consulta�on under the Act, and we will engage with specific statutory \\nconsultees through the consulta�on process. For further details of the process for making Codes and \\nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \\ndocument. It explains what each chapter is about, what proposals we are making, why we are \\nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \\nnavigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \\ncan respond to this consulta�on. Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \\npublish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \\nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \\nhave three months to conduct their risk assessment. The Codes will also be subject to a \\nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \\npoint the Codes will come into force. This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \\nput the new online safety regula�ons in place. We provide more detail on our roadmap for \\nimplemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \\nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\n\\n\\tOverview\\n\\tCauses and impacts of illegal harms\\n\\tAssessing risk\\n\\tMitigating risk\\n\\tIdentifying illegal content\\n\\tEnforcement\\n\\tNext steps\\n\\n\\n': {'governance and accountability': 0.24826580286026}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now do it properly\n",
    "res = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "    i = chunk.page_content\n",
    "    infer_res = infer(i.replace('\\n\\n', ''))[0]\n",
    "    if infer_res['label'] == \"Negative\": \n",
    "        infer_res['score'] = 1 - infer_res['score']  # keep scoring between 0 and 1\n",
    "\n",
    "    res[i] = {topic : infer_res['score']}\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.': {'governance and accountability': 0.1501895785331726,\n",
       "  'test': 0.1501895785331726},\n",
       " 'This should help prevent atempts to commit fraud online. We are also \\nrecommending large and high-risk services have dedicated fraud repor�ng channels; \\n\\n\\n\\n \\n\\n \\n\\n3 \\n \\n\\n• Where services operate account verifica�on schemes, they should be transparent about the \\nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \\ndeceived by posts on fake accounts and should help address fraud and foreign interference in \\nUK processes such as elec�ons; and \\n\\n• Blocking accounts run by banned terrorist organisa�ons. Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \\nmeans of tackling the priority illegal harms in scope of the Act.': {'governance and accountability': 0.14618158340454102,\n",
       "  'test': 0.14618158340454102},\n",
       " 'Some of the proposals set out in this \\nconsulta�on would apply to all services. However, we are targe�ng many of the more onerous \\nproposals only at services which are large and/or high risk. We provide a full list of all the measures \\nwe are proposing and who they would apply to in our “consulta�on at a glance” which is published \\nalongside this consulta�on. Our proposals will make a contribu�on to comba�ng violence against women and girls online, \\nincluding by se�ng out how services should assess their risk of coercive and controlling behaviour, \\nstalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \\nmeasures and some harm specific measures will begin to mi�gate these risks. However, we recognise \\nthat more work is needed in this area, and we will be publishing dra� guidance on how services can \\ncombat violence against women and girls in early 2025. Identifying illegal content \\nA new legal requirement of the Act is for all services to swi�ly take down specific illegal content \\nwhen they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \\n(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \\nlikely to be illegal. Enforcement \\nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \\nto issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \\ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \\nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \\n \\n\\nNext steps \\nWe welcome comments from stakeholders in response to the proposals set out in this document, \\nincluding any further evidence and suppor�ng informa�on to inform our final decisions.': {'governance and accountability': 0.45066750049591064,\n",
       "  'test': 0.45066750049591064},\n",
       " 'The \\nconsulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \\ndate. This is also a statutory consulta�on under the Act, and we will engage with specific statutory \\nconsultees through the consulta�on process. For further details of the process for making Codes and \\nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \\ndocument. It explains what each chapter is about, what proposals we are making, why we are \\nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \\nnavigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \\ncan respond to this consulta�on. Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \\npublish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \\nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \\nhave three months to conduct their risk assessment. The Codes will also be subject to a \\nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \\npoint the Codes will come into force. This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \\nput the new online safety regula�ons in place. We provide more detail on our roadmap for \\nimplemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \\nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\n\\n\\tOverview\\n\\tCauses and impacts of illegal harms\\n\\tAssessing risk\\n\\tMitigating risk\\n\\tIdentifying illegal content\\n\\tEnforcement\\n\\tNext steps\\n\\n\\n': {'governance and accountability': 0.24826580286026,\n",
       "  'test': 0.24826580286026}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    i = chunk.page_content\n",
    "    # print(i)\n",
    "    infer_res = infer(i.replace('\\n\\n', ''))[0]\n",
    "    if infer_res['label'] == \"Negative\": \n",
    "        infer_res['score'] = 1 - infer_res['score']  # keep scoring between 0 and 1\n",
    "\n",
    "    res[i][\"test\"] = infer_res['score']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n \\n\\n \\n\\n \\n \\n\\nOverview \\nThis document is the first of four major consulta�ons that Ofcom, as appointed regulator of the \\nnew Online Safety Act (‘the Act’), will publish as part of our work to establish the new regula�ons \\nover the next 18 months. It focuses on our proposals for how internet services which enable the sharing of user generated \\ncontent (‘user-to-user’ or ‘U2U’ services) and search services should approach their new du�es \\nrela�ng to illegal content. It covers the following areas: the causes and impacts of illegal harms; how \\nservices should assess and mi�gate the risks of illegal harms; how services can iden�fy illegal \\ncontent; and our approach to enforcement. The proposals in this document reflect research we have conducted over the past three years as well \\nas informa�on and evidence gathered through extensive engagement with industry and other \\nexperts. Causes and impacts of illegal harms \\nWe are consul�ng on our assessment of how the priority illegal harms covered by the Act manifest \\nonline, what factors give rise to a risk of these harms and what the impact of the harms is. This \\nanalysis underlines the need for services to take ac�on to combat online harms. It shows that a large \\npropor�on of the UK popula�on has experienced harm online and that the impact of online harms \\ncan, in cases, be extremely severe. Our assessment demonstrates that women, children and groups \\nwith protected characteris�cs are especially likely to be exposed to harm online. Assessing risk \\nWe are consul�ng on the guidance we propose to give about how regulated stakeholders should \\nassess the risk of illegal harm taking place on their services (the ‘Risk Assessment Guidance’), as well \\nas on our proposals about the governance services should put in place to manage risks. This \\ndocument also consults on guidance about how services should keep adequate records of their risk \\nassessments. Services should undertake a robust and comprehensive risk assessment. More specifically, we \\npropose that providers should follow the four-step process set out in Figure 1 below when assessing \\nthe risk of illegal content on their services. We are also proposing that services take several steps to ensure that they have strong governance \\nprocedures in place to mi�gate the risks associated with illegal content. For example, we are \\nproposing that senior governance bodies at large services review the service’s risk management \\nac�vi�es related to online safety at least annually and that all services iden�fy a named senior \\nexecu�ve who is accountable for compliance with the online safety du�es. 2 \\n \\n\\nFigure 1: Proposed four-step process for illegal content risk assessment \\n\\n \\n\\nMitigating risk \\nWe are consul�ng on our illegal harms Codes of Prac�ce, laying out recommended measures that \\nregulated services can take to mi�gate the risk of illegal harm. While the Codes are not binding, and \\nso services can choose to take a different approach to mee�ng their du�es, they act as a ‘safe \\nharbour’. This means any service that implements the recommenda�ons in the Codes would be \\ndeemed to be compliant with its related safety du�es. In the codes we propose to recommend that services put in place a series of measures which, taken \\ntogether, will help combat all of the priority illegal harms in scope of the Act. These measures \\ninclude: \\n\\n• ensuring content modera�on teams are appropriately resourced and trained;  \\n• having easy-to-use systems for users to report poten�ally illegal content and make \\n\\ncomplaints;  \\n• allowing users to block other users or disable comments;  \\n• conduc�ng tests when they update their algorithms that recommend content to users \\n\\n(‘recommender systems’) to assess the risk that the changes would increase the \\ndissemina�on of illegal content; and \\n\\n• a series of recommended steps to make their terms and condi�ons clear and accessible. In addi�on to these cross-cu�ng measures, we propose that relevant services should take a series of \\ntargeted steps to combat Child Sexual Exploita�on and Abuse (CSEA), fraud and terrorism. These \\ntargeted steps include: \\n\\n• Using a technology called ‘hash matching’ to detect and remove known CSAM (Child Sexual \\nAbuse Material). Consistent with the restric�ons in the Act, this proposal does not apply to \\nprivate communica�ons or end-to-end encrypted communica�ons. We are not making any \\nproposals that would involve breaking encryp�on. However, end-to-end encrypted services \\nare s�ll subject to all the safety du�es set out in the Act and will s�ll need to take steps to \\nmi�gate risks of CSAM on their services; \\n\\n• Taking steps to make it harder for perpetrators to groom children online. For example, \\nconfiguring default se�ngs so that children do not appear in lists sugges�ng other users \\nconnect with them; \\n \\n\\n• Deploying keyword detec�on systems to help find and remove posts linked to the sale of \\nstolen creden�als.': [],\n",
       " 'This should help prevent atempts to commit fraud online. We are also \\nrecommending large and high-risk services have dedicated fraud repor�ng channels; \\n\\n\\n\\n \\n\\n \\n\\n3 \\n \\n\\n• Where services operate account verifica�on schemes, they should be transparent about the \\nsteps they are taking to verify accounts. This is aimed at reducing the risk of users being \\ndeceived by posts on fake accounts and should help address fraud and foreign interference in \\nUK processes such as elec�ons; and \\n\\n• Blocking accounts run by banned terrorist organisa�ons. Our analysis suggests that, taken together, these measures will be an effec�ve and propor�onate \\nmeans of tackling the priority illegal harms in scope of the Act.': [],\n",
       " 'Some of the proposals set out in this \\nconsulta�on would apply to all services. However, we are targe�ng many of the more onerous \\nproposals only at services which are large and/or high risk. We provide a full list of all the measures \\nwe are proposing and who they would apply to in our “consulta�on at a glance” which is published \\nalongside this consulta�on. Our proposals will make a contribu�on to comba�ng violence against women and girls online, \\nincluding by se�ng out how services should assess their risk of coercive and controlling behaviour, \\nstalking, harassment and threats, and in�mate image abuse. Our proposals for cross-cu�ng \\nmeasures and some harm specific measures will begin to mi�gate these risks. However, we recognise \\nthat more work is needed in this area, and we will be publishing dra� guidance on how services can \\ncombat violence against women and girls in early 2025. Identifying illegal content \\nA new legal requirement of the Act is for all services to swi�ly take down specific illegal content \\nwhen they become aware of it. Today we are consul�ng on our Illegal Content Judgements Guidance \\n(‘ICJG’). This will provide guidance to services on how they can iden�fy whether a piece of content is \\nlikely to be illegal. Enforcement \\nThe Act gives us significant enforcement powers in the event of non-compliance, including the ability \\nto issue fines of up £18m or up to 10% of the service’s qualifying worldwide revenue (whichever is \\ngreater) and to apply for a court order requiring an internet service provider to withdraw access to \\nthe service to prevent a significant risk of harm to UK users as a result of its failure. 4 \\n \\n\\nNext steps \\nWe welcome comments from stakeholders in response to the proposals set out in this document, \\nincluding any further evidence and suppor�ng informa�on to inform our final decisions.': [],\n",
       " 'The \\nconsulta�on closes on 23 February 2024, and we invite stakeholders to provide responses by this \\ndate. This is also a statutory consulta�on under the Act, and we will engage with specific statutory \\nconsultees through the consulta�on process. For further details of the process for making Codes and \\nguidance, please refer to our Legal Framework (Annex 12). To improve the accessibility of our consulta�on, we are also publishing a “chapter summaries” \\ndocument. It explains what each chapter is about, what proposals we are making, why we are \\nmaking them and what input we would appreciate from stakeholders. Chapter 1 of this document (Introduc�on) provides further informa�on on how services can use and \\nnavigate the document. Annexes 1-4 explain our approach to consulta�ons and how stakeholders \\ncan respond to this consulta�on. Once the consulta�on closes, we will consider stakeholder responses, review our proposals, and \\npublish a statement se�ng out our final decisions in rela�on to our consulta�on proposals, including \\nfinal versions of our guidance and Codes. We currently plan to publish our Statement in Winter 2024. Following our Statement, services will \\nhave three months to conduct their risk assessment. The Codes will also be subject to a \\nparliamentary approval process. We expect this process to conclude by the end of 2024 at which \\npoint the Codes will come into force. This is one of a number of major consulta�ons we will publish as over the next 12-18 months as we \\nput the new online safety regula�ons in place. We provide more detail on our roadmap for \\nimplemen�ng the Online Safety Act at the following link: Crea�ng a safer life online for people in the \\nUK. https://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\nhttps://www.ofcom.org.uk/news-centre/2023/safer-life-online-for-people-in-uk\\n\\n\\tOverview\\n\\tCauses and impacts of illegal harms\\n\\tAssessing risk\\n\\tMitigating risk\\n\\tIdentifying illegal content\\n\\tEnforcement\\n\\tNext steps\\n\\n\\n': []}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strored_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload output back to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_output = output_folder + \"/infer_output.json\"\n",
    "with open(tagged_output, \"w\") as tagged_par: \n",
    "    json.dump(res, tagged_par, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEPS:\n",
    "\n",
    "# 0) Add topic name to json\n",
    "# 1) Adapt code into class\n",
    "# 2) Run code for top 3 topics (anything over 200 class 1 data points)\n",
    "# 3) Show Lolu json files (and code for reference)\n",
    "# 4) Fix and tidy Git and then move code to main Consul repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/llm_parser/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())  \u001b[38;5;66;03m# Should return True if a GPU is available\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())  \u001b[38;5;66;03m# Number of available GPUs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# The current device index (usually 0)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# The name of the device (e.g., 'NVIDIA GeForce GTX 1080 Ti')\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/llm_parser/lib/python3.12/site-packages/torch/cuda/__init__.py:787\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m/anaconda/envs/llm_parser/lib/python3.12/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Should return True if a GPU is available\n",
    "print(torch.cuda.device_count())  # Number of available GPUs\n",
    "print(torch.cuda.current_device())  # The current device index (usually 0)\n",
    "print(torch.cuda.get_device_name(0))  # The name of the device (e.g., 'NVIDIA GeForce GTX 1080 Ti')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
