{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import csv\\nimport json\\nimport pandas as pd\\nfrom collections import defaultdict'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/llm_parser/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-08 14:37:48.311006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 14:37:49.693170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "class BuildDataset:\n",
    "    \n",
    "    # Build the training set for training the model\n",
    "    def __init__(self, raw_trainingset_path):\n",
    "        self.full_dataset = raw_trainingset_path\n",
    "        self.foundation_model_name = \"distilbert/distilbert-base-uncased\"\n",
    "        # vectorize text using embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.foundation_model_name)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        #self.topic = topic\n",
    "        \n",
    "    \n",
    "    def raw_data_to_dict(self, csv_delimiter=None, csvFilePath=None):\n",
    "            # create a dataset dictionary\n",
    "            data = {}            \n",
    "            # Open a csv reader called DictReader\n",
    "            if csvFilePath is None: csvFilePath=self.full_dataset\n",
    "            with open(csvFilePath, encoding='utf-8-sig') as csvf:\n",
    "                if csv_delimiter == None:\n",
    "                    csvReader = csv.DictReader(csvf)#, delimiter=delimiter)#='\\t')\n",
    "                else:\n",
    "                    csvReader = csv.DictReader(csvf, delimiter=csv_delimiter)#='\\t')\n",
    "                for key, rows in enumerate(csvReader):                    \n",
    "                    data[key] = {\n",
    "                        \"Topic\":rows[\"Topic\"].strip().lower(),\n",
    "                        \"One-line summary\":rows[\"One-line summary\"],\n",
    "                        \"Full summary\":rows[\"Full summary of comment\"]\n",
    "                    }\n",
    "            return data\n",
    "    def topic_sample_size(self, data):\n",
    "        # Count the number of occurences of all the topics\n",
    "        data_check = defaultdict(int)\n",
    "        for i, rows in data.items():\n",
    "            data_check[rows['Topic'].lower().strip()] +=1\n",
    "        return data_check\n",
    "\n",
    "\n",
    "    def create_dataset(self, topic, data,number_of_pos_label, percentage_pos_label=100, percentage_neg_label=100):\n",
    "        # create trainset, testset and evaluationset\n",
    "        sampleset = []\n",
    "        evalset = []\n",
    "        data_check = defaultdict(int)\n",
    "        counter_for_neg_label = 0\n",
    "        counter_for_pos_label = 0\n",
    "        pos_size = int((percentage_pos_label/100)*number_of_pos_label)\n",
    "        neg_size = int((percentage_neg_label/100)*number_of_pos_label)\n",
    "        for i, rows in data.items():\n",
    "            data_check[rows['Topic'].strip().lower()] +=1\n",
    "\n",
    "            if rows[\"Topic\"].strip().lower().__eq__(topic.strip().lower()) and counter_for_pos_label<=pos_size:\n",
    "                sampleset.append(\n",
    "                    {\n",
    "                    \"text\": rows[\"Full summary\"],\n",
    "                    \"label\": 1, #governance\n",
    "                    \"summary\": rows[\"One-line summary\"]\n",
    "\n",
    "                    }\n",
    "                )\n",
    "                counter_for_pos_label +=1\n",
    "                \n",
    "            elif counter_for_neg_label <= neg_size and not (rows[\"Topic\"].strip().lower().__eq__(topic.strip().lower())):\n",
    "                    sampleset.append(\n",
    "                        {\n",
    "                            \"text\": rows[\"Full summary\"],\n",
    "                            \"label\": 0, #others8\n",
    "                            \"summary\": rows[\"One-line summary\"]\n",
    "\n",
    "                        }\n",
    "                        )\n",
    "                    counter_for_neg_label +=1\n",
    "            else:\n",
    "                    tag=0\n",
    "                    if rows[\"Topic\"].strip().lower().__eq__(topic.strip().lower()): tag=1 \n",
    "                    \n",
    "                    evalset.append(\n",
    "                        {\n",
    "                            \"text\": rows[\"Full summary\"],\n",
    "                            \"label\": tag, #others\n",
    "                            \"summary\": rows[\"One-line summary\"]\n",
    "                        }\n",
    "                    )\n",
    "            #counter_for_testset +=1\n",
    "        return sampleset, evalset, data_check\n",
    "    \n",
    "    def preprocess_function(self,examples):\n",
    "        return self.tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        \n",
    "    def text_preprocessing(self,trainingset):\n",
    "        # vectorize the dataset\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(trainingset)    \n",
    "        df = df.drop(columns =['summary'])        \n",
    "        # Convert the DataFrame into a Dataset\n",
    "        from datasets import Dataset\n",
    "        hugginface_data = Dataset.from_pandas(df)\n",
    "        tokenized_hugginface_data = hugginface_data.map(self.preprocess_function, batched=True)\n",
    "        split_tokenized_hugginface_data = tokenized_hugginface_data.train_test_split(test_size=0.1)\n",
    "        \n",
    "        return split_tokenized_hugginface_data\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "class TrainModel:\n",
    "    # Train the chosen foundation model\n",
    "   \n",
    "    def __init__(self, foundation_model_name = \"distilbert/distilbert-base-uncased\"):\n",
    "        self.id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "        self.label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "        self.num_labels = 2\n",
    "        self.foundation_model_name = foundation_model_name\n",
    "        self.accuracy = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "        \n",
    "    \n",
    "\n",
    "    def train_model(self, folder, dataset, original_tokenizer, data_collator):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                                                    self.foundation_model_name,\n",
    "                                                    num_labels=self.num_labels,\n",
    "                                                    id2label=self.id2label,\n",
    "                                                    label2id=self.label2id\n",
    "                                                    )\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=folder,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=20,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            return self.accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset= dataset['train'],\n",
    "            eval_dataset=dataset['test'],\n",
    "            tokenizer=original_tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "class Inference: \n",
    "    \n",
    "    # inference the trained model   \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "    def inference_pipeline(self, excerpt, max_length=512):\n",
    "        pipe = pipeline(\"text-classification\", self.model_path, max_length=max_length, truncation=True)\n",
    "        return pipe(excerpt)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the result of the training\n",
    "import pandas as pd \n",
    "from datasets import Dataset \n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#import Inference \n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "class EvaluateResult:\n",
    "  \n",
    "    def __init__(self, val_df, validation_data_size, model_path):\n",
    "        self.df = val_df[:validation_data_size]\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def get_pred_n_ref(self):\n",
    "        infer = Inference(self.model_path)\n",
    "        self.df = self.df.drop(columns =['summary'])\n",
    "        hugginface_data = Dataset.from_pandas(self.df)\n",
    "        prediction=[]\n",
    "        for out in infer.inference_pipeline(excerpt = KeyDataset(hugginface_data, \"text\")):\n",
    "            if out['label'].__eq__('NEGATIVE'):\n",
    "                prediction.append(0)\n",
    "            else:\n",
    "                prediction.append(1)\n",
    "        ref = self.df['label'].tolist()\n",
    "        return prediction, ref\n",
    "    \n",
    "    def eval_calculation(self):\n",
    "        prediction, ref = self.get_pred_n_ref()\n",
    "        y_true = np.array(ref)\n",
    "        y_pred = np.array(prediction)\n",
    "        return precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training set, evaluation set and validation set\n",
    "topic = \"Governance and accountability\"\n",
    "dataset_builder = BuildDataset(\"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/Llama_tutorial/data/consultation2.csv\")\n",
    "full_data_dict = dataset_builder.raw_data_to_dict(csv_delimiter = '\\t')\n",
    "all_topic_count = dataset_builder.topic_sample_size(full_data_dict)\n",
    "number_of_pos_label = all_topic_count[topic.strip().lower()]\n",
    "topic_sampleset, topic_evalset, _ = dataset_builder.create_dataset(topic=topic, data=full_data_dict,number_of_pos_label=int(number_of_pos_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 457/457 [00:00<00:00, 3652.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize\n",
    "split_tokenized_hugginface_data = dataset_builder.text_preprocessing(topic_sampleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/anaconda/envs/llm_parser/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 04:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.621262</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592432</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653311</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668924</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697973</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.826183</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828580</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.091977</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.918313</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056659</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.981480</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.036987</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.988877</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.018659</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.049557</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.061366</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.013578</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.083594</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>1.090289</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-26 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-52 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-78 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-104 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-130 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-156 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-182 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-208 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-234 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-260 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-286 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-312 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-338 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-364 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-390 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-416 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-442 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-468 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-494 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-520 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "folder ='/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance'\n",
    "dataset=split_tokenized_hugginface_data\n",
    "original_tokenizer= dataset_builder.tokenizer\n",
    "data_collator= dataset_builder.data_collator\n",
    "foundation_model_name = \"distilbert/distilbert-base-uncased\"\n",
    "bert_trainer = TrainModel(foundation_model_name)\n",
    "bert_trainer.train_model(folder, dataset, original_tokenizer, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1895"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_sampleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1438"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te = pd.DataFrame.from_dict(topic_evalset) #topic_evalset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8, 0.8, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "val_df = df_te\n",
    "validation_data_size = 200\n",
    "model_path = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-208\"\n",
    "evaluator = EvaluateResult(val_df, validation_data_size, model_path)\n",
    "evaluator.eval_calculation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference PDF\n",
    "import csv\n",
    "import json\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from tika import parser\n",
    "from transformers import pipeline\n",
    "class PdfELT:\n",
    "    def __init__(self, pdf_path, model_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"avsolatorio/GIST-small-Embedding-v0\")\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def get_pdf(self):\n",
    "        #get pdf from stored folder and parses it into pages\n",
    "\n",
    "        filepath = self.pdf_path  # Replace with the path of your file\n",
    "        parsed_document = parser.from_file(filepath)\n",
    "        broken_pdf = parsed_document['content']\n",
    "        #print(data)\n",
    "        return broken_pdf\n",
    "\n",
    "\n",
    "    def split_pdf(self,raw_txt,embedding=None):\n",
    "        # Uses semantic chunking to split the paragraphs in the document\n",
    "        if embedding is None: embedding= self.embeddings\n",
    "        text_splitter = SemanticChunker(embedding)\n",
    "        text_chunks = text_splitter.create_documents([raw_txt])\n",
    "        #print(docs[0].page_content)\n",
    "        len(text_chunks)\n",
    "        \n",
    "        return text_splitter, text_chunks\n",
    "    \n",
    "    def tag_pdf(self,text_chunks_dict, topic=\"governance and accountability\"):\n",
    "        dict_result = defaultdict(list)\n",
    "        infer = Inference(self.model_path)\n",
    "        class_name = topic\n",
    "        for content, tag_list in text_chunks_dict.items(): \n",
    "            paragraph = content.replace(\"\\n\\n\", \"\")\n",
    "            class_result = infer.inference_pipeline(paragraph)[0]   #pipe(paragraph.replace(\"\\n\", \"\"))[0]\n",
    "            #print(class_result)\n",
    "            class_result.update({'class_name':class_name})   \n",
    "            tag_list.append(class_result)\n",
    "            dict_result[paragraph] = tag_list\n",
    "        \n",
    "        return dict_result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full topic classification ETL\n",
    "from collections import defaultdict \n",
    "model_path = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/models/bert_model_governance/checkpoint-208\"\n",
    "pdf_path = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/Llama_tutorial/data/consultation2.csv\"\n",
    "empty_par_folder = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/results/empty_paragraph\"\n",
    "tagged_par_folder = \"/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/results/tagged_paragraph\"\n",
    "etl = PdfELT(pdf_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 14:49:36,759 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
      "2024-05-08 14:49:37,445 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "2024-05-08 14:49:37,799 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "# 1. pull pdf\n",
    "pdf_pages = etl.get_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. split PDF into chunks\n",
    "splitter, chunks = etl.split_pdf(pdf_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTopic\\tTopic (custom)\\tOne-line summary\\tFull summary of comment\\nUser reporting and complaints (U2U and search) \\t\\tEvery service should have a dedicated reporting channel for fraud, because there is lots of fraud on a range of platforms. All services should have a dedicated channel for reporting fraud. We see and hear about a lot of fraud on small platforms, especially dating sights and job boards. Governance and accountability \\tSegmentation \\tThis mitigation should apply to all services, large and small. This is b/c some small services have been taken over by scammers / human traffickers \\tThis mitigation should apply to all services, large and small. A community message board or website may be low risk. But some small dating sites, job boards, investment and eCommerce sites have been taken over by scammers and even human traffickers, in our experience. At least specific risk services like dating and job apps should have governance bodies). Governance and accountability \\tTracking evidence of new and increasingly illegal harm\\tIs suggesting a drafting change to the code. Substantive change appears to be that this info is shared with Ofcom. \"Evidence of new kinds of illegal content on a service, information about how illicit actors and groups are utilizing a platform, or increases in particular kinds of illegal content, is tracked and reported to the most senior governance body and shared with OfCom\"\\nGovernance and accountability \\tCode of conduct regarding protection of users from illegal harm\\tSuggesting drafting changes to the code. A Code of Conduct or principles provided to all staff that sets standards and expectations for employees around protecting users from risks of illegal harm, and keeping illicit actors and predators from abusing the service towards illegal ends\\nGovernance and accountability \\t\\tSuggesting drafting changes to the code. Staff compliance training\\tStaff, in particular engineers, involved in the design and operational management of a service are sufficiently trained in ways that illicit actors have used and abused online services, and are “sufficiently trained in a service’s approach to compliance\\nGovernance and accountability \\tSegmentation\\tAll services should have to comply with these measures. All services should have to comply with these measures.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Put chunks into json file and store in empty_paragraph folder\n",
    "dict_chunks = defaultdict(list)\n",
    "for par in chunks:\n",
    "    dict_chunks[par.page_content]=[]\n",
    "outfile = empty_par_folder + \"/overview_gov_class.json\"\n",
    "with open(outfile, \"w\") as outfile: \n",
    "    json.dump(dict_chunks, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/home/azureuser/cloudfiles/code/Users/Omololu.Makinde/work/outputs/results/empty_paragraph/overview_gov_class.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get chunks from json file\n",
    "outfile = empty_par_folder + \"/overview_gov_class.json\"\n",
    "with open(str(outfile), 'r') as empt_par:\n",
    "    strored_chunk = json.load(empt_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Tag chunk using classifier\n",
    "res = etl.tag_pdf(strored_chunk, topic=\"governance and accountability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Topic\\tTopic (custom)\\tOne-line summary\\tFull summary of comment\\nUser reporting and complaints (U2U and search) \\t\\tEvery service should have a dedicated reporting channel for fraud, because there is lots of fraud on a range of platforms. All services should have a dedicated channel for reporting fraud. We see and hear about a lot of fraud on small platforms, especially dating sights and job boards. Governance and accountability \\tSegmentation \\tThis mitigation should apply to all services, large and small. This is b/c some small services have been taken over by scammers / human traffickers \\tThis mitigation should apply to all services, large and small. A community message board or website may be low risk. But some small dating sites, job boards, investment and eCommerce sites have been taken over by scammers and even human traffickers, in our experience. At least specific risk services like dating and job apps should have governance bodies). Governance and accountability \\tTracking evidence of new and increasingly illegal harm\\tIs suggesting a drafting change to the code. Substantive change appears to be that this info is shared with Ofcom. \"Evidence of new kinds of illegal content on a service, information about how illicit actors and groups are utilizing a platform, or increases in particular kinds of illegal content, is tracked and reported to the most senior governance body and shared with OfCom\"\\nGovernance and accountability \\tCode of conduct regarding protection of users from illegal harm\\tSuggesting drafting changes to the code. A Code of Conduct or principles provided to all staff that sets standards and expectations for employees around protecting users from risks of illegal harm, and keeping illicit actors and predators from abusing the service towards illegal ends\\nGovernance and accountability \\t\\tSuggesting drafting changes to the code. Staff compliance training\\tStaff, in particular engineers, involved in the design and operational management of a service are sufficiently trained in ways that illicit actors have used and abused online services, and are “sufficiently trained in a service’s approach to compliance\\nGovernance and accountability \\tSegmentation\\tAll services should have to comply with these measures. All services should have to comply with these measures.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9967339038848877,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Governance and accountability \\t\\tFuture measure of tying renumeration to positive safety outcomes. ACCO would \"applaud\" this measure. Future measure of tying renumeration to positive safety outcomes. ACCO would \"applaud\" this measure. Content moderation (User to User) \\t\\tIn terms of prioritising content review, ACCO would put harm to children, chance of death or serious bodily harm above virality. When prioritising what content to review, regard is had to the following factors: virality of content, potential severity of content and the likelihood that content is illegal. We believe priority should be placed on harms to children, chance of death or serious bodily harm above virality. Approach to the Codes\\tSegmentation\\tIt doesn\\'t agree that we should just put most onerous measures on large / risky services. It thinks dating apps, and job and investment apps pose particular risks. (Disagree).': [{'label': 'POSITIVE',\n",
       "               'score': 0.9939661622047424,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As noted before, \"there should be special attention paid to dating apps, job and investment sites\". Governance and Accountability \\t\\tRSPCA supports our proposed governance measures and our segmentation\\tRSPCA supports our proposed governance measures and our segmentation\\nGovernance and Accountability \\t\\tLive streaming is a relevant risk factor. Live streaming of animal cruelty as defined under S4(1) of the AWA 2006 is a risk factor and has been identified by the RSPCA as a risk eg live feeding of live prey animals to snakes, live streaming of dog and other animal fights\\nGovernance and Accountability \\t\\tFinancial penalties are a good incentive to encourage compliance. Enforcing S4(1) offences arising from on line content has been assisted by enforcement using financial recoup such as the use of the Proceeds of Crime Act 2002 and Fraud Act 2006 – such activities can act as a disincentive for content to be posted on line \\nApproach to the Codes\\tSegmentation\\tThe RSPCA agrees that the most onerous measures should apply to the largest and riskiest services, and with our definition of large and multi-risk. The RSPCA agrees that the most onerous measures should apply to the largest / riskiest services. Though, it notes that high risk services may not correlate with the large or medium services. It also agrees with our proposed definition of \"large\" and \"multi-risk\". Approach to the Codes\\tSegmentation\\tThe RSPCA agrees that the most onerous measures should apply to the largest and riskiest services, and with our definition of large and multi-risk. The RSPCA agrees that the most onerous measures should apply to the largest / riskiest services. Though, it notes that high risk services may not correlate with the large or medium services. It also agrees with our proposed definition of \"large\" and \"multi-risk\". Approach to the Codes\\tSegmentation\\tThe RSPCA agrees that the most onerous measures should apply to the largest and riskiest services, and with our definition of large and multi-risk. The RSPCA agrees that the most onerous measures should apply to the largest / riskiest services. Though, it notes that high risk services may not correlate with the large or medium services.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9958851933479309,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It also agrees with our proposed definition of \"large\" and \"multi-risk\". Governance and accountability\\t\\tCarefulAI argues that we have missed mitigation of the risk on not acting upon suicidal ideation in online dialogue\\tCarefulAI argues that we have missed mitigation of the risk on not acting upon suicidal ideation in online dialogue\\nGovernance and accountability\\t\\tCarefulAI argues that there is a risk among those who take their own life following suicidial ideation language - in that it is hard to trace the action that was/was not taken. CarefulAI argues that 1 in 17 people who represent their suicidal ideation language in speech go onto self harm, and those that go on to take their own life represent a risk in not being able to make a link between a death and action not being taken upon spotting suicidal ideation language. Approach to the Codes\\t\\tCarefulAI argues that we did not specifically state the use of clinically validated lists of suicidal language to mitigate risks  \\tCarefulAI argues that we did not specifically state the use of clinically validated lists of suicidal language to mitigate risks  \\nApproach to the Codes\\t\\tCarefulAI agree we should apply the most onerous measures in our Codes only to services which are large and/or medium or high risk. CarefulAI agree we should apply the most onerous measures in our Codes only to services which are large and/or medium or high risk. User access to services (U2U)\\t\\tGeoComply supports Ofcom\\'s efforts to block accounts of users who share CSAM, and recommends leveraging device fingerprinting and authenticated multi-source geolocation data (as opposed to IP location) as part of the authentication process, to prevent a user from returning to a service. \"GeoComply outlined device fingerprinting/Fingerprinting as a service (FaaS) as a means to fight fraud and authenticate identity, and explained how it could enable platforms to flag devices associated with known suspicious/blocked users, in turn preventing multiple fake accounts and reducing risk of revictimisation. GeoComply also argue that FaaS can be used to detect hacking, which can combat financial sextortion. GeoComply outlined a disadvanatage of FaaS being false positives, though argued this should be combatted via risk management frameworks, and combining several fraud detection activities. Regarding Geolocation, GeoComply outlined how multi-sourced geolocation data points such as GPS and Wi-Fi triangulation could enhance authentication processes to identify anomalous user behaviour, and is more accurate than IP address (GeoComply gave examples of the limitations of IP for location). GeoComply advised against relying on IP for blocking/preventing users. GeoComply gave the example of regulated online gaming in the US to show how multi-source geolocation data can balance online safety with privacy. GeoComply argued for the benefits of using both geolocation with FaaS.. GeoComply also argue that geolocation can be used to identify and manage hotspots for illegal activity, for example, flagged locations associated with sextortion. GeoComply argued for empowering platforms with greater actionable insights by integrating multi-source geolocation and anomalous behaviour detection into their risk management frameworks. GeoComply argued for records regarding risk management, enforcing ToS and preventing device recidivism.\"\\nn/a\\tProportionality / help for smaller services\\tSupportive that many measures do not apply to smaller / low risk services. Asks what we can do to help ensure smaller providers / new entrants aren\\'t overwhelmed by requirements that might not apply to them. FCS welcomes Ofcom\\'s approach...it is important that the requirements are proportionate and do not stifle innovation, particularly from smaller providers, whilst, at the same time, keeping people safe on-line. The measures and requirements suggested in the Code of Practice cover a wide range of areas, many of which do not apply to small, low-risk services. It might be helpful, in due course, to provide specific guidance for those small, low-risk services so that smaller providers and new entrants are not overwhelmed by requirements that might not apply to them\\nGovernance and accountability\\t\\tAgree that either a named person or an overall governance body should carry out an annual review to record how the service has managed the risk of illegal harms. Agree that either a named person or an overall governance body should carry out an annual review to record how the service has managed the risk of illegal harms. Governance and accountability\\t\\tLarge services with specific risk should carry out internal moniroting and assurance to independently assess the effectiveness of measures. AVPA argue that it does not seem logical that the number of risks would affect the degree of governance for risk as a whole. Large services will generally have internal audit functions already in place, and these can be used to review the management of specific risks without a disproportionate impact. Content moderation (User to User) \\t\\tA smaller service with a specific risk should be required to adopt internal content moderation policies. A smaller service with a specific risk should be required to adopt internal content moderation policies, having regard to the findings of risk assessment and any evidence of emerging harms on the service. AVPA argue it does not seem to be logically defensible that the number of risks would affect the degree of management of the risk. Content moderation (Search)\\t\\tA smaller service with a specific risk should be required to adopt internal content moderation policies. A smaller service with a specific risk should be required to adopt internal content moderation policies, having regard to the findings of risk assessment and any evidence of emerging harms on the service. AVPA argue it does not seem to be logically defensible that the number of risks would affect the degree of management of the risk. Content moderation (User to User) \\t\\tSmaller services with a specific risk relating to content moderation should also have targets for content moderation functions and prioritise content review in line with multi-risk services. Smaller services with a specific risk relating to content moderation should also have targets for content moderation functions and prioritise content review in line with multi-risk services. If the specific risk that applies to the service is one that is mitigated by content moderation, staff should have training and materials to identify and take down illegal content. Content moderation (Search)\\t\\tSmaller services with a specific risk relating to content moderation should also have targets for content moderation functions and prioritise content review in line with multi-risk services. Smaller services with a specific risk relating to content moderation should also have targets for content moderation functions and prioritise content review in line with multi-risk services. If the specific risk that applies to the service is one that is mitigated by content moderation, staff should have training and materials to identify and take down illegal content. Automated content moderation (User to User) \\t\\t In addition to hash matching, there should be a requirement to apply automated age assurance to detect potential newly generated CSAM, and age verification for all performers in adult content. In addition to hash matching, there should be a requirement to apply automated age assurance to detect potential newly generated CSAM. This would be to flag content where anyone depicted appears to be under 18. This would have to operate in conjuction with age verification for all performers in adult content, with evidence available to confirm if the performer is in fact 18 or older. This would drive more comprehensive consent records. Without these changes, the guidance could allow a performer 18+ to create an account and then give the use of it to a child. Guidance also ignore risks presented by newly created CSAM as only requires checks against datacases of known CSAM - this cannot be ignored. Automated content moderation (User to User) \\t\\t In addition to hash matching, there should be a requirement to apply automated age assurance to detect potential newly generated CSAM, and age verification for all performers in adult content. In addition to hash matching, there should be a requirement to apply automated age assurance to detect potential newly generated CSAM. This would be to flag content where anyone depicted appears to be under 18. This would have to operate in conjuction with age verification for all performers in adult content, with evidence available to confirm if the performer is in fact 18 or older. This would drive more comprehensive consent records. Without these changes, the guidance could allow a performer 18+ to create an account and then give the use of it to a child. Guidance also ignore risks presented by newly created CSAM as only requires checks against datacases of known CSAM - this cannot be ignored. Default settings and user support (U2U)\\t\\tThese measures are only recommended for services which have an existing means of identifying child users, which deprives children on other services - this creates a perverse disincentive. These measures are only recommended for services which have an existing means of identifying child users, which deprives children on other services - this creates a perverse disincentive. If this loophole is retained, then the phrase “existing means of identifying child users” needs to be rewritten and clarified. AVPA argue the term \"identifying\" is misleading and better phrasing would be “has an existing means of knowing which users are under 18 years old”. AVPA then go on to list ways this can be achieved such as: self-declaration, age assurance, profiling, nature of service etc. Default settings and user support (U2U)\\t\\tThese measures are only recommended for services which have an existing means of identifying child users, which deprives children on other services - this creates a perverse disincentive. These measures are only recommended for services which have an existing means of identifying child users, which deprives children on other services - this creates a perverse disincentive. If this loophole is retained, then the phrase “existing means of identifying child users” needs to be rewritten and clarified. AVPA argue the term \"identifying\" is misleading and better phrasing would be “has an existing means of knowing which users are under 18 years old”. AVPA then go on to list ways this can be achieved such as: self-declaration, age assurance, profiling, nature of service etc. Enhanced user control (U2U) \\t\\tThese protections should also be in place for child users by default, including for smaller services where there is a specific risk, along with the requirements to prevent grooming\\tThese protections should also be in place for child users by default, including for smaller services where there is a specific risk, along with the requirements to prevent grooming\\nEnhanced user control (U2U) \\t\\tThese protections should also be in place for child users by default, including for smaller services where there is a specific risk, along with the requirements to prevent grooming\\tThese protections should also be in place for child users by default, including for smaller services where there is a specific risk, along with the requirements to prevent grooming\\nCumulative assessment\\t\\tOfcom\\'s approach to proportionality is excessively economic\\tWhile the Act requires proportionality, Ofcom is also required to look at the severity of harm when weighing the need for action. Even the smallest site could do great harm and should not be given a free pass, or appear to be given one - which AVPA suggests the guidance does at times. Automated content moderation (User to User) \\t\\tCSAM and Fraud detection should not be used on certain websites, namely particular sales websites and standalone websites. CSAM and Fraud detection should not be used on certain websites, namely particular sales websites and standalone websites (list of examples given). It should not be impossible for people to contact children who they have never spoken to before, stops ability to forge connections (gives anecdotal evidence). At the backend, it is hard to do in PHP-based systems to do a blocking system adequately. Media literacy/digital competency is key - there should instead be warnings \"this message was sent by someone you may not know, open at your own risk\". User reporting and complaints (U2U and search) \\t\\tThe list of trusted flaggers for fraud does not include Police Scotland. Re: section 16 (Reporting and complaints),  the list of trusted flaggers for fraud does not include Police Scotland, who  take public reports of fraud in Scotland (although suggested perhaps arrangements have been made through one of the other organisations listed?)\\nAutomated content moderation (User to User) \\t\\tThe LFF welcome Ofcom\\'s efforts to adjust the scope of this measure to capture services with lower user bases, and recognise the justification for not extending the measure to all medium and high-risk services regardless of size (including limited capacity of smaller services and potential for database ecosystem to be overwhelmed by increased demand). We are pleased to see that in respect of this recommendation, the size of user base for services that fall within scope has been adjusted to account for the risk and scale of offending in specific online spaces. We also note that the limited capacity of providers of hash matching databases is a factor in not making the recommendation apply to all medium and high-risk services regardless of size. We understand concerns that the database ecosystem would not be able to cope with levels of demand in the short term but very much hope that the ecosystem will be able to adapt quickly to meet future demand so that the recommendation on hash matching can become universally applicable.\\'\\nAutomated content moderation (User to User) \\t\\tThe LFF welcome Ofcom\\'s efforts to adjust the scope of this measure to capture services with lower user bases, and recognise the justification for not extending the measure to all medium and high-risk services regardless of size (including limited capacity of smaller services and potential for database ecosystem to be overwhelmed by increased demand). We are pleased to see that in respect of this recommendation, the size of user base for services that fall within scope has been adjusted to account for the risk and scale of offending in specific online spaces. We also note that the limited capacity of providers of hash matching databases is a factor in not making the recommendation apply to all medium and high-risk services regardless of size. We understand concerns that the database ecosystem would not be able to cope with levels of demand in the short term but very much hope that the ecosystem will be able to adapt quickly to meet future demand so that the recommendation on hash matching can become universally applicable.\\'\\nAutomated content moderation (User to User) \\t\\tMeasure 14 (CSAM hash matching): when images are blocked because of CSAM hash matching, a splash page should be shown which meets the IWF recommendations, including a link to the \\'Stop It Now\\' service\\tAn error 404 message represents an opportunity to serve a warning to the user who tried to access the hashed image. In respect of blocked URLs, an error 404 message was served prior to 2015. However, since then, the IWF has been recommending that a splash page is served which replaces the error 404 message. [Evidence from the IWF shows that] The splash page is having impact. As acknowledged in Volume 4 (paragraph 22.51), since 2015, the IWF reports that splash pages have resulted in 26,000 new users coming through to Stop It Now services via the website. We believe that a warning served in place of the error 404 message when an attempt to access a hashed image is made could easily replicate what the IWF splash page for blocked URLs does in notifying the user that viewing CSAM is illegal, it causes harm, there are serious consequences but there is help to stop available confidentially and anonymously with Stop It Now services. It could take the hash matching process one important step further in preventing offending.\\'\\nAutomated content moderation (User to User) \\t\\tMeasure 15 (CSAM URL detection): when a URL is detected as being linked to CSAM, a splash page should be shown which meets the IWF recommendations, including a link to \\'Stop It Now\\' service\\t[We] would like to see the recommendation for URL detection extended to include the serving of a splash page when an attempt to access a blocked URL has been made. It has been the recommendation of the IWF since 2015 that in tandem with using its URL List, technology companies should also serve a splash page which informs the user why they cannot access the page requested and signposts them to Stop It Now services for concerns about their online behaviour. We consider that the serving of a splash page takes the blocking of URLs one important step further. Splash pages prevent offending by informing people who try to access blocked URLs about the illegality of viewing CSAM, the harm sexual abuse causes to children and the consequences of offending for themselves and their families. In addition, the splash pages direct the user to confidential and anonymous help to change their behaviour from Stop It Now services.\\'\\nAutomated content moderation (User to User) \\t\\tThe LFF encourages Ofcom to use our info gathering powers to compel services to provide data on the efficacy of warning messages served to users searching for CSAM\\tGenerally, we have found that technology companies will not share this type of information [that is information on: how often a warning message is triggered, whether this has altered by the introduction of an online warning, and what users do once they see one of these online warnings], and will only do so once forced to by a regulator. We therefore very much encourage Ofcom to exercise its powers to obtain the necessary data which we believe exists with technology companies who currently deploy warning messages in order to build the evidence base which already exists. We would like to draw attention to the fact that the evaluation provides an important evidence-base for Ofcom to make recommendations in relation to warning messages. And we believe a more extensive evidence-base already exists and is sitting with those technology companies who have been deploying warning messages for some time.\\'\\nService design and user support (Search) \\t\\tMeasure 27 (CSAM search warning) should include guidance that services work with experts in deterrance messaging to ensure efficacy of their warnings, and should be reviewed over time to ensure optimisation\\'\\tWe believe that the recommendation for a warning could be strengthened further with guidance to technology companies that they work to develop these warnings with experts in deterrence messaging and that the effectiveness of warnings are reviewed over time to ensure optimisation.\\' [An incident in which Google removed the link to the Stop It Now provides] evidence that how a warning message appears and what it says are important factors relevant to the effectiveness of a warning in driving traffic to our website. We therefore consider that further guidance to technology companies to review, and improve the effectiveness of warnings, working alongside experts in deterrence messaging and perpetrator prevention, could add weight to this recommendation and elevate the efficacy of warning messages.\\'\\nService design and user support (Search) \\t\\tMeasure 27 (CSAM search warning) should be more ambitious and apply to services other than the largest search services, including U2U services, in line with research suggesting the efficacy of the measure on U2U services\\t[We] would like to see the recommendation for search moderation extended beyond large search services. Warning messages are currently being deployed in search functions within platforms, some examples are [Meta and Aylo.]... The evaluation of the reThink Chatbot project, a collaboration between the IWF, Aylo and the Lucy Faithfull Foundation provides evidence on the efficacy of safety measures in search functions.\\' Key findings from that study include: 1. 99.8% of sessions during the evaluation period did not contain a search that triggered a warning.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7941640019416809,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"2. Warnings resulted in 1,656 requests for more information about Stop It Now services. 3. There were 490 click-throughs to the Stop It Now website and approximately 68 calls and chats to the Stop It Now helpline. 4. There was a statistically significant trend showing a decrease in the number of searches for CSAM on Pornhub UK during the evaluation period. 5. Most sessions which triggered the warning and chatbot once do not appear to have searched for CSAM again. Content moderation (Search)\\t\\tMeasure 27 (CSAM search warning) should be more ambitious and apply to services other than the largest search services, including U2U services, in line with research suggesting the efficacy of the measure on U2U services\\t[We] would like to see the recommendation for search moderation extended beyond large search services. Warning messages are currently being deployed in search functions within platforms, some examples are [Meta and Aylo.]... The evaluation of the reThink Chatbot project, a collaboration between the IWF, Aylo and the Lucy Faithfull Foundation provides evidence on the efficacy of safety measures in search functions.' Key findings from that study include: 1. 99.8% of sessions during the evaluation period did not contain a search that triggered a warning.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933431148529053,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '2. Warnings resulted in 1,656 requests for more information about Stop It Now services. 3. There were 490 click-throughs to the Stop It Now website and approximately 68 calls and chats to the Stop It Now helpline. 4. There was a statistically significant trend showing a decrease in the number of searches for CSAM on Pornhub UK during the evaluation period. 5.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9834557175636292,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Most sessions which triggered the warning and chatbot once do not appear to have searched for CSAM again. Default settings and user support (U2U)\\t\\tFriction should be added for adult users as well as child users\\tWe hope that Ofcom can explore further the options and potential for creating friction in the grooming process on the perpetrator side as we believe that this coupled with safety measures by default for child users could result in more disruption of those seeking to groom children online.\\' For example: warning to adult users when making multiple, scattergun friend requests to children, and warnings when an adult user tries to communicate with a child user for the first time. \\'[W]hilst there may not be evidence as to the effectiveness of such interventions in the grooming process, we believe an evidence-base likely does exist in relation to other harms where such practices are commonplace such as detecting fraud and preventing data breaches.\\'\\nGovernance and accountability\\t\\tUnclear how to assign liability for a platform hosting a service, or a contributer or moderator, and no right to apply UK law on oversees entities. Unclear how to assign liability for a platform hosting a service, or a contributer or moderator. Codes don’t work for this area, we need to state what is illegal or not and prosecute if necessary. Ofcom do not have the right to impose UK law on overseas entities. This runs contrary to other nations’ law on freedom and privacy. How it would be enforced/how many millions of companies does Ofcom intend to contact every week to update their information. Governance and accountability\\t\\tUnrealistic to expect a worldwide company based overseas, or very small companies to spend thousands on specific measures - impact on innovation. Unrealistic to expect a worldwide company based overseas, or very small companies to spend thousands on specific measures. Innovation and creativity will cease - gave example of charity sector and compulsory risk assessments. Governance and accountability\\t\\tNot sure how the UK Gov. believes it will audit, or even identify the number of sites worldwide under scope. \"Not sure how the UK Gov. believes it will audit, or even identify the number of sites worldwide under scope. Danger that selection will be politically motivated.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9892278909683228,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'U.S. company will not want British auditors conducting audits on their systems, accessing \\nconfidential information.\"\\nGovernance and accountability\\t\\tVery large IT companies have 100ks of employees with complex lines of responsibility, unclear who the rules target. Very large IT companies have 100ks of employees with complex lines of responsibility, unclear who the rules target. Approach to the Codes\\t\\tCodes are too technical and prescriptive, and not enforceable. Hard to understand how a government entity can proscribe companies to comply, very technical, not enforceable - should just set goals and fines where required. Approach to the Codes\\t\\tCodes are too technical and prescriptive, and not enforceable. Hard to understand how a government entity can proscribe companies to comply, very technical, not enforceable - should just set goals and fines where required. Approach to the Codes\\t\\tUnclear what a user is defined as in the definition of large services. What does 7 million users mean - access to the site including unregistered users with links? Active users?': [{'label': 'POSITIVE',\n",
       "               'score': 0.9961452484130859,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Users from other countries? Approach to the Codes\\t\\tDoesn’t make sense to create a taxonomy of companies. Doesn’t make sense to create a taxonomy of companies - they constantly change and evolve. Approach to the Codes\\t\\tThe cost and time estimates are too low unless services have mitigations in place already. The cost and time estimates are too low unless services have mitigations in place already. Risks bankrupting small services. Content moderation (User to User) \\t\\tUnclear how this works on a global scale. Unclear how this works on a global scale - is Google supposed to censor content for UK users and not the rest of the world. UK cant dictate these things to the whole world. Content moderation (Search)\\t\\tUnclear how this works on a global scale. Unclear how this works on a global scale - is Google supposed to censor content for UK users and not the rest of the world. UK cant dictate these things to the whole world. Automated content moderation (User to User) \\t\\tUnclear how this works on a global scale. Unclear how this works on a global scale - is Google supposed to censor content for UK users and not the rest of the world. UK cant dictate these things to the whole world. Automated content moderation (User to User) \\t\\tHash matching doesn’t work. An image can be changed by altering one pixel so except for some limited instances, hash matching  is largely pointless. How does the UK government intend to force an overseas company to adhere to this when  the image may only be visible in the UK? Automated content moderation (User to User) \\t\\tCSAM URL detection measure is too costly for smaller services. CSAM URL detection measure is too costly for smaller services. Automated content moderation (User to User) \\t\\tNot for government to dictate how fraud is detected. Not for government to dictate how fraud is detected - measures are too prescriptive and don’t work UK only. Automated content moderation (User to User) \\t\\tNot for government to dictate how fraud is detected. Not for government to dictate how fraud is detected - measures are too prescriptive and don’t work UK only. Automated content moderation (Search) \\t\\tBetter to have offensive or hateful content out in the open so it can be challenged. Better to have offensive or hateful content out in the open so it can be challenged - this is best except in very extreme cases. User reporting and complaints (U2U and search)\\t\\tUnclear how this would work. Unclear how this would work- could lead to name calling and virture signalling. Terms of service and Publicly Available Statements\\t\\tService providers shouldn\\'t dictate what kind of content is allowed. Service providers shouldn\\'t dictate what kind of content is allowed/tolerated. Default settings and user support (U2U) \\t\\tRisks \"branding anyone that even attempts to talk to a child a paedophile\"\\tRisks \"branding anyone that even attempts to talk to a child a paedophile\"\\nDefault settings and user support (U2U) \\t\\tResponsibility is on the individidual to manage their own settings. Responsibility is on the individidual to manage their own settings, not for the state to dictate every threat via a legal Act. Default settings and user support (U2U) \\t\\tUsers should be informed of the risks associated with the proposed measures. Users should be informed of the risks associated with the proposed measures, such as blackmail and lifelong tracking. Recommender system testing (U2U) \\t\\tHow can companies separate the UK rules from their worldwide function. How can companies separate the UK rules from their worldwide function. Recommender system testing (U2U) \\t\\tOnline anonymity or at least obscuring of personal details such as passports, addresses etc is a way to improve user safety. Online anonymity or at least obscuring of personal details such as passports, addresses etc is a way to improve user safety. The proposals will mandate ID uploads which in turn will be linked to consent for “marketing” to “partners” so is dangerous. All users will be identified not just minors\\nEnhanced user control (U2U) \\t\\tMost companies offer this anyway, and commercial forces should solve this. Most companies offer this anyway, and commercial forces should solve this - eg  Twitter storm about certain content or features. User access to services (U2U)\\t\\tIf everyone needs to provide ID to all providers, this is a total loss of privacy and one user unfairly blocked is blocked from all services. If everyone needs to provide ID to all providers, this is a total loss of privacy and one user unfairly blocked is blocked from all services. Let the industry sort this and prosecute violations. Service design and user support (Search)\\t\\tToo prescriptive\\tToo prescriptive \\nCumulative Assessment\\t\\tNot proportionate - will cost the economy billions to implement, drive small businesses out of business, close down innovation and operation in the UK, impractical and unnecessary. Not proportionate - will cost the economy billions to implement, drive small businesses out of business, close down innovation and operation in the UK, impractical and unnecessary. Statutory tests\\t\\tEnforce the law don\\'t prescribe it. Enforce the law don\\'t prescribe it. Governance and Accountability \\t\\tFlags risk of third party-risk assessment. \"With any third party-risk assessment, there is a risk that a lack of depth of understanding of a\\nbusiness could influence any audit. This is a complex area, both due to the topic, as well as the range of services in scope. Even an\\nindependent third-party specialising in audits to mitigate and manage illegal content risks would\\nneed to develop an in-depth understanding of the working of the unique service in question. While services provide auditors with data and information for context, there can be significant\\ncomplexity due to the business product and structure of online services, which can be unique to\\neach service. This is especially so given the sheer diversity of online services in scope, and the\\nnuances in how they operate. Added to this, double-sided services (those servicing both businesses and consumers) can involve\\na further level of complexity as changes on one side of the service can have impacts on the other\\nside and these are not always easy to foresee or predict. By way of example, Trustpilot has experienced this first hand with external audits for the\\nFrench-based AFNOR certification, and also through our work with external consultants on\\nin-house projects. From these experiences, a significant amount of time is required for external\\nparties to explore the many moving parts which comprise our service, and this ultimately raises\\ncosts.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9915719032287598,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Ensuring consistency in this process is also critical. In other instances we have had experiences\\nwhere consistency has been lacking with regard to what different stakeholders are looking for and\\nexpecting through such a process. It is therefore key that well established and not overly\\nprescriptive requirements are set to deliver a consistent approach. It is vital that third party audits deliver quality and meaningful outputs. However, in light of our\\nexperiences, we underline the risk of such audits not delivering this due to the complexity of\\nonline services, and the cost required to ensure that third-parties have a suitable understanding of\\nservices (which are often complex and unique).\"\\nApproach to the Codes\\t\\tCommends approach considering size and functionality \\tTrustpilot commends the approach being taken in considering size and functionality when determining what measures apply to services. This is a significant step up from the approach being taken via the Digital Services Act (DSA) which focuses only on the size of services. This has been an area of concern with the approach being taken under the DSA, and raises questions as to whether focussing on size alone tackles risk as effectively as it could. Combining functionality with size takes a significant step forward in targeting risk.That said, we do have some concerns with regard to both how size is being established and how multi-risk services are defined. These concerns, and our suggestions for refinement, are set out in response to Q14 and Q15 respectively. Approach to the Codes\\t\\tChallenge definition of large service and user, proposes these are refined . \"We challenge how ‘large’ services are being determined and also how low the bar is being set to\\ndetermine whether a company is ‘multi-risk’. We would propose that these metrics are both\\nrefined. Our views on this are set out in response to Q14 and Q15.In Ofcom’s consultation documents, the approach to ‘users’ is defined in a number of key places, these include at: ● 6.10 - which sets out the questions used to identify the risk factors relevant to each group of characteristic for the “user base” ● 9.59 - where the rationale is set that “In general, all else being equal, the more users a service has, the more users can be affected by illegal content and the greater the impact of any illegal content. We have therefore proposed that services which reach certain user numbers should consider the potential impact of harm to be medium or high. ● 9.62 - where it is noted that the Service Risk Assessment Guidance noted that “in some instances the number of users may be a weak indicator of risk level.” And as such user numbers need to be considered alongside other risk factors. ● 11.58 - where the proposed definition for ‘large services’ is set. Throughout, it is clear that the aim of this guidance is to ensure that the impact of illegal content on individual users who are active users, engaging with content on services and thus at risk of encountering illegal content. Concerns regarding the definition of ‘user’ However, the definition of user (presented at 6.5) does not match this purpose. A user is defined in the following terms - “User base refers to the users of a service. A user does not need to be registered with a service to be considered a user of that service.” Registered users are a very important metric for services - helping services to track activity and take targeted approaches to developing their services, including in areas such as delivering product improvements and fixes. Expanding the definition of ‘user’ beyond registered users could lead to a very broad and onerous approach for services and platforms, including those which are lower risk. Overall, this very high level definition is incredibly broad meaning that anyone who clicks on a service would be counted as a user regardless of whether they interacted with any content on the service. For example, in the instance of Trustpilot, a user could accidentally click through to our service when scrolling on a different website page which displays one of our widgets, and then leave straight away. The user content we host is text based, as such a ‘click on, click off’ user would not be able to engage and process written content in order to be exposed to harm. However, despite the user not having engaged with the written user review content and thus not having had the potential to be exposed to any illegal content, they would count as a ‘user’ under this proposal. As this demonstrates, applying this broad approach across all services goes beyond the purpose of what the Act intends to tackle. Proposed new definition for ‘user’ To resolve this issue, we propose refining the definition of ‘user’ to be ‘active user’. This language is utilised in both the EU and Australia - jurisdictions which Ofcom has noted it’s alignment with in setting the bar for what is considered a “large service”. That said, while we believe that echoing the language on ‘active user’ is useful, we would call on Ofcom to use this in a more tailored manner - setting a reasonable and proportionate bar for what an active user is in this context. We would propose to determine that an ‘active user’ is “engaging at least once in [a set period of time] and being exposed to information for [a certain period of time - for example three or five seconds]”. The rationale behind this definition is that, firstly a user is considered to be an ‘active user’ and thus in the target of this Act - a user engaging with content and at risk of being exposed to illegal content. Secondly, the topic of whether said ‘user’ actually engaged with content and was indeed ‘active’ is also addressed in the form of having an opportunity to meaningfully and actively engage with content and thus be at risk of harm. With respect to what the time frame is set for how long a user must be on a service to digest the content, we suggest that this is linked to the type of content and the time frames are established via user testing. Our hypothesis being that a user accidentally clicking through to a service and clicking straight off could engage with picture content far faster than written content. Indeed, we understand that in Australia a framework based on a format of materials is being used in their approach to social media content - separating audio and text from other formats. This Act will cover over 100,000 vastly different services - the limitations of user numbers alone is already recognised by Ofcom.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9341973066329956,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Given this, a more nuanced approach is required to target the risk under consideration and this approach presents a nuanced and evidenced based way to do this. The bar for determining ‘large services’ We are concerned that the threshold for ‘large services’ is being set at 10% of UK users and that this is too low. This concern is heightened when the definition is coupled with the broad approach to measuring ‘users’. Our views on the definition of ‘users’ being set out above. If the definition of ‘user’ is kept as is, we strongly advocate that the 10% threshold is raised to take account of this. More broadly, we are concerned how broad the categorisation of ‘large services’ will be. While there is rightly a wish to ensure differentiation of services, classing a service with 10.1% of UK users in the same grouping as a service with over 50% of UK users seems out of proportion. If no further nuance is given to how services are categorised by size we would request that commitments are made to ensure that this definition is not copied for use in other policy and legislation. This request comes from experience in the EU, where the blunt ‘Very Large Online Platform’ definition is now being drawn on in relation to a range of policy proposals including in financial services which is completely inappropriate given the sectoral differences. Ofcom has recognised the limitation of purely looking at user numbers by coupling the size with functionality to determine risk, but we strongly caution against allowing this definition of size to be used as a basis of other policies and laws. Using resources efficiently A further point to raise is in relation to the mechanics of counting and reporting user numbers. From the perspective of a platform which is in the scope of both the Online Safety Act and the Digital Services Act we highlight an opportunity to avoid unnecessary duplication, if the broad definition of “users” is retained. Platforms, like Trustpilot, which are not classed as ‘Very Large Online Platforms’ under the DSA have to measure users via a dashboard which uses a six-month rolling average. With the DSA now in force, this dashboard has been created and is maintained by our Data Team. In Ofcom’s proposed approach, services will need to use a 12-month average of UK users per month. This will require additional resources across services to create an updated dashboard to capture and record the data in this manner. Given the 6-month rolling average of EU users vs the 12-month average of UK users has little material difference, we would suggest that Ofcom aligns with the DSA’s approach. This will enable services to use their resources as efficiently as possible - simply requiring a UK data set to be added to an existing dashboard, rather than requiring the creation of a new dashboard altogether. This would lessen the compliance burden for the majority of services without impacting the quality of the data which is being gathered. We fully recognise that Ofcom is enforcing the UK regime and this will differ to the EU’s regime. However, where there are opportunities to remove unnecessary duplications of workload for services, this should be taken, as in this case. “Large” services have varying levels of resource Further to this, we also draw Ofcom’s attention to the assertion made through the guidance regarding the resourcing of ‘large’ services. While large services have significantly more resources than small or micro services, when a comparison is made with those at the opposite end of the scale (namely, big tech), some large services are more akin to the former group of services. Given the bar for ‘large’ services is currently set at those with 10% of UK users (based on very basic definition), there could be a huge variation in both the nature and size of services which are labelled ‘large’. For example, a service with ‘user’ numbers equating to 10.1% of the UK population looks very different to a popular social media service, for example, which has ‘user’ numbers equating to over 50% of the UK population. It is critical to recognise that services which are not classed as ‘Category 1’ services in the UK but are considered ‘large’ are likely to have less resources than their larger counterparts. In this regard, it is important to note that multiple factors need to be recognised when considering resourcing: i) Firstly, services that have to comply with multiple regulatory regimes need to deploy their limited resources efficiently and effectively in order to maintain their ability to innovate and compete with larger incumbents. ii) Secondly, even if lower-risk services are able to justify differentiation to fulfil their duties under the Online Safety Act and explain any deviation from the recommended guidance measures, they must still follow the overall process, including via documentation and regular reassessments, all of which requires resourcing. iii) A further layer of complexity is added where rules consider some services more directly than others, for example social media. Whilst direct application has perhaps been focussed on one type of service due to risk level or prevalence, for other services - such as review services - direct application may not be as straightforward as envisaged, requiring more resources to be dedicated to interpreting and applying it. Against this backdrop, for many services there will be a trade off on a sliding scale between regulatory compliance and innovation.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9919284582138062,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It is therefore important that all required changes are carefully weighed, as even small changes can impact innovation. Overall, Trustpilot supports efforts to improve online safety, and the implementation of this Act is very important. To maximise the benefits and minimise unintended consequences, it is important to ensure that the application of this part of the Act does not become an academic exercise of justification and differentiation.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9939684867858887,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Otherwise, it risks favouring the largest and most well-resourced services - those with access to the most extensive legal and data resources to deploy. Such an outcome would be of detriment to competition and to services which fall at the lower end of the ‘large’ category. Likewise, it is important that unnecessary impacts on innovation are also avoided.\"\\nApproach to the Codes\\t\\tChallenge definition of multi-risk and user, proposes these are refined . \"We challenge how ‘large’ services are being determined and also how low the bar is being set to\\ndetermine whether a company is ‘multi-risk’. We would propose that these metrics are both\\nrefined. Our views on this are set out in response to Q14 and Q15.In Ofcom’s consultation documents, the approach to ‘users’ is defined in a number of key places, these include at: ● 6.10 - which sets out the questions used to identify the risk factors relevant to each group of characteristic for the “user base” ● 9.59 - where the rationale is set that “In general, all else being equal, the more users a service has, the more users can be affected by illegal content and the greater the impact of any illegal content. We have therefore proposed that services which reach certain user numbers should consider the potential impact of harm to be medium or high. ● 9.62 - where it is noted that the Service Risk Assessment Guidance noted that “in some instances the number of users may be a weak indicator of risk level.” And as such user numbers need to be considered alongside other risk factors. ● 11.58 - where the proposed definition for ‘large services’ is set. Throughout, it is clear that the aim of this guidance is to ensure that the impact of illegal content on individual users who are active users, engaging with content on services and thus at risk of encountering illegal content. Concerns regarding the definition of ‘user’ However, the definition of user (presented at 6.5) does not match this purpose. A user is defined in the following terms - “User base refers to the users of a service. A user does not need to be registered with a service to be considered a user of that service.” Registered users are a very important metric for services - helping services to track activity and take targeted approaches to developing their services, including in areas such as delivering product improvements and fixes. Expanding the definition of ‘user’ beyond registered users could lead to a very broad and onerous approach for services and platforms, including those which are lower risk. Overall, this very high level definition is incredibly broad meaning that anyone who clicks on a service would be counted as a user regardless of whether they interacted with any content on the service. For example, in the instance of Trustpilot, a user could accidentally click through to our service when scrolling on a different website page which displays one of our widgets, and then leave straight away. The user content we host is text based, as such a ‘click on, click off’ user would not be able to engage and process written content in order to be exposed to harm. However, despite the user not having engaged with the written user review content and thus not having had the potential to be exposed to any illegal content, they would count as a ‘user’ under this proposal. As this demonstrates, applying this broad approach across all services goes beyond the purpose of what the Act intends to tackle. Proposed new definition for ‘user’ To resolve this issue, we propose refining the definition of ‘user’ to be ‘active user’. This language is utilised in both the EU and Australia - jurisdictions which Ofcom has noted it’s alignment with in setting the bar for what is considered a “large service”. That said, while we believe that echoing the language on ‘active user’ is useful, we would call on Ofcom to use this in a more tailored manner - setting a reasonable and proportionate bar for what an active user is in this context. We would propose to determine that an ‘active user’ is “engaging at least once in [a set period of time] and being exposed to information for [a certain period of time - for example three or five seconds]”. The rationale behind this definition is that, firstly a user is considered to be an ‘active user’ and thus in the target of this Act - a user engaging with content and at risk of being exposed to illegal content. Secondly, the topic of whether said ‘user’ actually engaged with content and was indeed ‘active’ is also addressed in the form of having an opportunity to meaningfully and actively engage with content and thus be at risk of harm. With respect to what the time frame is set for how long a user must be on a service to digest the content, we suggest that this is linked to the type of content and the time frames are established via user testing. Our hypothesis being that a user accidentally clicking through to a service and clicking straight off could engage with picture content far faster than written content. Indeed, we understand that in Australia a framework based on a format of materials is being used in their approach to social media content - separating audio and text from other formats. This Act will cover over 100,000 vastly different services - the limitations of user numbers alone is already recognised by Ofcom.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928677082061768,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Otherwise, it risks favouring the largest and most well-resourced services - those with access to the most extensive legal and data resources to deploy. Such an outcome would be of detriment to competition and to services which fall at the lower end of the ‘large’ category. Likewise, it is important that unnecessary impacts on innovation are also avoided.\"\\nApproach to the Codes\\t\\tConcern reagrding lack of rationale for bar for mulit-risk and proposal to change it. \"Our concerns regard:\\ni) The lack of rationale for setting the bar for ‘multi-risk’ services\\nIn setting out its approach to categorising firms as ‘multi-risk’ there is a lack of evidence or\\nrationale provided by Ofcom as to why the threshold is being set at two ‘medium’ or ‘high’ risks\\ndesignations from the 15 types of priority offences which constitute a service being deemed\\n‘multi-risk’ service. As set out below, we believe that this should be refined, however if it is not and the threshold\\nremains at this level, Ofcom should set out the rationale for this level being appropriate. ii) The level of the bar for being a ‘multi-risk’ service\\nOur view is that setting the bar for a ‘multi-risk’ service at two medium or high risk categories\\nbeing identified from the 15 types of priority offences, is too broad and too low. For example, a review service could have in place systems and processes to reduce the harm of\\nhate speech by filtering out this content before it is published. Yet, in the event that the system\\ndoesn’t catch every single instance (no system is perfect) and those cases are flagged and reported\\nby users, this would be counted as ‘medium risk’ under the process being proposed. This is on the basis that one of the thresholds to assist services in determining their risk level (as\\nset out in Table 6 of Annex 5), sets out that a service would be considered “medium risk” on the\\nbasis that “You assess that there is a moderate likelihood that this illegal harm could occur on your\\nservice” and a condition that meets this is if “There is evidence that harm is likely to occur on your\\nservice” or “There is some evidence of harm taking place on your service”. The drafting in Table 6 of Annex 5 is very loose and leaves pretty much all services open to\\nmeeting these very low bars. A single piece of evidence would count as evidence of harm\\noccurring and, by virtue of a user flagging it, an argument could be made that harm was\\nexperienced. We strongly suggest that tighter wording is required to create a meaningful\\nthreshold. This approach also underlines the issue with how ‘multi risk’ is being defined with the same\\nweighting being given to medium and high risk categories when defining whether a service is\\n‘multi risk’. That a review service which has a handful of instances of written reviews containing hate speech\\nbeing flagged - having slipped through their systems - and swiftly taken down, is given the same\\nweighting under this framework as a site which say, regularly exposes users to CSAM content\\nwhich can go viral and be pushed through recommender systems seems out of balance and\\ndisproportionate in the context of the goals of this Act. Added to this, we note that whilst there will be an obligation on services to comply with this Act,\\nthere is also a requirement to uphold freedom of expression under Article 10 of the Human Rights\\nAct. It is therefore critical that the approach to implementing and enforcing the Online Safety Act\\ndoes not result in users\\' speech being over censored by services. Our understanding is that Ofcom expects a low number of services to be categorised as multi-risk. However, with the definition set so broad and the criteria arguably very low, we would envisage\\nthat many more companies would actually categorise themselves as being ‘multi-risk’ under the\\ncurrent proposals and we are not convinced that this will necessarily result in risk being tackled\\nmost effectively. iii) Unintended negative impacts of the proposed approach\\nAdded to this, in setting the threshold at the low level of two medium risks or above, there is a risk\\nthat services prematurely divert their resources from mitigating measures to target the two\\nmedium risks in question to reduce the risk level, to putting in place broader, multi-risk\\nobligations. Where a company has four or five medium or high different risks there is a clear benefit in\\nadhering to cross-cutting multi-risk obligations. Yet, for just two medium risks, the advantages\\nseem less clear and, indeed, the ability to target those risks is arguably watered down - a counter\\nintuitive outcome. Ultimately, to impactfully identify and target risk through this framework, the ability to effectively\\ndifferentiate risk and services is key. We are concerned that the proposed application framework\\ndoes not deliver this. iv) Potential bias to the largest and best resourced firms\\nAs it stands, a company deemed ‘large’ which is at the smaller end of the “large” scale, could\\nconduct a conscientious and subjective assessment and be placed in the same space as a much\\nlarger service which has a vastly different risk profile. What is more, the factor of resource arguably then becomes a key factor (a risk factor in other\\nparts of this approach too). Services with extensive resources (often the largest firms) can harness\\nthese to minimise how much stringent requirements impact them. Ofcom’s approach should deliver meaningful action, but there is a clear risk that firms with more\\nresources could target these on justifying and documenting their assessments in a manager which\\ngives them more conscience to downplay their risk as ‘low’. Proposed solution\\nUltimately, we want an approach where risk is tackled most effectively, where services and risk are\\nmeaningfully differentiated and where services with abundant resources can’t take advantage of\\nthe system in place and minimise their compliance simply due to their resources, in a way that\\nsmaller companies cannot. To address these concerns, we propose that the threshold for multi-risk is raised to require either:\\n● The threshold for being categorised as ‘multi-risk’ is raised to four medium or high risk\\ncategorisations, with at least two required to be high; or\\n● A grading or points system is introduced to differentiate between levels of harm. This\\nwould enable the process to take into account levels of harm and have services\\ncategorised accordingly. The differentiation could be based on either evidence of what\\ncategories cause the most severe online harm or the level of sentencing in relation to the\\nsentences for culprits of these harm using the hierarchy established by the application of\\nthe law to date. Both options would provide an evidence based hierarchy from which to\\nrefine Ofcom’s approach.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9903401732444763,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Using this framework, a suitable level can then be set for what\\naccumulation accords to a service being ‘high risk’. The refinement of this approach is especially key if Ofcom retains its approach to designating the\\nsize of a platform based on a blunt definition of ‘users’ (our concerns with regards to this are\\nraised in response to Q15). If this approach is retained, then the role of functionality becomes\\neven more important and the benefits for creating clearer differentiation are heightened in order\\nto accurately target the largest risks.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924029111862183,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Regardless, there is a strong case for this to be refined. \"\\nRecommender system testing (U2U) \\t\\tEvidence for risk profiles but helpful for rec system codes -\\t\"Recommender systems\\nWhen considering recommender systems, Volume 2 of the guidance points to two specific\\nrecommender systems which are considered as potentially risky. These are:\\n● Content recommender systems\\n● Network recommender systems\\nAs described in Volume 2, these systems - and effects - are most commonly found on social media\\nservices. While honing in on these two types of recommender systems is reflected somewhat in the Risk\\nProfiles, it is important to retain the specificity and this could be clearer than it currently is (the\\ntitle in the Risk Profiles suggested a far broader scope “8 Services with recommender systems”). The Online Safety Act is estimated to cover over 100,000 online services. Within these, a number\\nof services would not have content recommender systems or network recommender systems as\\ndefined in Volume 2, however would have systems which could fall under the much broader\\numbrella of general “recommender systems” which is denoted in the Risk Profile title (“8 Services\\nwith recommender systems”). The ambiguity of this title is therefore unhelpful and could result in\\nservices with more banal recommender systems having to give attention to this area when they\\nare not the intended targets, arguably diverting critical resources away from tackling relevant\\nareas of risk. As Ofcom itself recognises, not all recommender systems carry the same level of risk and it is the\\nspecific types of recommender systems which are raised in the guidance which are relevant. It is\\ntherefore important that this delineation is also recognised in the Risk Profiles too. We therefore recommend that the types of recommender systems which are being targeted\\nshould be better reflected in the U2U Risk Profiles. For example, in Table 14 of Annex 5, “8 Services\\nwith recommender systems” could be refined to be “8 Services with specific types of\\nrecommender systems”. This alignment would be a clearer signal for U2U services that have other types of lower-risk\\nrecommender systems. Enabling them to more efficiently address this part of the risk assessment\\nand dedicate their resources to areas of relevant risk for them. We broadly agree with the types of services that the Risk Profiles focus on. It is understandable that not all types of service can, or indeed should, be considered at an in-depth level, given the diversity of services in scope. From analysis of our reviews and reviews flagged by consumers and businesses, we believe that many types of illegal harm in the Online Safety Act are highly unlikely to be prevalent on our service, due to the nature of online reviews. As such, this justifies not addressing online review services directly as a sub-type of service within the guidance. Given this context, much of the insight that a service like Trustpilot gains from the risk factors is indirect. The approach taken is useful to understand which other types of services are likely to have such risks, and to understand how those risks manifest themselves in such examples. We are then able to utilise our own understanding of the differences between online review services and those other types of services listed in the Risk Profiles to compare and contrast this information. That said, we do think an improvement can be made with regard to the area of pseudonymity.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9846546649932861,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is set out below. \"\\nAutomated content moderation (User to User) \\t\\t\"Costs of applying our articles for use in frauds (standard keyword detection) measure,\\nincluding for smaller services;\"\\t\"With respect to standard keyword detection as discussed in Volume 4, Chapter 14 - we would note\\nthat our experience determines that keyword detection is only partly effective in this context and\\nwould require constant moderation and human input. Standard keyword detection has a number of positives as well as negatives, which include the\\ngeneration of false positives, limited ability to understand the context in which the keywords\\nwhich were used, a need for keywords to constantly be updated and the ability of scammers to\\nidentify the use of key words and deploy attempts to circumvent them and. In considering fraud and financial offences, we would agree that keyword detection would not be\\nsufficient to tackle offences. Keywords may be a good start if services have nothing in place, or\\nlimited technician resources, but more developed solutions are required to avoid a\\n“whack-a-mole” approach. At Trustpilot, we use more developed solutions such as AI and machine learning which are\\narguably far more effective and specialised than keyword detection. We therefore agree with Ofcom’s approach to not proposing keyword detection technology as a\\npanacea to the issue identified.\"\\nGovernance and Accountability \\t\\tGovernance & accountability Codes are not suited for decentralised services. \"We agree with the concept of governance and accountability to oversee the performance of illegal content duties. However, the proposed approach fails to consider a rapidly growing subset of online services built around decentralised architectures. As a result, the proposed governance and accountability are ill-suited for the wide range of online services that UK users may encounter. he proposed approach fails to consider and address decentralised online services. Under this approach, centralised governance would be ineffectual as the host of any decentralised service would have little to no autonomy over them. The micro-blogging service Mastodon is an example of a decentralised online service, offering users the opportunity to either join or host their own servers, which act as micro-environments for content sharing. Any control over the operation and moderation of a server would fall on the server owner, which can range from organisations to individuals. The German non-profit organization that develops the Mastodon software is clear that it is unable to remove servers that share harmful content given the nature of the service, instead choosing a de-prioritisation/prioritisation approach to signpost users to servers that have undergone a vetting process. This vetting process involves a review of the server against the Mastodon Server Covenant. Decentralised online services do not yet have the level of popularity as centralised ones, but the situation could change quickly with the right service. The EU Blockchain Forum’s recent report states that “[a]lthough the adoption of decentralised social media applications has not yet reached massive levels, it is evident that user awareness and migration from centralised to decentralised platforms are noteworthy. Decentralised social media has firmly established itself as a legitimate and valid alternative to the heavily centralised incumbent social media landscape. As these platforms mature, their appeal and user base are likely to grow, further reinforcing their credibility as a viable option in the digital space.”   If Ofcom wants to create future forward proposals, it should fully consider the governance and accountability of decentralised systems. \"\\nGovernance and Accountability \\t\\tAgree on approach if limited to centralised systems. Given that the proposals assume a centralised governance scheme, we agree with the types of services, provided that these types of services are limited to centralised systems. Ofcom would need to develop other proposals to cover the unique framework of decentralised architectures. Approach to the Codes\\t\\tOfcom should apply the most onerous measures to services that pose a high risk of causing harm, regardless of their size. We believe Ofcom should apply the most onerous measures to services that pose a high risk of causing harm, regardless of their size. As Ofcom correctly notes, “Where risks are very high, it is important that people are afforded protection even when the services they are using are relatively small.” However, Ofcom should not apply the most onerous measures to services simply because they are large. Just because a service is large that does not mean that it has the resources to cover costs for onerous measures.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9800547361373901,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'For example, Twitter (now X) famously operated with massive losses for several years. Approach to the Codes\\t\\tCounting users for large service is not simple on decentralised service. We agree that Ofcom should use a simple definition for large based on the number of users (ideally active monthly users), not metrics such as number of employees or revenue. However, counting the number of users of decentralised services is not straightforward. For example, the total number of users of an interoperable social network may exceed the threshold for a large service, but these users may be distributed across different service providers. Ofcom should provide clear guidance on how this threshold would apply to decentralised services. Approach to the Codes\\t\\tDo not agree with multi-risk definition\\tWe believe this definition does not allow for any meaningful distinction between different types of online services. The definition of multi-risk services will likely apply to most covered online services because bad actors can use these services for more than one type of illegal offense and Ofcom has set the threshold at two potential online harms. For example, any online service that has direct messaging capabilities poses a risk for drug offences and intimate image abuse. It would therefore be more meaningful to use a higher threshold or remove the distinction altogether. Approach to the Codes\\t\\tAgree with no taking one-size fits all approach, but doesn’t seen Ofcom proposals at future proof\\t\"We agree with not taking a “one size fits all” approach. However, Ofcom only applies this principle to user-to-user services and search services. But its definitions of these services are not future proof. For example, Ofcom does not consider other methods of information retrieval, such as AI chatbots, that consumers may substitute for traditional search engines. Use of AI chatbots such as ChatGPT as the first source for information retrieval is increasing, as well as the integration of generative AI solutions into existing search engines such as Copilot in Bing. The current proposals split online services into either user-to-user or search services, for which neither appropriately capture the use of generative AI-as-a-service for gathering information. The taxonomy of search services looks at either general or vertical search services, and Ofcom does not consider this new form of information retrieval in its definitions. Ofcom should strive to create technology-neutral rules that neither favour nor penalize the use of emerging technology like AI. The taxonomy of search services should consider a third category for systems that are unable to provide real-time information based on indexing but can nonetheless offer a source of information similar to a search engine, either based on historical training data and/or Retrieval-Augmented Generation (RAG).\"\\nContent moderation (User to User) \\t\\tAgree with some proposals and approach but have questions on enforcement and KPIs\\tContent moderation involves both reprioritisation of the recommender algorithm, as well as the take down of illegal content under the Act. Therefore, Ofcom should bear in mind that the KPIs will differ depending on how a service under approach 3 develops its content moderation system. Whilst the main KPIs in content moderation tend to be speed of take down and accuracy of identifying illegal content, the proposals should allow other KPIs for alternative content moderation systems, which may prefer prioritisation and the efficacy of the recommender algorithm to de-prioritise potentially illegal content. Content moderation (User to User) \\t\\tSmaller services with specific risk should have measures apply to them. \"Response: We agree with Ofcom that given the diverse range of services in scope of the new\\nregulations, a one-size-fits-all approach to content moderation would not be appropriate. Nonetheless, we believe that the number of risks should not affect the level of management of the risk. We would therefore suggest that given the risks arising from certain high severity harms even smaller services with a specific risk should:\\n- Set and record internal content policies. - Set and record performance targets\\n- Prepare and apply a policy about the prioritisation of content for review\\n- Resource its content moderation function\\n- Ensure people working in content moderation receive training and materials that enable\\nthem to moderate content effectively\"\\nAutomated content moderation (User to User) \\t\\tMeasure to detect new CSAM by applying age assurance. \"We would suggest that alongside hash matching, a requirement to detect potential\\nnewly generated CSAM is included by applying automated age assurance. This would identify\\nexplicit content where anyone seems to be underage and would work alongside age verification\\nfor all individuals involved in the content. If someone in the content is flagged to be underage,\\nthere should be evidence available to confirm they are actually 18 or above\"\\nDefault settings and user support (U2U) \\t\\t\"Challenges age assurance approach - Ofcom should encourage prompt adoption and implementation by\\nas many services as possible.\"\\t\" We agree with Ofcom that child user accounts need some dedicated default settings\\nand protections. This is why we think there is a missed opportunity if these\\nrequirements/measures only “apply to the extent that a service has an existing means of\\nidentifying child users and would apply where the information available to services indicates that a user is a child”. Robust age assurance solutions already exist with a wide-range of options available to estimate the age of a user. Ofcom should encourage prompt adoption and implementation by as many services as possible.\"\\nEnhanced user control (U2U) \\t\\tSupport measures and wants them extended to all child user accounts on small services with specific risk. \"We welcome measures dedicated to empower users and give them more control. We\\nbelieve that for child users accounts the requirements should be extended also to smaller services where there is a specific risk. This would also mean ensuring that services have the right tools in place to estimate the age of their users and put in place protections accordingly\"\\nGovernance and Accountability \\t\\tBelieves we should recommend external audit as services shouldn’t \"mark their own homework\"\\t\"In the section on internal assurance and compliance functions, Ofcom rejects options\\nthat include elements of external audit or scrutiny in favour of those that rely on\\ninternal assurance mechanism. The main basis on which Ofcom rejects options that\\ninclude external scrutiny is uncertainty over cost. We believe that platforms should not mark their own homework, and that there is\\nsignificant risk in delaying the introduction of any future plan to require services to\\nhave measures to mitigate and manage illegal content risks audited by an\\nindependent third-party. While we agree that all platforms should ensure that they\\nhave established appropriate internal governance frameworks including internal\\nmonitoring and assurance functions, it is not credible to describe or consider these\\nas independent. It is important that Ofcom ensures that there are genuinely\\nindependent monitoring, reporting and assessment processes. We agree that the risk of bias is mitigated to some extent by the framework of\\ngovernance and accountability proposed by Ofcom, which we strongly support. Nevertheless, we believe that it would be unrealistic to assume that incentives for\\nemployees could be so effectively demarcated as to render them genuinely\\nindependent. We note that Institutional Shareholder Services (ISS) uses a number of\\ncriteria to assess the independence of a member of a UK company’s Board of\\nDirectors5\\n. The first of these criteria is that for a director to be considered\\nindependent, they must not have been an employee of the company or group during\\nthe previous five years. This is a relevant parallel, we believe, and puts into context\\nwhat we see as an overly hopeful expectation of independence in Ofcom’s proposal,\\nnotwithstanding appropriate governance arrangements. We have also provided proposals below as to how to establish appropriate and costeffective monitoring, reporting and assessment processes in response to other\\nVolumes. We believe that our proposal for credible, independent, third-party scrutiny is\\ncommensurate with standard auditing requirements for all companies and, for\\nexample, with prudential regulation requirements for large financial services\\ninstitutions. We cannot see any reason to delay implementation of this requirement. We do not believe that Ofcom’s stated reason for not pursuing options which include\\nexternal scrutiny – that it does not understand the costs of implementing them – are\\nparticularly persuasive. This is especially the case since it has not assessed the\\ncosts of other options. To the extent that this is the main or only reason Ofcom is not\\nminded to take forward such proposals, we believe it is incumbent on Ofcom to\\nobtain such cost information. 5\\nISS - Institutional Shareholder Services, 2024; Proxy Voting Guidelines, Benchmark Policy\\nRecommendations, United Kingdom and Ireland;\\nhttps://www.issgovernance.com/file/policy/active/emea/UK-and-Ireland-Voting-Guidelines.pdf\\nFinally, we would stress that the costs of these audits would be borne by the\\nplatforms, and not by Ofcom. Nonetheless, we would not expect these costs to be\\ndisproportionate set against the substantial revenues and profits generated by the\\nplatform industry. Moreover, platforms have benefited from excess revenues and\\nprofits that have been generated through hosting the very illegal and harmful content\\nthat the Online Safety Act seeks to eradicate.\"\\nUser reporting and complaints (U2U and search) \\t\\tWants periodic reporting of complaints and actions from platforms to ofcom. \"In Chapter 16 of the Consultation, Reporting and complaints, Ofcom deals with\\nreporting of complaints to the platforms. We underscore in our response to Volume 6\\nof the Consultation below the need for comprehensive periodic reporting of\\ncomplaints and actions from the platforms to Ofcom. We have not seen a proposal\\nfrom Ofcom on the need for such periodic reporting, nor any comments relating to\\nsuch periodic reporting.\"\\nUser reporting and complaints (U2U and search) \\t\\tWants Trust Flaggers scheme expanded to hate\\t\"On reporting from users to platforms, we agree that it is appropriate for Ofcom to\\nestablish dedicated reporting channels (DRCs) for ‘trusted flaggers.’ In paragraph\\n16.242 of the Consultation, Ofcom proposes that there be a maximum of 7 trusted\\nflaggers, and that this number relates to cost efficiency when developing and\\nmaintaining the reporting function. These trusted flaggers are named: HM Revenue\\nand Customs (HMRC), Department for Work and Pensions (DWP), City of London\\nPolice (ColP), National Crime Agency (NCA), National Cyber Security Centre\\n(NCSC), Dedicated Card Payment Crime Unit (DCPCU), and the Financial Conduct\\nAuthority (FCA). While we are not best placed to comment on issues relating to other minority groups,\\nwe would note that antisemitism and antisemitic content are often characterised by\\nthe use of subtle formulations, nuances, conspiracy theories, tropes and conflations. We believe that none of the designated trusted flaggers identified by Ofcom has\\ndemonstrated a sufficiently thorough understanding of antisemitic content, and as\\nsuch, cannot be replied upon to appropriately identify and report such content to the\\nplatforms. There should therefore be more than 7 trusted flaggers designated by Ofcom. We believe that for complex and multi-faceted categories of hate crime, among\\nwhich we would include antisemitism, civil society bodies such as ourselves will need\\nto play a crucial role. The issues will require bodies with the ability to identify illegal\\nor harmful content, and to educate other trusted flagger organisations and the\\nplatforms themselves. Prerequisite for this will be deep expertise in monitoring,\\nlitigating and educating which a limited number of organisations have. It is instructive that the CAA has been designated a trusted flagger by certain\\nplatforms. We therefore have direct knowledge of the value that we can bring to the\\nprocess as trusted flaggers, and of the problems that we solve. An important issue to\\nhighlight is that government bodies (such as those mentioned as designated trusted\\nflaggers by Ofcom) have frequently shown that they are unacceptably slow in their\\ncommunication of illegal content to the platforms, resulting in posts remaining online\\nfor far too long. In order to be effective, a trusted flagger needs to be able to flag\\nillegal content quickly. Moreover, the ability to recognise emerging patterns of illegal\\ncontent, and to understand developing tropes, is far more honed among those\\norganisations that specialise in understanding specific categories of content, such as\\nantisemitism. Moreover, opening up the field of trusted flaggers would fulfil any objectives to\\ninvolve civil society organisations and institutions. This would bring to bear more\\nculturally diverse perspectives from religious and other special interest and minority\\ngroups which can often better understand and reflect the interests of the\\ncommunities and groups that they represent. We do not believe that there would be any material incremental cost to Ofcom, and\\nreinforcing platforms’ ability to effectively remove illegal content will likely\\nconsiderably reduce the cost of enforcement for Ofcom.\"\\nUser reporting and complaints (U2U and search) \\t\\tTrusted Flaggers: a single common standard for a set of application programming interfaces for reporting be devised by a working group\\t\"In an effort to further reduce costs for the industry, we believe that Ofcom’s intimation\\nthat each of the 7 trusted flaggers would need to engage with each relevant platform\\nfor reporting risks being wasteful and could be anachronistic. If 7 trusted flaggers\\nwere to each establish DRCs with, say, 6 platforms, then that would require 42\\nbilateral arrangements to be designed, agreed, and implemented at some\\nconsiderable cost, presumably to be incurred by both the platforms and the public\\npurse (the trusted flaggers). We therefore propose what we believe would be a much\\nmore cost-effective solution for the industry that would simultaneously enable a\\ngreater number of trusted flaggers to be designated by Ofcom, and which would\\ninvolve only a single standard for all such relationships. We propose that a single, common standard for a set of APIs (application\\nprogramming interfaces) for reporting be devised by a working group. The working\\ngroup could be responsible for setting standards for the sharing of information the\\nplatforms and trusted flaggers. In reference to our proposal with respect to Volume 6\\nof the Consultation, we further propose that this group could set standards for\\nreporting of the incidence and treatment of illegal and harmful content from the\\nplatforms to Ofcom. Ofcom may wish to oversee this working group. The concept would be akin to Open Banking, an UK initiative that has established a\\nworld-leading framework for setting standards for data sharing within banking and\\nother industries. Open Banking was proposed in the Fingleton Report (Data Sharing\\nand Open Data for Banks6\\n) which was commissioned by HM Treasury and the\\nCabinet Office in 2014 as it sought to improve competition between banks and to\\nlower barriers to entry to the banking industry. The Competition and Markets\\nAuthority imposed Open Banking by way of an order following the Retail Banking\\n6 Data Sharing and Open Data for Banks -‐ A report for HM Treasury and Cabinet Office;\\nSeptember 2014, Fingleton Associates and the Open Data Institute. Market Investigation of 2017. The standards were ultimately agreed by a working\\ngroup that was later named the Open Banking Implementation Entity (OBIE). In a similar way, a single set of open, or non-proprietary standards could be agreed\\nfor online platforms in such a way as to contain costs for the entire industry, ensuring\\nthat the way data is shared between organisations is designed and agreed only once\\nfor all participants. Costs for smaller platforms could be set such that they would be\\nproportionate to their size, or possibly de minimis, and would therefore not add\\nadditional or prohibitive cost burdens to smaller platforms. It therefore removes a\\nbarrier to entry for small and potentially disruptive new platforms, and so supports\\nOfcom’s competition brief.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9917885661125183,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Future iteration and amendments to the standard could\\nalso be discussed, agreed and implemented across the board (rather than 42 times\\nin the example above).': [{'label': 'POSITIVE',\n",
       "               'score': 0.9504469633102417,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The fact that Open Banking was possible in the banking industry, itself riddled with\\nlayer upon layer of legacy systems, and with monolithic decision-making structures,\\nimplies that younger and more technology-forward online platform industries should\\nbe easily capable of developing a functional framework. We believe that a single\\ncommon standard for APIs would encapsulate the concept of DRCs, and thus would\\nnegate the need to establish so many, and the costs of doing so.\"\\nApproach to the Codes\\tDefinition of large services\\tThere should be close coordination between EU and UK governments to avoid conflicting standards between OSA and DSA implementation\\tWe encourage close coordination between the EU and UK governments to avoid conflicting rules as the EU’s Digital Services Act (DSA) and the UK’s Online Safety Act (OSA) are implemented… Organizations such as the Digital Trust & Safety Partnership, and the Trust and Safety Professional Association have published a glossary of terms and definitions to ensure shared vocabulary among a variety of public and private stakeholders. A baseline knowledge of terminology in a growing Trust & Safety ecosystem will benefit smaller companies that are new to this field.\\'\\nCumulative assessment\\tBurden on low-risk small and micro businesses\\tUS DoJ etc support our prioritisation of the needs of small and micro services and wish to understand further how Ofcom plans to support these services overcome financial barriers to market\\tWe support Ofcom’s proactive approach in prioritizing the needs of startups/small and micro businesses throughout the implementation of the OSA. We are pleased to see that Ofcom is not only acknowledging the importance of these entities but are actively recognizing these companies specific economic and regulatory hurdles during the implementation and process... By recognizing the unique challenges and opportunities faced by this segment of industry, Ofcom demonstrates a commitment to fostering a diverse and thriving marketplace, without compromising innovation... While we recognize that it is difficult to fully quantify the cost of complying with the OSA, understanding how Ofcom looks to help support service providers (with minimal resources) overcome financial barriers to market is of significant interest to us. Technical solutions to comply with OSA require significant upfront cost to implement. We recommend the following December 2021 publication on Startups and Content Moderation by Engine, a 501(c)(3)/(4) non-profit organization that works with thousands of startups across the country to advocate for pro-startup, pro-entrepreneurship policy. This primer explains how startups moderate their services and how that differs from mid-sized online service providers. For example, startups have limited resources to moderate content on their sites, but they spend more per user than mid-sized content-hosting companies.\\'\\nCumulative assessment\\tBurden on high-risk small and micro businesses\\tUS DoJ etc. support our approach\\tWe commend Ofcom for its commitment to providing special attention to startups/small and micro businesses, recognizing that they may pose specific and significant risks to the digital ecosystem and its users. By prioritizing assistance over immediate penalization, Ofcom demonstrates a proactive approach that fosters a conducive environment for these businesses to thrive while ensuring compliance with regulations.\\'\\nCumulative assessment\\tBurden on larger services\\tUS DoJ etc. support our approach\\tWe recognize that large services with an expansive user may be better equipped to shoulder compliance costs of implementation. We understand that without allocating additional financial resources and efforts towards compliance, companies may struggle to commit to comply with the Online Safety Act.\\'\\nRecommender system testing (U2U) \\t\\tRegister of Risks answer but helpful for this measure - Recommender systems and role in FIO\\tVol. 2 Foreign Interference Offence (FIO).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9640923738479614,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '6P. Recommender systems are being exploited by Foreign Intelligence Services (FIS) and by sub-state proxies to disseminate content. The framing of the argument against regulation of the current recommender algorithms is potentially flawed. A valid and important framework in the context of the Online Harms Act (and other contexts – including the transnational/international dimensions) is why content is being pushed by social media platforms or placed in front of users. It is almost as important as what content is being pushed or placed in front of users because it can be stopped or minimised at source or be used for Cyber Threat Intelligence (CTI). Based on measurable studies conducted by journalists, academics, and whistleblowers it appears clear that content is being pushed by social media platforms, or placed in front of users, prompting them to engage with that content. This potentially/actively includes FIS messaging. FIS messaging seeks to negatively influence social cohesion and political systems especially on socially or politically divisive issues and/or promote views of the world at odds with those of HMG. 1 Whilst these activities are international in scope, these sets of issues affect the UK and this section of the Online Harms Act, and the Foreign Interference Offence (FIO) are valuable and needed actions. 2 Generative AI and deepfakes are part of FIS messaging and part of a wider set of difficult issues. Increasing media literacy is helpful in this and other respects but is not an answer in itself.3 Improving media and digital literacy are two key components in resilience against FIOs/influence operations, allied to the promotion of critical thinking skills through education programs.4 \\nEnhanced user control (U2U) \\t\\tRegister of Risks answer but helpful for this measure - Disputes inclusion/framing of FIO in verifcation measure\\t20. Enhanced User Control.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992561399936676,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Foreign interference/political influence campaigns should not be conflated with other types of online harms like fraud, however. An actor/intent model is perhaps a better (though also potentially flawed) approach. User verification schemes can be fooled or circumvented – especially by FIS and sophisticated cybercriminals/cybercriminal gangs. The latter are increasingly organised, motivated, and professionalising their activities and business model.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9960237741470337,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This includes through ransomware. In Russia especially, they have been known to work or be coopted into working for/with FIS.5 The way the Foreign Influence Offense (FIO) has been framed appears abstracted as though it has not yet occurred. There are detailed studies (including work conducted by the Oxford Internet Institute – OII –, Cambridge Disinformation Lab, the EUvsDisinfo initiative, and a US Senate Select Committee report in 2017) as well as UK government reports that demonstrate the material reality.6 Intelligence-led foreign political influence campaigns are not new but what is new is coordinated state intelligence use and uses of social media for strategic messaging against Western states including the UK (especially by Russia, China, and to a lesser extent Iran). Since renewed Russian election interference operations first began in 2004 upwards of 38 nations have been targets spanning four continents: Europe, North America, Af rica, and Asia. There are also suspicions that Russia’s Wagner Group has conducted operations in Latin America.7 As well as cyberespionage/information warfare influence campaigns, money has been covertly siphoned to foreign political parties, officials, and politicians. Some of this remains undetected.8 These too are part of active measures campaigns which increasingly leverage social media to significant effect. The 2018 Senate Select Committee on Intelligence (SSCI)- commissioned report, ‘The Tactics & Tropes of the Internet Research Agency’ said Russian interference was “designed to exploit societal fractures, blur the lines between reality and fiction, erode our trust in media entities and the information environment, in government, in each other, and in democracy itself”.9 This included the use of the St. Petersburg based Internet Research Agency (IRA) with funding channeled through Yevgeny Prigozhin, the head of the Wagner Group with close ties to the Kremlin. They were adept at exploiting wedge issues which mined pre-existing cracks in society as well as trying to fund and exploit agents of influence.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9119396209716797,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The results have arguably and demonstrably deepened societal divisions and increased political polarization on issues such as nationalism, culture, identity politics, and immigration. The IRA, as well as the GRU and SVR used Western social media to target sections of their electorates rapidly and pro-actively and reactively at scale.10 In the U.S. presidential election these coordinated efforts were allied to the hacking and leaking of documents following separate breaches of the Democratic Congressional Campaign Committee (DCCC) and Democratic National Committee (DNC). American society was already highly polarized and politically fractured but it is argued Russian election interference in 2016 also “created a crisis of confidence in the legitimacy of outcomes and the electoral process, and exacerbated social fissures – all worthy Russian goals even if the Kremlin did not have a direct electoral impact”.11 The use of propaganda to incite or promote division and destabilization was also a feature of the Brexit referendum and other European elections or referenda including in France, Italy, Germany, Spain (especially in the Catalonian referendum of 2017), Ukraine, the Balkans and Turkey.12 Further systematic (and ongoing) study is needed. Their ability to message is not helped by the fact that many/all of the main social media platforms have withdrawn access to their Application Programming Interfaces (APIs) and are the only ones with live data and datasets.13 6P10. Attributing (whether UK or allied government policy or not) is tricky less for technical or resource reasons but because of the way state-actors like Russia use proxies. This encompasses much more nuance than organised actors like the Internet Research Agency (IRA). FIS activities are multi-dimensional. Whether the full scope of these operations is captured by the act will be tested. Moreover, whether the UK’s security services (were and possibly still are) fully geared to deal with current and evolving threats in this space is also a valid (but much wider) question. The framing of the offence and some parts of the Online Harms Act (because of the narrative framing) give rise to these concerns. Automated content moderation (User to User) \\t\\tEvidence on use of URL detection of terroist content \\tThe guidance focuses on the use of URLs to share terrorist content. This is understandable and reflects the academic research in this area. While the use of URLs to circumvent content moderation and share content is important, it should be emphasised that terrorist groups and their supporters also use URLs in other ways and for other purposes. At paragraph 14.158, the guidance notes the risks of inadvertent viewing of CSAM. A similar point applies to terrorist content. Research has found that IS supporters use such tactics as hashtag hijacking and use of the @reply and @mention functions to increase the reach of their propaganda and expose unsuspecting users to it.17 In a new VOX-Pol report, Stuart Macdonald and Sean McCafferty use data collected from four platforms over a two-month period to examine how a total of 796 items of jihadist propaganda were disseminated. 18 In respect of URLs, the study found that outlinking was the predominant method of content dissemination employed by AlShabaab, and that it was used regularly by Al-Qaeda. For Islamic State, outlinking was widely used to share videos, magazines, and instructional materials, but rarely used for other types of content such as bulletins, banners and photosets. The study also examined the use of inlinking. While this was the least commonly used method for content-sharing, the report expresses concern at the use of inlinks to create manually a filter bubble effect. Many of the inlinks that were collected were included in posts beneath another item of content – so that after viewing one item users could then choose to view another, similar item on the same platform. In fact, some of these posts provided catalogues of similar content. This was the case, for example, with the nasheeds contained within the dataset. At a time when much concern is being expressed about the potential for algorithmic recommender systems to create echo chambers and take users down the ‘rabbit hole’, it is important that manual efforts to use inlinks to do something similar are not overlooked. There is also evidence that inlinks tend to be used more frequently to direct users to other dissemination spaces (such as channels or groups) on the same platform, as opposed to other items of content. 19 This exacerbates concerns about the use of inlinks to create a filter bubble effect – as consumers of such content are signposted to other dissemination outlets – and is a harm that is overlooked by an exclusive focus on the use of URLs to share content. Freedom of expression International human rights treaties stipulate that restrictions on the right to freedom of expression are permissible, provided that such restrictions are prescribed by law, pursue a legitimate objective, and meet the demands of necessity and proportionality. Legitimate objectives include the prevention of crime and the protection of national security. Accordingly, where URLs are being used deliberately to disseminate content that promotes or encourages terrorism, it is permissible to impose restrictions on the right to freedom of expression of the users posting these URLs. To minimise the impact on freedom of expression, in most instances the URL that is deactivated should link to a specific item of content. However, in some cases it may be justifiable to shut down a broader space, such as a channel or group that exists for the explicit purpose of sharing terrorist content. This mirrors the approach taken in the guidance to CSAM, at paragraph 14.163. At the same time, it is important to acknowledge that the effort to identify and disable these URLs does entail some risks to freedom of expression. There are several contributory factors: • The UK’s statutory definition of terrorism has been widely criticised for being overly broad, including by the Supreme Court.20 The effect is to vest significant discretion in those applying the definition: in the current context, human moderators, or even automated tools. This creates a risk of inconsistent, possibly inappropriate, application of the definition, and raises the question whether the interference with the right is sufficiently clear to meet the prescribed by law threshold. • Tech companies may adopt a cautious approach to content moderation, in order to avoid accusations of failing to remove extremist or terrorist content from their platforms. This can result in over-enforcement. It has been argued that regulatory regimes that impose time limits for the removal of content may exacerbate this risk. • When applying prohibitions on terrorism-promoting content, many tech companies refer to a “greyzone”.21 This is particularly relevant to content posted by activist groups and movements. Here, the right to freedom of expression is especially important. These actors use online platforms to raise awareness of their cause, to coordinate their activities and to document human rights abuses. Yet it may be unclear whether such content falls within expansive definitions of terrorism. In some circumstances, prohibitions on terrorism-promoting content may even be used to silence activists and their supporters. The upshot is that the protection of freedom of expression requires the exercise of nuance and judgement in the application of prohibitions on terrorist content. While automated tools for the identification of online terrorist content are essential, their limitations must also be acknowledged. Machine learning algorithms have difficulty understanding context and accounting for such things as subtlety, irony, and sarcasm. This is particularly important for some types of content, e.g., memes.': [{'label': 'POSITIVE',\n",
       "               'score': 0.990864098072052,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They also have difficulty making inferences of intention. Yet intention is central to definitions of terrorism. And there are linguistic and cultural limitations, such as assessing culturally shaped English usage in countries in the Global South. As well as overenforcement, the limitations of machine learning algorithms have resulted in documented failures to remove hate speech. This poses an additional risk to freedom of expression, as it has a chilling effect on the use of online platforms by the targeted users and groups. It has also contributed to real-world violence in some instances, such as in Ethiopia and Romania.22 One safeguard against overenforcement is to require tech companies to have publicly available definitions of terrorism that are properly circumscribed. For example, in 2019 Facebook narrowed its definition of terrorism so that, instead of referring simply to “violence against persons or property”, it instead referred to violence against “civilians, or any other person not taking direct part in the hostilities in a situation of armed conflict”.23 The reason for this change was so that the definition could not be accused of including broader dissident groups or activist networks in conflict zones. A further safeguard is to ensure human-in-the-loop content moderation processes.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910581111907959,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'However, companies employing human moderators should be mindful of two important considerations. The first is capacity, in terms of both volume of content and the necessary expertise.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9923058152198792,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This encompasses linguistic and cultural understanding, as well as subject matter expertise.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9938110709190369,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"The second is health and wellbeing. Moderators have consistently reported suffering from significant mental health issues, with an absence of meaningful programmes to help address the consequences of regularly viewing large volumes of the most graphic and harmful content. Lack of the necessary capacity, expertise or wellbeing provision has been shown to have a detrimental impact on the quality of content moderation decisions. 24 It should be regarded as a systemic risk to the moderation of online terrorist content in compliance with users’ freedom of expression. Recommender system testing (U2U) \\t\\tWelcome proposal, but disputes how much illegal content is spread by recommender systems, offers evidence in other areas and legal but harmful content. We welcome the proposals in Volume 4 (19) to carry out on-platform tests to ensure the minimisation of the potential amplification of illegal content. Volume 4 (19) notes that if illegal content that is uploaded on a U2U service and is missed by content moderation systems, it could potentially be amplified and spread. It must be stressed, however, that research does not suggest that there is widespread proliferation of illegal content being amplified by recommender systems. In his literature review of 13 studies on the amplification of extremist content, Whittaker notes that while most studies analyse content that would be considered “legal but harmful” or “borderline” content.25 In the rare cases that the studies do focus on illegal content, such as those by Murthy, 26 or Berger, 27 the data were collected in the mid-2010s when content moderation norms were substantially different online. Similarly, Yesilada & Lewandowsky's do not focus explicitly on the legality of material in their meta-analysis of YouTube’s recommendation system.28 However, most of the studies which they include appear to focus on content that would be considered legal under UK law. For example, health misinformation, pseudoscientific content, content which is unsafe for children, and extremist (but not necessarily illegal) content. They do include a category of “racist content”, although it is again unclear whether this would be illegal. In an ongoing scoping review conducted by three of the authors of this response, which includes over 50 pieces of empirical research, there is again very little illegal content under study. On legal but harmful content and recommender systems Existing research points to a very modest amount of illegal content being amplified by recommender systems on U2U platforms. This means that the majority of potentially harmful content will not be covered by the Online Safety Act. As mentioned above, three of the authors of this paper are conducting a scoping review on the amplification of illegal, or legal but harmful content by recommendation systems. While only a small fraction of the 53 pieces of empirical research relates to content that would be considered illegal, 25 focused on mis/disinformation and 20 focused on extremist-related content (with a further four focusing on both). For misinformation, there were a number of studies that suggested conspiracy and misinformation content may be promoted by recommender systems, if users were seeking this content out and there was some evidence of misinformation filter bubbles. For example, Hussein et al focused on the promotion of conspiracy content and found evidence of a misinformation filter bubble for search results.29 Additionally, Matamoros-Fernandez et al found some misleading content is still recommended on YouTube.30 There was also some evidence of extremism-related content being promoted in certain instances, and some evidence of echo chambers and filter bubbles. For example, Charles explored the amplification of white supremacist content by YouTube’s recommender system and found that creators de-emphasised race in their content to market racism in an appealing way to mainstream politics.31 Furthermore, Cockroft found that openly white nationalist figures were not as well represented in recommendations as political influencers whose content could be categorised as borderline hateful.32 The two studies on disturbing and violent content found that this content can be promoted following non-disturbing children’s videos. 33 Furthermore, the study focusing on eating disorder content found themes including the glorification of weight loss and food to achieve health and thinness within content in algorithmically promoted hashtags on TikTok. 34 Overall, the findings from the scoping review suggested that harmful content can promoted by recommender systems in certain circumstances, but limited amounts of this content is illegal. As such, unless the user is a young person, most of the potentially harmful content that is on platforms and can be promoted by recommender systems is not in scope to be addressed within the consultation. On other design features of recommendation systems One suggestion to address the potential amplification of illegal content is the use of algorithms to promote counter-speech. The bestknown example of this is Moonshot’s Redirect Method. The pilot of this campaign used google ad technology to redirect users searching for jihadist extremist content towards counter-narrative playlists in YouTube. 35 This approach was deemed somewhat effective as 500,070 minutes of video were watched by 320,906 individuals during the 8-week pilot. 36 This campaign has since been expanded to redirect users searching for far-right content, as well as other types of harmful content such as users searching for child sexual abuse materials being redirected towards support, 37 as well as being deployed on Facebook. 38 Another example of this use of algorithms is a Swedish Facebook group which utilised Facebook’s commenting algorithm to amplify their comments whilst burying hateful comments as their counterspeech strategy. 39 Furthermore, the reach of #faces4heritage Facebook’s page was found to exponentially increase when their posts were sponsored because Facebook’s algorithm prioritised them over organic posts. 40 As such, there are a range of ways that algorithms can be used to promote counter-speech. However, counter-speech needs to be used with caution to avoid causing further harm or counter-productive impacts. For example, it is hard to know how effective the redirect method is as short-term reach and engagement metrics that are used do not provide a full picture of the long-term impact of these programmes (positive or negative). 41 Additionally, Schmitt et al found that counter-speech videos associated with ExitUSA were connected with extremist videos within two clicks via YouTube’s recommendation algorithm.42 Zieringer and Rieger supported these findings.43 More broadly, the efficacy of counter-narratives has been questioned. In a meta-review on this topic, Jones finds there to be littleto-no robust evaluation and as a result, none of the 139 campaigns being deemed to be effective.44 Similarly, in a review of interventions conducted by Hassan et al. only three campaigns were found to show mostly positive results, and none of these actually measured whether viewing such a narrative had a positive effect on attitudes or behaviours, which limits the positive conclusions that one can draw.45 More recently, studies by Carthy & Sarma, 46 and Braddock, 47 have used rigorous methodologies to assess the efficacy of counter-narratives and have both shown positive results. Importantly, algorithms cannot replace human involvement in the design of counter-speech campaigns, as automatically designing counter-speech can have harmful consequences. For example, Estrella Vallecillo-Rodríguez et al automatically designed counterspeech via natural language processing algorithms.48 This was a time-efficient way of responding to hateful content, but it also resulted in grammatical errors and inconsistencies/ false information within message content. This is conducive to creating a say-do gap, which can have harmful impacts of further marginalising minority communities and exacerbating individuals’ radical beliefs. Counterspeech campaigns need to be designed and disseminated appropriately to mitigate counter-productive impacts and careful ongoing monitoring and evaluation is essential to measure campaigns’ impacts, and to adjust where necessary. Automated content moderation (User to User) \\tPublic/Private\\tGeneral comments on the guidance, highlighting factors\\tWe welcome the emphasis in the guidance (at para A9.19) on matters of substance, as opposed to whether the content (or parts of the service on which the content is generated, shared, or uploaded) is labelled as ‘private’. The guidance also states that the fact that content has been generated, shared, or uploaded by a user that has anonymity or using a pseudonym is not expected to be relevant to the question of whether content has been communicated ‘publicly’ or ‘privately’ (para A9.19). There may be situations, however, in which conditions of anonymity are telling. For example, private Islamic State (IS) channels on Telegram are secretive and difficult to access. For some, qualities such as secrecy, anonymity and limited access are the hallmark of privacy. Yet there are important respects in which private IS Telegram channels are not analogous to, for example, a private group in which family members exchange messages and photos. In particular, private IS channels are characterised by anonymity, with pseudonyms or random strings of letters and numbers used for user IDs. Channel administrators will often not know the identities of the users in the channel. (It is this anonymity that enables some researchers and investigators to gain access). In reality, the difficulties in accessing these groups are designed not to limit the members of the group to trusted family and friends. Rather, it is to limit access to just one section of the public (pro-IS users). But, as the guidance states at para A9.23, where content “is accessible to a substantial section of the public, it should be considered as communicated ‘publicly’.”14 The distinction between how content is communicated, and the nature of the content itself, is a useful one (para A9.15). As the guidance points out, it is possible for content that engages a person’s Article 8 ECHR right to privacy to be communicated publicly. At the same time, it is also worth noting that there may be circumstances in which the nature of the content should inform the decision whether the content is communicated publicly or not. As an example, take an official newsletter or magazine of a terrorist organisation. Such a publication is produced with the express purpose that it 14 The word ‘substantial’ is discussed further below. Question (Volume 4) Your response be widely circulated. When it is initially shared between group members for onward distribution, this communication (however secretive) should be viewed in its wider context. The public-facing nature of the content, and the desire to disseminate it to as wide an audience as possible, should inform the decision whether the initial communication was public or not. This raises wider questions regarding chain dissemination processes in which terrorist propaganda is disseminated to the public via a multi-step process, often involving multiple platforms.15 The guidance offers some advice for such situations: • The fact that the initial communication is private does not mean that subsequent communications of the same content are also private (para A9.20). • It may be virtually impossible for services to prevent content from being shared or forwarded in certain ways (such as by taking a screenshot of content and then sending it to another user, or where a user has been given a password to access specific content and chooses to share that password with others). This does not indicate that content can be forwarded or shared with ease for the purpose of Factor (C) (para A9.39). The guidance also states that: (1) the more individuals in the UK are able to access the content, the more likely it is to be communicated publicly (para A9.23); and, (2) the converse is not necessarily true. As para A9.24 recognises, “The fact that it may be difficult for individuals to access the content (for example, because users need to take time to locate the content and it is not easily discoverable) does not mean that content should be considered as communicated ‘privately’.” This is especially relevant to the chain dissemination of official terrorist propaganda. When this content is initially released, the restrictions on access are designed to safeguard the early stages of the dissemination process and enable wider subsequent circulation of the materials. Here, the fact that the restriction serves the purpose of wider dissemination and making the content more, not less, publicly available should be taken into account. Comments on Factor (A): Number of UK individuals able to access the content The guidance states that content should be considered as communicated publicly where it is accessible to a ‘substantial’ section of the 15 public (paras A9.23, A9.29, A9.40). This raises the question how substantiality is to be assessed.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.8532865643501282,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Is it a quantitative assessment? Or a qualitative one?': [{'label': 'POSITIVE',\n",
       "               'score': 0.9110888242721558,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Or either/some combination of both? It should also be stated explicitly that the fact that content is accessible to a section of the public that is less than substantial does not mean that the content is not communicated publicly. A contrary position would be out of sync with the ‘Encouragement of Terrorism’ offence (Terrorism Act 2006, s. 1) – one of the priority offences listed in Schedule 5 of the Online Safety Act. For this offence, it is enough that a statement is published to any section of the public. It would be incoherent if a statement could be communicated publicly for the purposes of the Encouragement of Terrorism offence, yet be regarded as communicated privately for the purposes of the Online Safety Act. The guidance on Factor (A) does not address the accessing of content using a VPN. If a particular communication channel is geoblocked in the UK, but not elsewhere, should UK users be regarded as unable to access the channel’s content notwithstanding the possibility that it might be accessed using a VPN? This appears to be the assumption, but it would be useful to make this explicit.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928377270698547,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The importance of the words “by means of the service” should also be noted (Online Safety Act 2023, s. 232(2)(a)). Factor (A) does not simply require consideration of how many individuals in the UK are able to access the content. It requires consideration of how many individuals in the UK are able to access the content by means of the service. In the context of chain dissemination of terrorist propaganda, this distinction is significant when considering ‘aggregator’ platforms (i.e., ones that provide lists of URLs from which items of propaganda can be downloaded, to be publicised on ‘beacon’ platforms). An aggregator platform might play an important role in enabling numbers of UK individuals to access a propaganda item, but these users would not be accessing the content by means of the aggregator platform’s service. It should be noted, therefore that additional factors beyond those listed in the statute may be considered relevant (para A9.18). An aggregator platform should be expected to consider the extent to which it facilitates access by UK individuals to content by means of another service. Comments on Factor (B): Access restrictions In the application of this factor, regard should be had not just to the technical features of the platform, but also these features’ practical operation. For example, at the technical level Telegram private Question (Volume 4) Your response channels are designed to restrict users to those approved by the channel administrator. But in practice, joinlinks to private IS channels are often made openly available (albeit difficult to locate). This undercuts the raison d’etre of the privacy-enabling feature. It seems implicit in the guidance that the practical operation of restrictions on access should be considered, as well as the nature of the restrictions themselves. Noneetheless, an explicit statement to this effect would be worthwhile. Approach to the Codes\\t\\tThey think we should recommend a number of measures to protect sex workers online. \"We believe that AT THE VERY LEAST, websites that host prostitution adverts should:\\n1. Require punters to register using government approved photo ID; \\n2. Ban the display of contact details to users who have not logged in with a registered account backed by government approved photo ID; and \\n3. Require third-parties who place advertisements for others to log in with a registered account backed by government approved photo ID.\"\\nApproach to the Codes\\t\\tThink any site that carries prostitution adverts should be behind a robust age verification system that only allows access to people 18 or over. (b/c young people look at these sites - see ICJG comment)... At the very least therefore any site that carries prostitution advertisements and listings etc must be behind a robust age verification system that allows access only to people of 18 years or over. https://www.ohchr.org/en/instruments-mechanisms/instruments/protocol-prevent-suppress-and-punish-trafficking-persons\\nn/a\\tTs+Cs\\tIt is important for T+Cs not to be too transparent, or might help abusers do harmful things. Additionally, we believe that it is important not to include too many details on our anti-abuse practices in our T&Cs to avoid abusers from guessing and bypassing these. We trust Ofcom will be sensible to the need for online services to make sure their measures are not being undermined by excessive transparency requirements. Approach to the Codes\\tSegmentation\\tSupportive of our willingness to take a flexible and differentiated approach between small/large services and those with low/high risk. Notes there is no one \"silver bullet\" that can fit all the different U2U services. \"We welcome Ofcom’s willingness to come up with a flexible and a differentiated approach between small and large services on one side, and between the different levels \\nof risk on the other. While we acknowledge that there is no silver bullet that can fit all the different U2U services, we have some remarks on the approach chosen by Ofcom as it currently stands.\"\\nApproach to the Codes\\tSegmentation\\tSupportive of our willingness to take a flexible and differentiated approach between small/large services and those with low/high risk. Notes there is no one \"silver bullet\" that can fit all the different U2U services. \"We welcome Ofcom’s willingness to come up with a flexible and a differentiated approach between small and large services on one side, and between the different levels \\nof risk on the other. While we acknowledge that there is no silver bullet that can fit all the different U2U services, we have some remarks on the approach chosen by Ofcom as it currently stands.\"\\nApproach to the Codes\\tDefinition of large services\\tNote that we haven\\'t provided a methodology or definition of \"user\" to count the 7 m users (large). In order to define large services, the focus is put on the number of monthly UK users – 7 million at least in this specific case. However, in the proposals provided we did not encounter any definition of ‘user’ or methodology to count them. Approach to the Codes\\tDefinition of large/other services\\tThere are many different ways protonmail could count its users. All could have potential issues. Our service that is concretely in scope for the Act, Proton Drive, is classified as a file storage and file sharing service. There are a number of different ways in which we could count users of this service. Should we count as a UK-based user someone who has a Proton account for another part of the ecosystem, even if they do not use Proton Drive? Does this refer only to those who upload and download files and have a Proton account? Shall we also take into account non-Proton users who interact with the service, even if they only read a file shared via a link? In all these cases, there would be potential issues. Approach to the Codes\\tDefinition of large/other services\\tIssues with calculating how many users is a particular challenge for Protonmail given its privacy focus. To add to this, we do not know precisely where our users live, even less where people accessing links live.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916097521781921,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Attempting to obtain such information would require us to collect much more data than we currently do, and put at risk the privacy and security of those whose data we are collecting. Privacy-invasive services would be in a better position to comply than privacy-preserving ones – thus putting companies with more dangerous business models for the society in an advantageous position to comply with the requirements of the OSA. Automated content moderation (User to User) \\tSegmentation\\tProton consider that applying CSAM hashmatching to services with 0.1% of the UK population is disproportionate. There is a similar issue on the specific measures for file storage and file sharing services that have more than 70,000 monthly UK users. Applying stringent measures to services used by only 0.1% of the UK population lacks any form of proportionality in our opinion. Automated content moderation (User to User) \\tSegmentation\\tProton consider that applying CSAM hashmatching to services with 0.1% of the UK population is disproportionate. There is a similar issue on the specific measures for file storage and file sharing services that have more than 70,000 monthly UK users. Applying stringent measures to services used by only 0.1% of the UK population lacks any form of proportionality in our opinion. Approach to the Codes\\tDefinition of large/other services\\tThink it may be difficult for some services to know how many UK users they have.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922387003898621,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'And as counting UK users will be very difficult and imprecise, many small services will not be able to know whether they have more of less than 70,000 monthly users. Approach to the Codes\\tDefinition of multi-risk\\tProton is supportive of the categories \"low\", \"specific\" and \"multi-risk\", but thinks two kinds of risk is too inclusive, and would mean the vast majority of U2U services would be considered multi-risk. We were also surprised by the division in different risk levels for each service, and the three-tiered approach adopted. While a division between low risk, specific risk and multi-risk seems sensible, labelling a service “multi-risk” only when two different kinds of harms are identified as medium or high risk is excessive. Reading Annex 3, it is clear that if this classification was to be retained in the final proposals, the vast majority of U2U services in scope of the OSA would be considered “multi-risk”. Automated content moderation (User to User) \\t\\tProtonmail thinks its classification of in-scope for CSAM HM is unfair/ disproportionate, as it doesn\\'t take into account: (i) measures taken by service to mitigate harm, (ii) number of users, (iii) record of past misuse. According to table 7 in Annex 5, Proton Drive is automatically considered to be “high risk” for image based CSAM. We find such a classification to be unfair and disproportionate, as it does not take into account the potential measures taken by the service to mitigate harm, its number of users nor any past records of misuse. Approach to the Codes\\t\\tIt is not fair that services that already do something to mitigate a harm have to do the same thing as services that do nothing to mitigate that same harm. It does not seem reasonable for a service which already has measures in place against CSAM to face the same burden as a service which wouldn’t. Approach to the Codes\\t\\tIt is not fair that services that already do something to mitigate a harm have to do the same thing as services that do nothing to mitigate that same harm. It does not seem reasonable for a service which already has measures in place against CSAM to face the same burden as a service which wouldn’t. Automated content moderation (User to User) \\t\\tProton is supportive that our proposals do not apply to its email service (E2EE), and notes the impossiblity of applying HM technology to private messaging. We are extremely pleased to see that Ofcom recognises that “end-to-end encrypted services are currently unable to analyse user-generated content in the ways set-out in our proposals”. It is indeed impossible for Proton to look at the content of, for example an email sent with Proton Mail. In the proposals, a lot of references are made to the impossibility to apply hash-matching technology in E2EE private messaging, and Ofcom makes clear that “privacy communications or end-to-end encrypted communications do not have to implement such technologies”. Automated content moderation (User to User) \\t\\tProton is supportive that our proposals do not apply to its email service (E2EE), and notes the impossiblity of applying HM technology to private messaging. We are extremely pleased to see that Ofcom recognises that “end-to-end encrypted services are currently unable to analyse user-generated content in the ways set-out in our proposals”. It is indeed impossible for Proton to look at the content of, for example an email sent with Proton Mail. In the proposals, a lot of references are made to the impossibility to apply hash-matching technology in E2EE private messaging, and Ofcom makes clear that “privacy communications or end-to-end encrypted communications do not have to implement such technologies”. Automated content moderation (User to User) \\t\\tProton would like more clarity on where things stand for an E2EE file storage / file sharing service, and whether it is required to search this content. A clear reference that no E2EE service is expected to apply HM would solve the issue. \"However, we would welcome more clarity on where things stand with regards to our file storage and file-sharing service, Proton Drive. Proton’s file-storage and sharing service is also end-to-end encrypted, meaning that Proton cannot access the content of what is being stored on it. Similarly to what happens on E2EE messaging services like WhatsApp, Proton is unable to scan files hosted on Proton Drive. A clear reference stating that no E2EE service is expected to put in place hash matching would \\nsolve this issue.\"\\nAutomated content moderation (User to User) \\t\\tProton would like more clarity on where things stand for an E2EE file storage / file sharing service, and whether it is required to search this content. A clear reference that no E2EE service is expected to apply HM would solve the issue. \"However, we would welcome more clarity on where things stand with regards to our file storage and file-sharing service, Proton Drive. Proton’s file-storage and sharing service is also end-to-end encrypted, meaning that Proton cannot access the content of what is being stored on it. Similarly to what happens on E2EE messaging services like WhatsApp, Proton is unable to scan files hosted on Proton Drive. A clear reference stating that no E2EE service is expected to put in place hash matching would \\nsolve this issue.\"\\nAutomated content moderation (User to User) \\t\\tThere are a number of problems with the application of hash matching. Think more focus should be on helping to ensure law enforcement rescue children in real life (before harm is done). We have on several occasions voiced our concerns2 on its prescription to all U2U services at risk of CSAM and/or grooming. Hash matching can still lead to many false positive (or false negatives), and beyond its financial cost for smaller services, it is also not easy to implement as each service has its own specificities. Additionally, maintaining a functioning hash matching technology on a service is difficult and does not come without strong privacy and security risks. There is not technological silver bullet unfortunately, and we strongly believe that the focus should be on making sure that law enforcement has the means to rescue children in real life, as hash matching will only come into play once some harm has been done to children\\nApproach to the Codes\\tSegmentation\\tBT considers we should extend the scope of some of our codes measures (detail below). Whilst it understands our intent to take a proportionate approach, to ensure resources are focused on where the risk is greatest, BT consider that smaller services, in particular those that are multi-risk, should do more to ensure users are sufficiently protected. Approach to the Codes\\tSegmentation\\tBT considers we should extend the scope of some of our codes measures (detail below). Whilst it understands our intent to take a proportionate approach, to ensure resources are focused on where the risk is greatest, BT consider that smaller services, in particular those that are multi-risk, should do more to ensure users are sufficiently protected. Governance and accountability \\tSegmentation\\tBT considers measure 3C should be extended to smaller services with specific risks\\tIt thinks measure 3C should be extended to smaller services with specific risks, to increase individual accountability and encourage online safety risks to be taken seriously, and help set the right cultural \"tone from the top\". Approach to the Codes\\tSegmentation\\tBT considers measure 3C should be extended to smaller services with specific risks\\tIt thinks measure 3C should be extended to smaller services with specific risks, to increase individual accountability and encourage online safety risks to be taken seriously, and help set the right cultural \"tone from the top\". Approach to the Codes\\tSegmentation\\tBT considers measure 3C should be extended to smaller services with specific risks\\tIt thinks measure 3C should be extended to smaller services with specific risks, to increase individual accountability and encourage online safety risks to be taken seriously, and help set the right cultural \"tone from the top\". Governance and accountability \\tSegmentation\\tBT considers measure 3E should be extended to all services. It is critical that all services have processes in place to scan for and identify new risks. Should this be absent, services may continue to categorise themselves as low risk, with few and insufficient risk management controls implemented. Reporting of emerging risks to the senior governing body ensures high level accountability for managing online safety risks\\nApproach to the Codes\\tSegmentation\\tBT considers measure 3E should be extended to all services. It is critical that all services have processes in place to scan for and identify new risks. Should this be absent, services may continue to categorise themselves as low risk, with few and insufficient risk management controls implemented. Reporting of emerging risks to the senior governing body ensures high level accountability for managing online safety risks\\nApproach to the Codes\\tSegmentation\\tBT considers measure 3E should be extended to all services. It is critical that all services have processes in place to scan for and identify new risks. Should this be absent, services may continue to categorise themselves as low risk, with few and insufficient risk management controls implemented. Reporting of emerging risks to the senior governing body ensures high level accountability for managing online safety risks\\nGovernance and accountability \\tSegmentation\\tBT considers staff training on compliance should be extended to other services (unclear which). Staff training is required to ensure controls are operating effectively and is integral to embedding a risk management culture across the organisation (hence should broaden application of measure). Approach to the Codes\\tSegmentation\\tBT considers staff training on compliance should be extended to other services (unclear which). Staff training is required to ensure controls are operating effectively and is integral to embedding a risk management culture across the organisation (hence should broaden application of measure). Approach to the Codes\\tSegmentation\\tBT considers staff training on compliance should be extended to other services (unclear which). Staff training is required to ensure controls are operating effectively and is integral to embedding a risk management culture across the organisation (hence should broaden application of measure). Approach to the Codes\\tSegmentation\\tBT considers conmod training and materials is a basic requirement on orgs to ensure their conmod functions work (think this means to all services; though not entirely clear). We consider this (conmod training and materials) to be a basic requirement on organisations to ensure effective operation of their content moderation functions. Approach to the Codes\\tSegmentation\\tBT considers conmod training and materials is a basic requirement on orgs to ensure their conmod functions work (think this means to all services; though not entirely clear). We consider this (conmod training and materials) to be a basic requirement on organisations to ensure effective operation of their content moderation functions. Content moderation (User to User) \\tSegmentation\\tBT considers conmod training and materials is a basic requirement on orgs to ensure their conmod functions work (think this means to all services; though not entirely clear). We consider this (conmod training and materials) to be a basic requirement on organisations to ensure effective operation of their content moderation functions. Approach to the Codes\\tSegmentation\\tBT considers blocking / muting individuals/ disabling comments should be extended to more services (think all, but unclear). This (users can block/mute individual users/disable comments) appears to be basic functionality to enable users to have better control over their online experiences and reduce the risk of encountering harm online. Approach to the Codes\\tSegmentation\\tBT considers blocking / muting individuals/ disabling comments should be extended to more services (think all, but unclear). This (users can block/mute individual users/disable comments) appears to be basic functionality to enable users to have better control over their online experiences and reduce the risk of encountering harm online. Enhanced user control (U2U) \\tSegmentation of enhanced user controls measures\\tBT considers blocking / muting individuals/ disabling comments should be extended to more services (think all, but unclear). This (users can block/mute individual users/disable comments) appears to be basic functionality to enable users to have better control over their online experiences and reduce the risk of encountering harm online. Approach to the Codes\\tDefinition of large services\\tDefinition of large services sets the bar too high, meaning services with significant reach don\\'t fall in scope of some measures.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927421808242798,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We need to ensure there is a mechanism to test this (and other criteria) once the regime is operational. Thinks our definition of large services sets the bar too high, so many services with significant reach aren\\'t captured. When the regime is operational, we should ensure there is a mechanism in place to reassess the size threshold / requirements placed on different categories of service, to ensure we are placing sufficient requirements on all services with the greatest risk of harm. Approach to the Codes\\tDefinition of large services\\tDefinition of large services sets the bar too high, meaning services with significant reach don\\'t fall in scope of some measures. We need to ensure there is a mechanism to test this (and other criteria) once the regime is operational. Thinks our definition of large services sets the bar too high, so many services with significant reach aren\\'t captured. When the regime is operational, we should ensure there is a mechanism in place to reassess the size threshold / requirements placed on different categories of service, to ensure we are placing sufficient requirements on all services with the greatest risk of harm. Automated content moderation (User to User) \\tPublic and Private Guidance + E2EE\\tBT is concerned about our description of E2EE technology, and that we seem to be saying it\\'s treated the same as private communications in the Act. \"In respect of measures 14-16 on p4 of the summary document Consultation at a glance: our proposals and who they apply to (ofcom.org.uk), and the relevant footnotes (a)-(c) on pp7-8, it is stated that \"\"Measure does not apply to private communications or end-to-end encrypted communications\"\". The guidance further clarifies that \"\"These proposals only apply in relation to content communicated publicly on U2U services, where it is technically feasible to implement them. Consistent with the restrictions in the Act, they do not apply to private communications or end-to-end encrypted communications. In Annex 9 to this consultation, we have set out draft guidance which is intended to assist services in deciding whether content has been communicated “publicly” or “privately” for this purpose.\"\"\\nWhilst we accept that scanning is a \"\"proactive technology\"\" and therefore only applies to content communicated publicly, we have concerns at the addition of “or end-to-end encrypted”, which does not reflect the wording of the Act. The Act distinguishes between content communicated publicly and content communicated privately. The Ofcom guidance adds \"\"or end-to-end encrypted communications\"\" and appears to treat it in a similar way to that \"\"communicated privately\"\". However the addition of the word “or” seems to suggest that E2EE is not necessarily accepted to be the same as “communicated privately”.\"\\nAutomated content moderation (User to User) \\tPublic and Private Guidance + E2EE\\tBT is concerned that as E2EE traffic grows, it poses a greater and greater risks to user safety. We note that both E2EE services and community building services are explicitly recognised in Volume 2 of the draft guidance as high risk with regard to a range of harms including the dissemination of priority illegal content, and therefore require additional mitigations. As more and more traffic is subject to E2EE and this proportion continues to grow, we have concerns about whether this creates a significant gap for large groups of users communicating together and/or at scale via E2EE, and whether it is appropriate to treat such traffic as equivalent to a private communication. This seems to risk creating the “free pass” for E2EE services which was explicitly ruled out by the government during the Bill’s passage\\nAutomated content moderation (User to User) \\tPublic and Private Guidance + E2EE\\tBT supports a requirement for tech companies to scan encrypted traffic for CSAM etc. Notes it is technically possible. We support a requirement on regulated tech companies to find technical solutions to scanning encrypted content (e.g. client-side scanning) to prevent the sharing of child sexual abuse and exploitation material (CSAM). Such technical solutions have been proven to be both possible and capable of being implemented.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923877716064453,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Where they have been implemented and subsequently withdrawn this has generally been driven by user feedback, which in our view demonstrates that there is no technical objection to requiring the use of such technology via regulation. BT notes this article on Apple\\'s experience: https://www.forbes.com/sites/thomasbrewster/2023/09/07/apple-icloud-child-sexual-abuse-material-privacy/\\nAutomated content moderation (User to User) \\t\\tBT is supportive of measures that help address violence against women and girls (inc CSAM). BT welcomes the introduction of new criminal offences in the Act to address online Violence Against Women and Girls, and Ofcom’s specific measures relating to Child Sexual Abuse Material (CSAM). We are strongly supportive of measures to give greater protections for women and girls online and we have previously run public campaigns to highlight the abuse women and girls often face online, via Hope United and with the charity Glitch. Approach to the Codes\\tVAWG Measures\\tBT would like Ofcom to bring forward measures to protect women and girls from online harm\\tThe current consultation on Illegal Harms does not specifically address the safety of the majority of women and girls online. The Act requires Ofcom to provide Guidance to in-scope services on reducing risks of harm to women and girls, which we are expecting Ofcom to consult on in Q2 2025. This timeline for Ofcom to consult on, and then publish, specific Guidance on Violence Against Women and Girls will mean that women and girls will remain exposed to harms for years before service providers are required to implement effective safety measures. In light of the implementation gap, we’d like to see greater consideration of this issue in the current Guidance. In this respect we are aligned with the comments submitted by the Online Safety Act Network osa-network-ofcom-illegal-harms-sign-on-feb-2024.pdf. Approach to the Codes\\tVAWG Measures\\tBT would like Ofcom to bring forward measures to protect women and girls from online harm\\tThe current consultation on Illegal Harms does not specifically address the safety of the majority of women and girls online. The Act requires Ofcom to provide Guidance to in-scope services on reducing risks of harm to women and girls, which we are expecting Ofcom to consult on in Q2 2025. This timeline for Ofcom to consult on, and then publish, specific Guidance on Violence Against Women and Girls will mean that women and girls will remain exposed to harms for years before service providers are required to implement effective safety measures. In light of the implementation gap, we’d like to see greater consideration of this issue in the current Guidance. In this respect we are aligned with the comments submitted by the Online Safety Act Network osa-network-ofcom-illegal-harms-sign-on-feb-2024.pdf. Approach to the Codes\\tOur approach to the ICJG\\tThe proposed code of practice measures do not go far enough to tackle online CSEA\\tBarnardo\\'s think that the proposed CSEA code of practice measures lack ambition and do not do anything more than what is in place on many services already. Barnardo\\'s are concerned that Ofcom\\'s high bar for evidence has hindered these codes and will hinder future codes, if it is only possible to suggest measures that are already widely used by services. Further, Barnardo\\'s are concerned that Ofcom\\'s \\'undue\\' focus on proportionality could mean that small, risky companies are let off the hook, which is particularly concerning when coupled with the safe harbour. Barnardo\\'s highlight a number of specific CSEA concerns and gaps: 1) CSAM hash-matching has been in place on services since 2003; 2) small risky CSAM sites such as collector sites for CSAM may not be in scope; 3) unambitious codes will not incentivise services to innovate and improve child protection, which will impact emerging harms such as gen-AI; 4) none of the proposed measures apply to private messaging services despite the volume of CSAM and grooming evidence; 5) major gap in codes is no proactive tech measures to detect unknown CSAM or to detect and disrupt grooming; 6) no measures relating to livestreaming, despite risks outlined in Volme 2 and evidence of the harm. Approach to the Codes\\t\\tLimiting the most onerous Codes masures to only large/medium/high risk services will limit to OSA\\'s ambition to better protect children. Barnardo\\'s does not agree with the approach to proportionality proposed by Ofcom and is not consistent wit the intent of the Online Safety Act. Barnardo\\'s thinks that Ofcom\\'s proposed approach is primarily economic, rather than considering the severity of the harm. Barnardo\\'s sets out evidence indicating the scale of CSEA harms online, including on smaller services that may not be considered high risk, in comparison with the revenue and value of online services. Barnardo\\'s disagree with the notion that small platforms = less harm and think that Ofcom\\'s approach downplays the harm and misses many risky services, such as gaming services. Cites HoL debate in which the relevant Govt Minister committed to small, risky services being in scope. Barnardo\\'s disagree that implenting measures could hinder services\\' ability to compete and could stifle innovation, and instead considers safety from IH, inc CSEA, should take precedent over economic considerations. Content moderation (User to User) \\t\\tContent moderation proposals are welcome but do not go far enough. Barnardo\\'s support proposal to take illegal content down swiftly, but do not think that this measure goes far enough. Given the harm caused by illegal harm and CSEA, more servces should be in scope of the content moderation measures. Barnardo\\'s disagrees with the weighting given to proportionality. Baranardo\\'s states that proactive tech measures to tackle unknown illegal content should be included. Content moderation (Search)\\t\\tSearch proposals are welcome but do not go far enough. Barnardo\\'s support the proposal for search engines to deindex illegal content, however this measure does not go far enough and Ofcom should propose a measure that search services should activtely detect and tackle unknown illegal content. Automated content moderation (User to User)\\t\\tCSAM automated content moderation proposals are welcome but do not go far enough. Barnardo\\'s agree with the CSAM hash matching and CSAM URL detection proposals, however believe these measures do not go far enough and should be revised. They have set out a few reasons for this: 1) proposals will only apply to a small number of services, many of which already use hash matching and URL detection so Ofcom is just reinforcing the status quo with codes; 2) concerned high threshold for evidence means Ofcom has discarded other measures; 3) concerned these measures will not apply to private messaging services, despite the risks presented; 4) the measures focus on known images and will not detect unknown CSAM, despite tools like Google\\'s ML tool identified over 6 billion new CSAM images in 6 months in 2021; 5) URL detection measure does not include that a splash page should be served when URL blocked, and the measure should be updated to include this. Default settings and user support (U2U)\\t\\tGrooming proposals are welcome but do not go far enough and age assurance should have been recommended. Barnardo\\'s welcomes measures to make online grooming more difficult but has concerns with the approach suggested. Barnardo\\'s raises particular concerns about: 1) age assurance measures should have been included in this version of codes as self declaration is not adequate, agree AA should be in next iteration; 2) restricted scope of measure risks displacement of offenders to services not in scope which have fewer protections for children; 3) the settings being \\'default\\' and not \\'disabled\\', as this puts an onus on children to protect themselves, and children could be at risk if they face pressures to deactivate the safer settings (either by peers or exploiter) which Barnardo\\'s has evidence to support and suggests specific meassaging that should be provided to children if they seek to deactivate the settings; 4) ACM measures should be introduced to tackle grooming, such as keyword detection and machine learning (specifically reference Swansea University Project Dragon-S); 5) measures should be introduced to identify offenders and signpost them to support, such as detecting suspcious activity, adding accounts that sexualise children, searching for egregious/coded terms, or creating fake profiles;\\nAutomated content moderation (User to User)\\t\\tConcerned by a lack of a definition for content communicated privately and we disagree that the communication of this content would not engage Article 8 ECHR. \"E2EE is not specifically mentioned in the Online Safety Act.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9880332946777344,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It is understood to be an intended interpretation of the phrase “content communicated privately,”  and is implied in Ofcom’s statements that proactive measures in S.10 cannot be applied to E2EE services. The Guidance on this point lacks clarity. Ofcom has sought to define content communicated “publicly” but has not defined content communicated “privately.” It is unclear as to whether Ofcom is looking at the technical delivery of the service or the path of an individual piece of content. The question of whether the communication of the content is private, does indeed engage Article 8 ECHR. It applies not only to the individual pieces of content, but also to the service used for communication. There is case law to this effect. [provides case law]We assume that “content communicated privately” means encrypted services, as has generally been assumed. We recommend that Ofcom provide a positive confirmation of the meaning the ‘privately’ in the context of the Online Safety Act, where the method of communication (for example the use of E2EE) is evidence that the communication was private\"\\nContent moderation (User to User) \\t\\tSpeaks of the risks of content moderation on encrypted services\\t\"Client-side scanning is not a viable solution for content moderation in encrypted environments due to issues of inherent systemic risk and the violation of user trust. For example, putting the hash algorithm onto the client device would open it up to reverse engineering. The average user’s expectations of privacy would be violated while criminals and hostile state actors would encounter little more than a speed bump that they would quickly develop techniques to circumvent. Compliance with any measures to screen encrypted content would therefore introduce systemic weaknesses that would endanger users’ security\"\\nStatutory tests\\tTechnical feasibility and proportionality of encrypted services\\tDisparity between our Codes and guidance\\tWith regard to technical feasibility, Ofcom has stated in the Consultation that it cannot impose measures on encrypted services that are not technically feasible. We are grateful to Ofcom for its acknowledgement of this position. However, as we’ve said in our response to Question 1, there is a disparity in Ofcom’s position. In Volume 2, Ofcom asks for an extensive risk assessment of harms on encrypted services. The assessment concerns around a dozen harms, detected using a variety of content moderation technologies. In the Introduction to the whole Consultation,  it says that encrypted services still have to mitigate risks of CSEA on their services (without saying what that means) and are subject to all the safety duties in the Online Safety Act (which is contradictory to the Act)\\nStatutory tests\\tProportionality of our approach of encrypted services\\tDisparity between our Codes and guidance\\t\"A key factor in the proportionality assessment for an encrypted service, is the possibility of arbitrary surveillance of users who are not the target of the measures, sometimes referred to as “collateral damage”. It’s important to consider the big picture, rather than individual measures, and look at the regime that is being created and that Ofcom will oversee. The question is whether it creates “collateral damage” by interfering in an arbitrary way with the rights of innocent users. On an encrypted service, the creation of backdoors and systemic vulnerabilities and weaknesses is known to result in that kind of interference, as the ECtHR stated. We feel the guidance would benefit from a redraft to clarify the proportionality of the measures and set out the fair balance of rights.\"\\nStatutory tests\\tProportionality of our approach of encrypted services\\tDisparity between our Codes and guidance\\t\"A key factor in the proportionality assessment for an encrypted service, is the possibility of arbitrary surveillance of users who are not the target of the measures, sometimes referred to as “collateral damage”. It’s important to consider the big picture, rather than individual measures, and look at the regime that is being created and that Ofcom will oversee. The question is whether it creates “collateral damage” by interfering in an arbitrary way with the rights of innocent users. On an encrypted service, the creation of backdoors and systemic vulnerabilities and weaknesses is known to result in that kind of interference, as the ECtHR stated. We feel the guidance would benefit from a redraft to clarify the proportionality of the measures and set out the fair balance of rights.\"\\nStatutory tests\\tProportionality of our approach of encrypted services\\tDoes not agree with our general approach of proportionality\\t\"This appears to combine a human rights assessment with an assessment of economic proportionality (cost-benefit analysis for providers), offset against the anticipated “benefits” of content removal. The weakness of this approach is that it does not provide insights into the potential downsides and in so doing it misses some important outcomes that would influence decision-making. [gives a case study to demonstrate our measures would be unlawful on encrypted services]\\nMapping this onto the Online Safety Act, long-standing protections for British citizens against State intrusion into their private lives could be undermined if such measures were required. \"\\nAutomated content moderation (User to User)\\t\\tDisparity between our Codes and guidance\\tWe welcome Ofcom’s proposals, to hold back on the deployment of automated content moderation technologies that engage the right to freedom of expression, acknowledging the strong risk of interference. It is not clear whether these intentions are embedded in the overall Guidance. Providers of encrypted services are asked to make a risk assessment of encrypted services for around a dozen different offences. Content and behaviours are described that could only be addressed by the automated systems. We interpret this as an indication of a future direction, where Codes of Practice could ask providers to monitor for these offences. Approach to the Codes\\tSegmentation\\tDisagree with small services not doing as much\\t\"Ofcom\\'s guidance and public stance of ‘ensuring that the burden on smaller or less-resourced businesses is not disproportionate’ appears to be skewed to interpret that smaller or less-resourced businesses should be exempt from preventing illegal harm. Would the same logic apply in other sectors, where health and safety of consumers is paramount; that the cost of\\ndoing business should not require investment in solutions to keep consumers safe? Should the size of the organisation be measured or rather the risks assessed in terms of 4Cs - content, conduct, contact, contract to minors?\"\\nGovernance and accountability \\t\\tDisagree that there should be a differential burden and treatment for large vs small orgs\\t\"However, we disagree again with the argument that there should be a differential burden and treatment for large versus small organisations, where smaller startups ‘might not be expected to implement as many of the recommended mitigations’. Safety tech providers have built solutions\\nwhich can be easily integrated by large or small organisations in just a couple of hours. Neither the cost or effort are out of reach for small organisations. The requirement for smaller and larger organisations to use safety tech to comply with regulation will encourage more competition\\nbetween safety tech suppliers ensuring services are affordable\"\\nGovernance and accountability \\t\\tDisagree with specific segmentation of measures\\t\"We disagree in a number of areas: We think that it would be preferable to extend monitoring and assurance to all services with specific risks, not just the largest platforms. In particular we suggest that protections from grooming should be applied to all sites, not only those which already do age assurance as this creates a perverse incentive not to assure age. We suggest that there is a rephrasing to “means of knowing users are under 18” rather than the term ‘identifying’ which has other connotations. We suggest that Ofcom should set out the range of ways by which sites may already know or suspect that users as under 18 (self-declaration, profiling or marketing, research findings, evidence of the age of users on similar sites). We would ask Ofcom to extend adult user controls to all users, including children, by default. Given the importance of prevention, we would ask Ofcom to require processes to detect new CSAM, for all sizes of sites. We ask Ofcom to require age assurance for performers to preventing underage performers - on all sizes of sites; not just the largest platforms. We would ask Ofcom to explicitly state how it will work across the eco system to extend its support and education to all sizes of platforms. We\\nwould like to see transparency as to how Ofcom is working in conjunction with payment processors and advertising networks - given the adage ‘follow the money’ - in order to really bring to bear the spirit of the Online Safety Act in full and make it a level playing field across all organisations.\"\\nGovernance and accountability \\tAudits\\tInternal audit functions of large organisations\\tWe would advocate that the internal audit functions of large organisations which may own many titles or sub brands, as well as regulators themselves, and payment processors and ad networks can deploy supervisory technology to assess if appropriate and effective measures are in place. We are aware and have shared with Ofcom, details of services to scan for compliance, which are available and accessible to all organisations in the ecosystem\\nGovernance and accountability \\tAudits\\tMutual recognition of international certification \\tWe would like to see Ofcom and other regulators becoming more active in the field of mutual recognition of international certification and international standards, such as those being developed by the IEEE and ISO. This could be a topic for consideration by the Global Online Safety Regulators Network and the international working group across data protection regulators. For instance, Yoti has been approved during a lengthy process by two regulators the FSM and KJM, over a couple of years, by the ACCS and the NCC Group in the UK, as described in the paragraphs below. The KJM for instance has recently undertaken more research into minimum standards for data minimised age checks, requiring specific stages to be undertaken. It would be useful for all involved that minimum standards and audit processes can be understood by other nations; rather than every country replicating the same due diligence. [provides details of the certification they went through elsewhere]\\nApproach to the Codes\\t\\tDisagrees that small services should do less and that they are less risky\\tCurrently we see that there is a huge gulf between the requirements for large companies (with over 7m+ monthly users) and then all other medium, small companies. Based on the track record of the VSP regime, where several years after implementation, there are still no penalties for non compliance even of the largest platforms; this sends very unclear messaging to organisations large, medium and small. Volume 4 states that ‘small’ companies will be exempt from following many of the measures to avoid incurring costs. Yet, even a small hamburger joint still has to abide by health and safety standards. It seems odd that 7m monthly users has been set as the bar for ‘large’. Would we let a food chain with 6 million UK customers not be obliged to invest and follow stringent health and safety standards? We would question, why should the bar for important (child) consumer protection be set so much lower online than offline? It is also important for Ofcom to reconsider its assumption that a ‘small’ service will lead to ‘less harm’ due to less reach. In particular this does not take into the account the severe harm to minority groups who may be targets on small sites. Approach to the Codes\\t\\tFeel we should to more safety by design measures\\t\"We think that the key objective should be prevention upstream, rather than ‘take down, downstream’. In our view the choices need to be made at the design phase, to achieve ‘safety by design’. This should happen before there is content subsequently flowing through the system. Platforms should be incentivised to build with safety by design principles and be held accountable for the harms that their design choices allow downstream. As it is currently written, sadly the regime does not seem to deliver the safety by design outcomes it set out to achieve; rather to offer tick boxes for companies, to show how they may comply. OFCOM’s current focus on reactive take down will not be an effective approach for preventing unconsented publication of intimate images and videos on user to user generated content platforms. \"\\nApproach to the Codes\\t\\tFeel the balance of FoE vs adverse impacts on fundamental rights is wrong\\t\"We are concerned by the tone and stance, which prioritises users’ freedom of expression above the adverse impacts on fundamental rights of others. There seems to be a concern for inconveniencing users; without actual understanding the proportionality of preventative measures.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992286205291748,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'For instance an age estimate takes a consumer on average 1 second. The actual impact or ‘burden’ of putting in upstream mitigation or safety tech tools is very low. For a platform to put place age assurance, consent mechanisms such as e signatures with a range of methods takes approximately two hours. We would also point to how experiential research of users attitudes\\ntowards age assurance can be supportive. In contrast Ofcom has complied evidence of the lifelong impact of harms and abuse on victims, the impact in terms of their mental and economic wellbeing and digital participation subsequently. This does not appear proportionate\"\\nApproach to the Codes\\t\\t\"process by which technologies are\\nassessed for inclusion, such as technologies developed to prevent\\nthe sharing of non consensual image sharing\"\\t\"We would welcome more information published by Ofcom as to ‘the particular needs of [the] United Kingdom user base’ and ‘the likely reading age of the youngest person permitted to agree to [a provider’s] service’s terms of service’ (A5.5 and A5.6). We would like to understand the process by which technologies are assessed for inclusion, such as technologies developed to prevent the sharing of non consensual image sharing. It would seem logical in terms of standard practise for Ofcom to require the assessment of age and collection of consent by all parties uploading adult content to an adult content platform or ‘fan site’, regardless of size. It would seem logical for Ofcom to refer to well established technologies such as e-signatures for gaining\\nconsent and age assurance for ascertaining that only adults can upload content and that co-performers are over the age of 18 and have provided their consent.\"\\nApproach to the Codes\\t\\tConsent from adults to upload images\\t\"Similarly in terms of content moderation, it would seem logical for Ofcom to require sites to ascertain that all adult content from all performers has been uploaded with consent by an adult and to screen that all new actors in content are over the age of 18 and that content from banned or self-excluded people is screened out and that people are checked against trafficked or missing persons lists. To give more granular detail, this would require\\n- Identity checks to ensure new performers are 18+\\n- Authentication check to ensure returning performers are 18+\\n- Selfie verification to ensure the right person (performing in the content) is signing the consent form\\n- Liveness technology confirms a real person is signing the document, preventing people from using a photo or video to spoof the system\"\\nApproach to the Codes\\t\\tDisagree with applying most onerous measures to large and or medium or high risk services\\tWe do not think the measures are too inconvenient for users or too high a burden for platforms of any size. As an exemplar, we are already working with platforms of all sizes. We understood that the aim was to apply safety by design measures for all services to make the UK the safest place in the world to be online and not just on platforms with over 7m monthly users. Approach to the Codes\\t\\tDisagree with 7m threshold\\tWe disagree that 7m should be the appropriate level for ‘large’. 7m is much too high a threshold to define a large service. Approach to the Codes\\t\\ton ACM/conmod. Disagree with segmentation of the measures\\t\"As above, we disagree that solely automated content moderation, after the fact, should be the sole mechanism. The focus should be on preventative measures. In addition to hash matching, there should be a requirement to apply automated age assurance to detect potential newly generated CSAM. We do not think that only larger services should be in scope.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9919838309288025,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We believe that smaller services with a specific risk should be required to adopt Internal content moderation policies, having regard to the findings of risk assessment and any evidence of emerging harms\\non the service. It is illogical that the number of risks would affect the degree of management of the risk. There is not a high burden of time or effort; the focus should be on creating a level playing field. We would advocate that smaller services with a specific risk relating to content moderation should also have targets for content moderation functions and prioritise content review in line with multi-risk services. Similarly, if the specific risk that applies to the service is one that is mitigated by content moderation, staff should have training and materials to identify and take down illegal content.\"\\nApproach to the Codes\\t\\tCosts of preventative CSAM measures\\t\"We question as to why there is no inclusion of costing of preventative measures for CSAM in terms of costs? (e.g. there is a section ‘Further analysis on CSAM hash matching measures’..as\\nthough this was the only possible approach). It is worth considering the costs for law enforcement and civil society will continue to spiral, if preventative measures are not deployed across the ecosystem, with regulators working in conjunction with payment processors and ad networks as well as platforms.\"\\nn/a\\tAge assurance \\tAge assurance \\tWe provide a link here to rate card levels of e signatures as just one example of an approach that can support content uploaded to adult content platforms is given with consent and by an adult and that coperformers in content are also over 18 and have given consent. We provide a link to our blog where we mention the free offer of the Yoti reusable digital identity app, for sharing a data minimised 18 plus attribute; from within the Yoti Age Verification Service (AVS), where we offer a range of age assurance services. We would ask Ofcom to look at tokenised approaches to age assurance and to work with co regulators in the Global Online Safety Regulators Network and across the EU to consider interoperable, tokenised approaches. This requires some very basic reflection as to how long should a token last in certain contexts; eg.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916661381721497,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'for access to adult content. Despite the fact that the OSA mandates Category 1 services to provide adult users with controls for specific types of content and includes the ability to filter out non-verified users. This implies that Category 1 services must offer all adult users the option to verify their identity. It is stated that the verification process can use any method and doesn\\'t necessarily require documentation. However, there seems to be no costing or further detail provided as to verification options; which is very peculiar. Content moderation (User to User) \\t\\tDisappointed there wasn\\'t more on verification and optional user verification\\t\"we had expected to read more from Ofcom about verification and optional user verification, given the following,\\n1) ‘We note that other types of services (such as marketplaces or dating services) also operate forms of verification schemes and we would like to expand our policy thinking as our evidence base on illegal harms and remedies grows”. 2) the broad definition of “measures” offered in section 10 includes “functionalities allowing users to control the \"\"content they encounter”. 3) “Pseudonymity and anonymity” is highlighted in the introduction, identified alongside end to end encryption, live streaming, and recommender systems as 1 of 4 functionalities which “stand out as posing particular risks”. It would appear common sense that optional user verification\\nwould be a sensible measure to reduce and prevent illegal harms across a wide range of platforms. We do not follow the logic as to why the potential future existence of these additional duties for Category 1 platforms can justify not considering optional user verification as a measure to reduce illegal harms across platforms, in the light of Ofcom’s duties under section 41(3) of the Act. We were puzzled as to why Ofcom were able to assess proportionality in all other areas, but were “currently unable to assess the proportionality of a recommendation that services apply\\nany sort of IDV measure to comply with the illegal content safety duty”. We would be grateful if Ofcom could explain on what basis it was unable to complete its assessment - e.g. does it lack information about the efficacy of verification approaches or the costs of implementation or the measurement of the “chilling effect” in relation to impact of individual users on others..\"\\nUser access to services (U2U) \\t\\tFeel our measure on blocking/muting is weak\\t\"We presume that user verification could also support users “options to block or mute other user accounts on the service (whether or not they are connected on the service), and the option to block all non-connected users”. Surely this would fall down, if users could simply just shut one account where they have transgressed community standards and create a new account, with no\\nauthentication measures. We would point Ofcom to research on the role that anonymous\\nand fake accounts play in enabling fraud. [provides evidence]\"\\nDefault settings and user support (U2U)\\t\\tQuestions the lack of verification measures for the grooming measures\\t\"‘We therefore provisionally consider that services should only be in scope of this measure if they have existing means of identifying child users, whether that is a form of age assurance or another method.’ We presume that this has been written in error; otherwise it would be very odd to indicate to sites, that you’re not in scope if you have not yet put any measures in place.. We have clear evidence that it takes only a couple of hours for a small service to integrate and put in place a range of age assurance options. The term ‘identifying’ gives the impression that the full identity details of an individual are required; rather than a data minimised age attribute. We would suggest that it be made clear in c) that only the data minimised age attribute is required from a hard identifier or from a reusable digital ID app. c) Age verification using hard identifiers – This can include asking a user to input credit card details, open banking or capturing information from a photo-ID document uploaded by the user. We would strongly disagree that self-declaration on its own is sufficient and therefore should not be included in this list.\"\\nAutomated content moderation (User to User) \\t\\tDo not think approach should be limited to three take down proposals\\tWe do not think the approaches should be limited to three take down proposals, as detailed above, there should be equal focus on preventative measures. We are concerned OFCOM has not yet gained sufficient understanding to recognise that unintentional unconsented publication; of genuine porn, or deep fake lookalike porn, can be prevented using existing safety tech and this safety tech is already ‘in market’\\nAutomated content moderation (Search) \\t\\tTakedown measures won\\'t work\\t\"We strongly disagree that these takedown measures alone will be effective. We would invite Ofcom to look at other prevention techniques currently available.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916165471076965,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We also fear that the focus is just on the largest platforms; when a level playing field is needed. The harms do not only manifest in large platforms. Safety tech approaches can serve all sizes of organisations. As per the government Safety tech sector analysis report [provides link] There are now over 115 safety tech businesses based in the UK…These are essential technologies in the fight against online harms, and they ensure that a wide range of digital platforms have\\nthe tools they need to keep their users safe online.\"\\nDefault settings and user support (U2U)\\t\\tWant more age assurance, and think self-delcaration should be rules out. \"We welcome the inclusion in this document of recommendations to use age assurance technologies (‘Where services are already using age assurance technologies, they should use these to determine whether someone is a child for the purposes of the protections set out below’), however regret the suggestion that other providers may resort to using the outcome of self-declarations ‘for the time being’. We would repeat the need for a speedier delivery of the roadmap (‘Ofcom’s approach to implementing the Online Safety Act’) to regulation, particularly on age assurance. - [they continue to explain their logic for a few paragraphs.]  Therefore we disagree with 18.80, and believe self-declaration should be ruled out. Finally, a point that we will repeat throughout the documentation is the need for independent third party auditing and benchmarking of the effectiveness of age assurance solutions in order to help better guide relying parties when they look to put in place measures to mitigate the harms arising from their risk assessments. Indeed, whilst very large platforms may have the resources to conduct internal studies into each type of age assurance technology, this will not be true for the overwhelming majority of providers.\"\\nRecommender system testing (U2U) \\t\\tConsider more upstream (safety design) features\\t\"Again we would encourage Ofcom to consider upstream what can be done to deter illegal / CSAM content to be uploaded from the outset, how to gather consent and ensure uploading is only possible from adults and that all individuals in content are over 18 and have provided consent. We would ask Ofcom to review the outcomes of the government\\nfunded Safety tech Challenge Fund [provides evidence link]\"\\nUser access to services (U2U) \\t\\tEncouraging Ofcom to recommend providers with specific certificates\\t\"In vol 4, 21.7 While we do not propose to recommend identity verification in our Codes for illegal harms, we note that, under the Act, ‘Category 1 services’ have an additional specific duty to offer optional identity verification as a user empowerment tool. We will issue guidance in respect of the user empowerment duty for Category 1 services in later phases of our work. We would encourage Ofcom to bring forward this work and to engage with the vibrant digital identity verification provider community in the UK to develop clear guidelines We would recommend that Ofcom recommend providers in scope of the regime use a certified IDSP only, and further that they specify minimum standards of verification so as to add the highest level of trust and assurance to the whole technological supply chain. For instance, a document upload, without liveness detection, face matching or a document authenticity check would not achieve a high level of assurance. At the time of writing, there is a very healthy ecosystem of providers certified to the trust framework, to support this.\"\\nUser access to services (U2U) \\t\\tProposing rewording\\tWe would suggest a rewording of ‘requiring users to verify their age has the potential to prevent children from being exposed to other illegal harms’ to read ‘requiring the implementation of age assurance helps prevent children from being exposed to other illegal harms’. This is again because we do not believe verifying the full age of users is proportionate in all circumstances, where either the selective disclosure of an age attribute or facial age estimation could provide a quicker, less disruptive and more privacy-preserving solution. This must be made clear throughout the guidance. To the point raised in 21.108 (‘There are a range of age assurance techniques available which are capable of achieving varying degrees of accuracy and effectiveness’), we would repeat that we would like to see age assurance techniques independently assessed. We will continue to support Ofcom’s work in this field. User access to services (U2U) \\t\\tProposing to re-outline measures around CSAM\\t\"As mentioned above, as a provider of identity verification and age assurance approaches, we would be happy to engage with Ofcom to re outline measures:\\n-to ensure that those uploading content are over 18 and that all performers are over 18 and provide consent. -to provide a range of approaches for optional or platform mandated verification\\n- to support content moderation\"\\nService design and user support (Search) \\t\\tShifting to prevention\\tWe reiterate our recommendation that the focus should shift to prevention, rather than solely take down. Cumulative Assessment  \\t\\tDisagree \\tThey outline throughout their response as to why they do not feel it is proportionate for small and micro businesses (feel they should apply more measures to them)\\nCumulative Assessment  \\t\\tDisagree \\tThey outline throughout their response as to why they do not feel it is proportionate for small and micro businesses (feel they should apply more measures to them)\\nCumulative Assessment  \\t\\tDisagree \\tThey outline throughout their response as to why they do not feel it is proportionate for small and micro businesses (feel they should apply more measures to them)\\nStatutory tests\\t\\tHard to assess as VSP regime has not published any findings yet\\tAs we have said elsewhere in the document, we would suggest that it is hard to assess whether the Codes are appropriate as drafted and the statement in 30.4 ‘Ofcom has decided to adopt a supervisory approach that builds on Ofcom’s experiences of developing supervisory relationships with the video sharing platforms we already regulate’ as we have not seen a report on the VSP regime published in 2023. From our experience, organisations which are subject to this regime, are still not complying, as they do not feel at risk of any enforcement penalties. This is three and a half years after the VSP regime came into force on 1 November 2020. Statutory tests\\t\\tDisagree about smaller sites not doing measures\\tWe also contend that smaller websites should not be relieved of safety obligations. Safety technology is imperative for all sites, regardless of their size. For instance, in parallel with playground equipment or small food retailers, all must comply with specific health and safety standards regardless of size. All licensed premises regardless of size have to comply with performing age checks to prevent the illegal sale of alcohol to under 18s. If regulators only focus on larger businesses complying with safety regulations, it creates a perverse situation where smaller businesses gain market share by not complying, currently larger sites lose market share by complying with regulations and children are able to access age restricted content from smaller businesses. This is not a good regulatory effect.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916054010391235,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Governance and accountability \\t\\tProposed governance and accountability measures seem a sensible starting proposition and need to be ratcheted up towards maximum ambition as the Codes of Practice become established. Proposed governance and accountability measures seem a sensible starting proposition and need to be ratcheted up towards maximum ambition as the Codes of Practice become established. Right to focus initial efforts on establishing who within services has primary responsibility for illegal content duties. Staff members with clear, established responsibility should face sanctions if those duties are not fulfilled. The suggestion that in time senior manager remuneration should be tied to positive online safety outcomes would be an effective sanction. Approach to the Codes\\tSegmentation\\tWhen segmenting measures, Amazon think we\\'ve put too much emphasis on size instead of prioritising risk (negatively affecting low risk, large services). Ofcom’s current draft proposals disproportionately apply obligations based on size, instead of prioritising the risk level presented by a service. This focus detracts from the overarching principle of the OSA and could likely lead to unintended consequences for low-risk, large services while failing to recognize that bad actors are equally, or even more likely, to abuse smaller services that lack the capacity for appropriate controls\\nApproach to the Codes\\tSegmentation\\tAmazon think an \"ancillary\" exception should be created, where U2U content is ancillary to the core purpose of the service. This would help ensure measures are proportionate. We welcome Ofcom’s acknowledgement that there are different levels of risk and Ofcom should also consider the functionality and proliferation of U2U content when determining the risk level of a service. An ancillary exception should be created when U2U content is ancillary to the core purpose of the wider service, as compliance measures should not be disproportionate to the limited risk associated with part of a service\\nApproach to the Codes\\tSegmentation\\tAmazon supports setting out different risk levels (single, specific, multi), but it thinks we need further differentiation between these categories to better focus on high risk services. It notes that, for example, some of the 15 harms overlap, resulting in one type of risk covering multiple harm categories. Defining multi-risk as two or more, risks miscategorising a significant number of services as high risk. Whilst we welcome Ofcom setting out different risk levels (single, specific, multi-risk) we would urge greater differentiation between these categories and to have a clearer focus on high risk services. As drafted, multi-risk requires only two or more of the 15 harms, some of which overlap, rendering it highly likely that what is essentially one type of risk will be treated as covering multiple harms categories. Using the potential engagement of two or more of these overlapping categories to impose heightened general compliance obligations risks miscategorising a significant number of services as high risk. This blurred categorisation may also lead to subjective and inconsistent application of the OSA, and weakens the use of risk assessment as a meaningful tool for businesses to identify and address potential harms. Approach to the Codes\\tSegmentation\\tSegmentation should be focused on risk (and proportionality) not size. In order for the OSA to reach its goal of making the UK safest place in the world to be online, Ofcom should reserve the most significant obligations for services with the highest risk of harm. We are concerned current draft proposals could lose sight of the OSA’s emphasis on risk and proportionality, and instead adopt an approach which has an undue focus on size. Governance and accountability \\tSegmentation\\tIt should not be the case that the obligations for small/medium sized services of high risk are less than obligations for large low risk services. Many of the proposed compliance obligations for small or medium-sized services with high risk are less than the obligations for larger services with low risk3 (Consultations at a Glance, p.2). For example, small services at medium or high-risk for a specific kind of harm or for at least two different kinds of harms do not need to have a corporate board review of (i) the assessment of the service’s online harms risk management, and (ii) the monitoring and management of developing risks\\nApproach to the Codes\\tSegmentation\\tIt should not be the case that the obligations for small/medium sized services of high risk are less than obligations for large low risk services. Many of the proposed compliance obligations for small or medium-sized services with high risk are less than the obligations for larger services with low risk3 (Consultations at a Glance, p.2). For example, small services at medium or high-risk for a specific kind of harm or for at least two different kinds of harms do not need to have a corporate board review of (i) the assessment of the service’s online harms risk management, and (ii) the monitoring and management of developing risks\\nApproach to the Codes\\tSegmentation\\tRegardless of size, a high risk service should do a number of the con mod / search mod recommended measures. Regardless of its size, a high-risk organization can and should have senior management oversight of how the organization is addressing potential harms. Similar obligations, including training on OSA compliance, internal content moderation policies regarding the risk assessment findings and evidence of emerging harms, prioritising what content to review, and training staff to identify and take down illegal content are likely better tailored to apply to services based on the frequency and severity of harms presented, considering any systems and processes already in place to mitigate harm. Ofcom should allow for a more proportionate, tailored set of compliance obligations which reflect the risk profile of a service, without focusing solely on size of the service. Content moderation (User to User)  \\tSegmentation \\tRegardless of size, a high risk service should do a number of the con mod / search mod recommended measures. Regardless of its size, a high-risk organization can and should have senior management oversight of how the organization is addressing potential harms. Similar obligations, including training on OSA compliance, internal content moderation policies regarding the risk assessment findings and evidence of emerging harms, prioritising what content to review, and training staff to identify and take down illegal content are likely better tailored to apply to services based on the frequency and severity of harms presented, considering any systems and processes already in place to mitigate harm. Ofcom should allow for a more proportionate, tailored set of compliance obligations which reflect the risk profile of a service, without focusing solely on size of the service. Content moderation (Search)\\tSegmentation \\tRegardless of size, a high risk service should do a number of the con mod / search mod recommended measures. Regardless of its size, a high-risk organization can and should have senior management oversight of how the organization is addressing potential harms. Similar obligations, including training on OSA compliance, internal content moderation policies regarding the risk assessment findings and evidence of emerging harms, prioritising what content to review, and training staff to identify and take down illegal content are likely better tailored to apply to services based on the frequency and severity of harms presented, considering any systems and processes already in place to mitigate harm. Ofcom should allow for a more proportionate, tailored set of compliance obligations which reflect the risk profile of a service, without focusing solely on size of the service. Approach to the Codes\\tSegmentation\\tCan Ofcom consider whether an \"ancillary\" exception (potentially involving a small fraction of customer interactions) for low risk services could be applied? This would help ensure proportionality, and that the regime focuses on the services that pose the highest risk to UK users. As Ofcom notes, “companies like … Amazon run multiple services that specialise in different kinds of functionality and enable different kinds of user interactions. In some situations, only a discrete or peripheral part of a service will be in scope of the regime\", rather than the entire service. We agree that only the U2U element of a service will be in scope of the OSA in these instances. However, for some services, the core function and offerings of the service may have no U2U component, but some minor, ancillary or specific functionality (potentially involving only a tiny fraction of customer interactions) may have a U2U element. We would urge Ofcom to consider whether ancillary exceptions can be introduced for certain low risk services. Functionality is an essential criterion to helping understand the risk profile of a service and it would not be proportionate to impose the same standards on core features and ancillary features. Clearer differentiation would also help the regime remain focused on the services that pose the highest risk to UK users, whilst balancing the need for proportionality. Approach to the Codes\\tDefintion of \"user\"\\tAmazon would like services to be given the discretion to determine how they calculate what a \"user\" is. We would also ask Ofcom to consider the different nuances in what constitutes a “user”. Ofcom has stated that the “average user base… per month in the UK” will be the basis for determining if a service is “large”. Ofcom should acknowledge that companies are in the best position to understand how their customers use and interact with the service, so should be allowed to apply the most appropriate calculation of size in order to address risk. This should also take into account what data on use and access a company has available. Approach to the Codes\\tSegmentation \\tAgree that broad general measures (e.g. conmod/gov) will be less effective when applied to specific risk services. Also supportive of our argument that single risk services, are likely to better understand that risk (and therefore posing a lower risk). We appreciate Ofcom’s acknowledgment6 that broad, general measures which are not targeted to specific risks (such as governance measures and content moderation) will be less effective when imposed on services with a risk of only a single kind of harm. As Ofcom acknowledges, the extent of the risk of harm across the service will tend to be lower in such a case, and a single risk is more likely to already be better understood across the organisation. Approach to the Codes\\tSegmentation \\tAmazon thinks some of the harms haven\\'t yet been defined and there is a a risk that they overlap (therefore, two is too few, where they are representing the same harm). For this reason, Ofcom should reconsider its proposal that the presence (without significant regard to prevalence or risk of harm) of two or more of Ofcom’s Harms Categories, warrants additional general compliance obligations. Several of the harms are yet to be defined and are at real risk of overlap. For example, fraudulent transaction taking place on a service may fall into both the “proceeds of crime offences” and “fraud and financial services offences” categories. Equally, content which is categorised as \"harassment, stalking, threats and abuse\" would also likely fall into the category of \"controlling behaviour\". Content which is categorised as \"sexual exploitation of adults\" would also likely fall into the categories of \"extreme pornography\" / \"intimate image abuse\". Approach to the Codes\\tSegmentation based on 15 illegal harms categories (risk)\\tAmazon thinks we need more thresholds for multi-risk as a service with two harms would face the same measures as one with ten (not proportionate). There are also no further thresholds imposed by Ofcom; a service defined as at risk of two illegal harms would face the same obligations as a service at risk of ten or more different harms. This approach does not appear proportionate to the risk of harm on a service; and we would urge Ofcom to re-consider the thresholds of its categories to ensure that the largest obligations are imposed on the services that pose the most widespread and significant number of risks. Approach to the Codes\\tSegmentation based on 15 illegal harms categories (risk)\\tAlso, by just looking at presence (of a harm), without considering frequency or severity, there is a risk that the risk assessment becomes a flawed tool. The approach set out in the consultation could also lead to the imposition of disproportionate obligations on services which may, in reality, be exposed to only one specific and limited incident type. By designating a threshold based on the presence of a topic, without also measuring important safety factors such as the topic’s severity or frequency, means that the risk assessment is in danger of becoming a flawed tool which presupposes an outcome, rather than being a useful and functional insight to accurately reflect the realities of a service, to improve customer safety, and to advance low-impact design. Approach to the Codes\\tSegmentation based on 15 illegal harms categories (risk)\\tDisagrees with current articulation of multi-risk. Thinks it shouldn\\'t allow for overlapping harms.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9967798590660095,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Thinks it should account for frequency/severity. In order to effectively target online harms, enhanced compliance obligations should arise not as a result of a coincidence that one fact-pattern falls in two closely-related and overlapping Harms Categories, but on the basis of the likelihood and severity of risks inherent to that service. This would also be consistent with the approach adopted by human rights law in the UK and before the European Court of Human Rights, where restrictions on the exercise of rights and freedoms must be justified on the basis of a “pressing social need”. Governance and accountability \\t\\tConsideration for specific training and safeguarding is needed as part of proposals. Agree with proposals, however suggest that those assessing risk should be supported with specific training such as on CSA, safeguarding due to the complex nature of those risks. Governance and accountability \\t\\tAgree with the types of services the measures applies to. Agree with the types of services the measures applies to. Approach to the Codes\\t\\tAgree that more accountability should be placed on services with a higher risk and larger user base. Agree that more accountability should be placed on services with a higher risk and larger user base. Approach to the Codes\\t\\tAgree with our definition of large services. Agree with our definition of large services. Approach to the Codes\\t\\tAgree with our definition of multi-risk services. Agree with our definition of multi-risk services. Content moderation (User to User) \\t\\tAgree with our proposals, but note that the effectiveness will depend on the ability of services to detect illegal content and ability of users to report it. Agree with our proposals, but note that the effectiveness will depend on the ability of services to detect illegal content and ability of users to report it. Content moderation (Search)\\t\\tAgree with our proposals. Agree with our proposals. Automated content moderation (User to User)\\t\\tAgree with our proposals. Agree with our proposals. Automated content moderation (Search)\\t\\tAgree with our proposals. Agree with our proposals. User reporting and complaints (U2U and search)\\t\\tAgree with our proposals. Agree with our proposals. Terms of service and Publicly Available Statements\\t\\tAgree with our proposals. Agree with our proposals. Default settings and user support (U2U)\\t\\tAgree with proposals, but call for more support for children with additional needs. Agree with our proposals. However, they call for stronger recommendations for services to provide default settings for those with learning difficulties, and for services to recommend having a nominated person to report/complain on behalf of victims with additional needs. They also highlight the need for services to provide support to users even once the content is identified as illegal and harmful. Default settings and user support (U2U)\\t\\tEmphasise the need for messaging to be age-appropriate, factual, contain support advice and non victim blaming. Emphasise the need for messaging to be age-appropriate, factual, contain support advice and non victim blaming. Recommender system testing (U2U)\\t\\tAgree with our proposals. Agree with our proposals. Enhanced user control (U2U)\\t\\tAgree with our proposals. Agree with our proposals. Enhanced user control (U2U)\\t\\tYes, suggest that users should be fully informed and have accessibility to information on how to use protective measures. Yes, suggest that users should be fully informed and have accessibility to information on how to use protective measures. Enhanced user control (U2U)\\t\\tThere are positives and negatives of undertaking verifications. Suggest that the positives of having verifications are the increased likelihood of perpertators being caught for creating illegal harms, and that negatives are that it may deter users from excercising freedom of speech and also there may be the increased risk of illegal behaviours offline. User access to services (U2U)\\t\\tAgree with our proposals.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9205233454704285,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agree with our proposals. Service design and user support (Search)\\t\\tAgree with our proposals. Agree with our proposals. Cumulative Assessment\\t\\tYes, believe that smaller services should comply to a scaled version of the measures, in line with their budgetary and structural abilities. Yes, believe that smaller services should comply to a scaled version of the measures, in line with their budgetary and structural abilities. Cumulative Assessment\\t\\tYes, believe that smaller services should comply to a scaled version of the measures, in line with their budgetary and structural abilities. Yes, believe that smaller services should comply to a scaled version of the measures, in line with their budgetary and structural abilities. Cumulative Assessment\\t\\tYes, believe that larger services have the financial and structural capabillity to comply with more measures. Yes, believe that larger services have the financial and structural capabillity to comply with more measures. Statutory tests\\t\\tAgree with our proposed recommendations.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9878286123275757,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agree with our proposed recommendations. Governance and accountability \\t\\tAgree with the proposals. Agree with the proposals. Governance and accountability \\t\\tSome of the measures should apply to all services, including  smaller services. Believe all services, including smaller services, should set out responsibilities of staff, track new evidence of illegal content, set out a code of conduct and ensure the right training is place. Approach to the Codes\\t\\tAgree onerous measures should apply to large and/or medium or high risk services. Agree onerous measures should apply to large and/or medium or high risk services. Approach to the Codes\\t\\tAgree with definition of large services. Agree with definition of large services. Approach to the Codes\\t\\tAgree with definition of mult-risk services. Agree with definition of mult-risk services. Content moderation (User to User) \\t\\tAgree with proposals, but highlight importance of training of staff to cover harms faced by those with protected characteristics. Agree with proposals. Suggest this will ensure a clear and consistent application of content moderation. Welcome recommendation to provide training to staff, as important to raise awareness of the harms faced by those with protected characteristics. Content moderation (Search)\\t\\tAgree with proposals.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9861531257629395,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agree with proposals. Automated content moderation (Search)\\t\\tAgree with proposals. Agree with proposals. User reporting and complaints (U2U and search)\\t\\tAgree with proposals, but would like to discuss inclusion of chartities and reprentative bodies as trusted flaggers. Agree with proposals. In particular, welcome the reference to ensuring accessibility for users with disabilities, as this is a key issue with the current reporting system which people with learning disabilities have told them about. Would also welcome discussion on the inclusion of charities and other representative bodies to be included as trusted flaggers. Terms of service and Publicly Available Statements\\t\\tAgree with proposals, but want us to ensure documents are accessible. Agree with proposals, but would like stronger reference to accessibility of these documents, to ensure people with learning difficulties find them accessible. Default settings and user support (U2U)\\t\\tAgree with proposals. Agree with proposals. Recommender system testing (U2U)\\t\\tAgree with proposals. Agree with proposals. Enhanced user control (U2U)\\t\\tAgree with proposals, but would like to see more on provision of accessible materials to users with learning difficulties. Agree with proposals, but would like to see more on provision of accessible materials to users with learning difficulties. Enhanced user control (U2U)\\t\\tYes, should include requirements for how these controls are made known to users. Yes, should include requirements for how these controls are made known to users. Enhanced user control (U2U)\\t\\tRisk that the voluntary verification scheme may not be clear to all users. Suggest there is a risk that people with learnining difficultues do not fully understand the voluntary nature of a verifictaion scheme. User access to services (U2U)\\t\\tAgree with proposals. Agree with proposals. Service design and user support (Search)\\t\\tAgree with proposals. Agree with proposals. Cumulative Assessment\\t\\tAgree. Suggest the additional requirements seem reasonable, given the number of users and staff that large services have. Agree. Suggest the additional requirements seem reasonable, given the number of users and staff that large services have. Approach to the Codes\\t\\tSuggest some measures should be expanded to smaller services, as believe this would not be disproportionate to the benefits. Suggest there is clear evidence that smaller services also pose a high risk of illegal harms to children. Highlight that some harms (e.g. grooming) originate on large services before offenders move to smaller services to commit abuse, and by excluding smaller services, there is a risk that illegal harms are displaced from large to small services. Suggest that like health and safety laws, services of all size and risk profiles should comply with our risk assessment proposals, governance proposals and content moderation proposals, and that a greater number of services should implement hash matching to detect CSAM. Belive not disproportionate considering the serverity of harm, and spend at which small platforms can grow. Approach to the Codes\\t \\tSuggest our codes of practice could be improved around three areas: child-on-child abuse, detection of unknown CSAM and self-harm/suicidal content. 1) Suggest a seperate approach and measures are needed to tackle child-on-child abuse. They provide some evidence for the scale and impact of this abuse - they believe this abuse is likely to account for a sizeable, if not majority, of online CSEA offences. 2) Suggest as an urgent priority that Ofcom should consider measures to detect unknown CSAM, highlight importance that companies have incentives to continue investing in this area. 3) Think they also had suggestions around the self-harm/sucidal content measures, but this may have got missed off. User reporting and complaints (U2U and search)\\t\\tParents should play a formalised role in user reporting and complaints. Their research suggests that parental involvement is important for younger children. They suggest that parents should play a formalised role in user reporting and complaints, for example by providing parents with terms of services to support parent-children conversations, and an option for children to nomimate a parent (or a trusted adult) to make reports on their behalf. Approach to the Codes\\t\\tIt should be clear that measures only have to be taken when proportionate to do so and when the measure address a harm on the platform\\t\"We would welcome further clarity from Ofcom on the proportionality of certain obligations rather than leaving it to platforms to rely on justifying ‘alternative approaches’ to the draft Codes\". Services should only need to take code measures where \"it would be proportionate to do so and where it directly relates to addressing a specific harm set out in the Act or in the platform’s risk assessment. Some of the examples we include in our response include duties around appeals and complaints for downranking content and the lack of differentiation in the measures and risks that apply to different types of content across services\"(p3). See also more specific examples in Google\\'s answer to Q37. See also answer to Q48\\nGovernance and accountability\\t\\tShould be able to nominate different accountable people for different things\\t\"Given the flexibility that the Act envisages for services to designate a responsible person at the point of enforcement action or an information notice being issued\" the codes should allow large multi-service platforms to pick differnet accountable people for different things. Governance and accountability\\tSegmentation\\tDisagree on definition of large and multi risk\\tSee Google\\'s answers to questions 14 and 15\\nGovernance and accountability\\tExternal audits\\tExternal audits would be a disproportionate burden on services\\tAs well as external audits being a disproportionate burden, Google not aware of any evidence of efficacy, rather could divert significant resources from actually mitigating the risk of illegal content. Estimate costs to exceed $10 million.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924675822257996,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Reference to a NYT article as evidence. Governance and accountability\\tIncentives for managers\\tTying manager remuneration to online safety outcomes not workable nor consistent with Act\\t\"This sort of measure is not workable in practice because there is no principled basis to compare “online safety outcomes\\'\\' consistently and reliably between services. Such a measure is also not consistent with the scheme of the Act and risks improperly shifting focus away from the adequacy and appropriateness of systems and processes.\" More detail on passage of the OSA to support argument. Regulation on individual staff will be an important factor in location decisions. Approach to the Codes\\t\\tMeasures should be more principles-based, less prescriptive\\t\"Our primary recommendation is to simplify the Codes so they are more principles-based rather than being overly prescriptive or mandating specific types of technologies … Provided the measures are sufficiently effective, platforms themselves are best placed to decide on the most appropriate technological measures ... We recommend Ofcom avoid mandating specific technological solutions or, where this is not possible, expressly note in the Codes that these measures are only illustrative examples of how the safety objective can be achieved\". Disadvantages with Ofcom\\'s current approach: risk of incentivising services to implement a less effective measure to demonstrate compliance with Codes; platforms adopting more effective measures penalised by not being in safe harbour; \\'freezes\\' compliance with no flex to accommodate new and more effective measures. Proposed measure do \"not provide platforms with the necessary flexibility to innovate and implement changes at the speed with which bad actors operate\". If Ofcom cannot do this, then it should ensure \"a clear mechanism is in place for platforms to implement/record alternative measures\" in a way that is not unnecessarily burdensome. Proportionality should be baked into Codes more. For more prescriptive measures, Ofcom should (in codes) \"expressly include a proportionality qualification to avoid platforms diverting resources from high/medium risk concerns to low-risk concerns.\" See also answer to Q20, Q25, Q48\\nApproach to the Codes\\tSegmentation\\tOnerous measures should (1) not apply to large services that are low risk and (2) only be applied to high risk specifically (not high or medium risk)\\tFirstly, \" ... more onerous obligations should not apply where a service is only “large” (without also being at high risk of particular content) ...': [{'label': 'POSITIVE',\n",
       "               'score': 0.994753360748291,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'many large platforms have long-established risk mitigations in place ... Furthermore, high user reach is not necessarily indicative of a service being high risk ... Smaller platforms also pose risks to users and if obligations remain tied to size of service alone, it may lead to pushing harmful content onto smaller, but higher risk services\". Secondly, \"It would be beneficial to limit the more onerous measures to services assessed as being “high risk” for one or more kinds of harm only, as this would increase the incentives on services to reduce the risk of harm from high to medium or below.\" Specific amendments suggested. Approach to the Codes\\tSegmentation\\tBar for \"large services\" set too low\\t\"Ofcom suggests that this is consistent with the DSA approach to VLOPs (i.e. based on roughly 10% of the population). However, the DSA also uses a functionality test in practice given that it only applies to search engines and online platforms (which have to allow for the public dissemination of content that is not a minor and ancillary feature).\"\\nApproach to the Codes\\tSegmentation\\tDefinition of \"users\" should allow flexibility, eg to count only signed-in users and various other suggestions\\tDifficult to define \"user\" and varies by services. Some details on this in response. [CONFIDENTIAL: see also Google\\'s response to Ofcom’s Call for Evidence on categorisation for details]. Google suggest some flexibility to be build into count of users, eg only counting signed in users, only users that generate content. \"given the range of users and services, clear guidance of which services are required to proactively provide user counting would be helpful.\" Other specific suggestions and suggested amendments.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9936245679855347,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'One suggestion: \"it would be proportionate ... for a service to ‘self-certify’ that it meets the relevant threshold without needing to provide user counts\"\\nApproach to the Codes\\tSegmentation\\tDefinition of \"users\" should be calculated over 6 months rather than 12 months\\t\"Ofcom has proposed that users should be counted over a period of 12 months, however, it is rare that our services would hold this volume of data, due to data retention policies, and we would suggest that a six month period would give sufficient evidential basis ... We also note that a six month period would align with DSA requirements, so anything exceeding this period would require significant changes to systems and processes that have been developed for DSA compliance.\"\\nApproach to the Codes\\tSegmentation\\t\"multi-risk\" should be changed\\tmulti-risk should be ”limited to those designated as high risk in relation to those offences. Without this change, there would be insufficient delineation between the treatment of services that are medium risk for an illegal harm and services that are high risk for that harm.\" Suggestion: multi-risk changed to \"focus on a) residual risk ... and b) services assessed as posing one or more high “specific risk” of harm.\" [my emphasis]\\nApproach to the Codes\\t\\tMeasures too prescriptive, should be more flexible\\tSee Google\\'s answer to Q12\\nApproach to the Codes\\t\\tApproach to costs excessively broad-brush\\t\"We cannot therefore comment meaningfully on the cost assumptions set out in Annex 14, except to note that ... excessively broad-brush and generalised. We do not think it is possible to generate reliable cost assumptions in this way, as opposed to making cost estimates on a more individual basis, factoring in the nature of the relevant service provider, its processes, and its resources. We expect that the costs of a particular measure will vary significantly due to a number of factors, including: Location of employees ... The complexity of the compliance measure ... How sophisticated existing systems are ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927588701248169,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Level of resource within the product ...': [{'label': 'POSITIVE',\n",
       "               'score': 0.9492631554603577,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'A failure to take these factors into account in a more specific way has the effect of making cost estimates inaccurate and unreliable in all but a limited number of circumstances. We would therefore welcome Ofcom recognising in its approach to regulatory oversight and enforcement generally, that many factors will be relevant to the proportionality of what is expected from service providers when complying with the measures ...\"\\nContent moderation (User to User) \\t\\tOfcom should explicity say that nothing in the draft code obliges platforms to generally monitor for illegal content\\tGeneral monitoring was \"certainly not the intention during the legislative phase, as confirmed from the despatch box, and we understand that is not the policy intent either. However, we believe further clarity from Ofcom would add regulatory certainty and reduce the risk of significant over removal of legal content.\" Want prohibition on general monitoring explicitly referenced through the Codes. Especially for each automated content moderation requirement.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933480620384216,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Specific wording suggested.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8451799750328064,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See also Google response to Q20, page 42-43.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.5147749781608582,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See also page 4 in Google\\'s summary. Content moderation (User to User) \\t\\tRecommend removing the reference to ‘emerging harms’ when setting internal content policies\\t\"To provide more context, YouTube invests significant resources to ensure that we are tracking emerging trends on online platforms, not just YouTube. Our Intelligence Desk monitors the news, social media and user reports to detect new trends surrounding inappropriate content, and works to make sure that our teams are prepared to address them before they can become a larger issue. However, we believe this consideration is more appropriately covered in risk assessments. It may be that there is harmful, trending content observed on non-Google platforms. Whilst it is appropriate to monitor these developments and consider preparedness, practically policies cannot consider the nuance of how these harms may manifest until we can assess how it may impact our own platforms.\"\\nContent moderation (User to User) \\t\\tCurrent wording would hinder existing processes and urge Ofcom to provide more flexibility\\tMetrics \"would require us to maintain separate Trust & Safety metrics for content that is violative of our Community Guidelines and illegal under UK law, thus hindering existing processes and mechanisms to evaluate the efficacy of existing content moderation measures. YouTube, for example, measures illegal content through published “Violative View Rate” (VVR) in respect of YouTube content. The VVR shows how many times content has been viewed before it is removed for breaching our policies. We see these VVR as our “North Star” for measuring our progress in combating harmful content and, although not perfect as a metric, is significantly more meaningful than a simple ‘time on platform’. ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9889920949935913,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We would urge Ofcom to provide flexibility to allow the varying different services to which the Codes apply to use a range of performance metrics that most effectively address the issue outlined.\" Specific changes suggested in response. Content moderation (User to User) \\t\\tShould be more flexibility in how prioritisation is done\\t\"Google currently uses prioritisation frameworks, which include a number of factors, but the policies adopted depend on: the type and severity of harm. For example, CSAM and fraud might have different assessments, to reflect the factors that are present when identifying this type of content; and the type of product, to reflect the differing functionality and risk profiles. For example, YouTube might have a different framework to Google Docs.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9935362935066223,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8511837124824524,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We would suggest that the requirement should be to ensure that the service has an appropriate prioritisation framework that can be explained and evaluated by reference to the risks specific to the relevant service, rather than prescriptively setting out what the framework should be.\" Specific changes are suggested. Content moderation (Search)\\tDeindexing\\tWe should use \"delist\" rather than \"deindex\"\\t\"Deindexing (i.e., deleting content from the search index) doesn’t enable the kinds of flexibility that our products rely on, and which is also demanded by other obligations in the Codes, e.g. in the event of a successful appeal, if content has been blocked from serving through delisting, it can be reinstated to results immediately.\" Also delisting allows country variation. Specific wording suggestions made. Content moderation (Search)\\tDownranking\\tNot clear what downranking search results means\\t\"It is not clear to us what “downrank” means under the draft Codes ... For example, if a user searches for “UK regulator”, Ofcom\\'s website appears as the fifth result. However, if you search for a UK digital safety regulator, it appears as the first result. Does the first scenario qualify as downranking only because the first four results had a higher quality score ... ? Moreover, when speaking about pages with potentially harmful but legal content, applying a penalty to a page might result in it not appearing highly in search results for a general query; but the search content might rank more highly in response to\" more specific queries. \"It’s therefore not clear what “downranking” means in a context where the query has one obviously correct answer\" Specific change suggested on page 40. Content moderation (Search)\\t\\tSearch services can\\'t take account of the \\'prevalence\\' of search results\\t\"The Code requires services to take into account the “prevalence of the illegal content” ... However, search services are not able to determine “prevalence” of content, as they don’t host the site and do not record metrics like violative view rates\". Specific changes are suggested. Content moderation (Search)\\t\\tSearch services should not be required to downrank, rather than delist\\t\"\"\"We also note that the requirement for search engines to either deindex or downrank seems to depend on whether the webpage contains “only a small amount of less severe illegal content and a large volume of valuable lawful content” (Vol 5, p60). We would recommend that services are not required to downrank in circumstances where a URL contains any amount of unlawful\\ncontent, and instead give services the discretion to delist.\"\" Specific changes are suggested.\"\\nContent moderation (Search)\\t\\tCodes should be explicit that removals shoudol be at URL level rather than domain-based\\t\"Further, we would want Ofcom’s Codes to be explicit that any removal request is at the URL level to avoid the risk of over removal. Domain-based actions should be limited to “downranking” or demotions. We apply demotions to domains with a disproportionate density of violative material. For example, sites with a high rate of NCEI reports will receive a penalty demotion, over and above the removal and reporting actions we take for individual URLs.\"\\nContent moderation (Search)\\tPrioritisation in search moderation\\tOfcom should not mandate the factors services should look at in prioritising\\t\"… we do not believe that certain factors should be mandated by Ofcom in the Codes (such as how frequently search requests are made) as it is not always necessary or appropriate to consider these factors, for every type of harm or every type of search service. Such a prescriptive approach could ultimately slow down the review process\".': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934822916984558,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Specific wording changes suggested in response. Content moderation (Search)\\tPerformance targets for search moderation\\tPerformance targets don\\'t work for search, especially for downranking\\tPerformance targets for downranking challenging as difficult to specify to what extent a result is downranked and why. Also lack of clarity in codes on how to measure performance targets. The term \"on the service\" doesn\\'t make sense for search. Specific amendments suggested. Automated content moderation (User to User) \\t\\tWording of measure is not future-proof, eg if better technology comes along\\t\"concerned that mandating specific forms of technology in the Codes is not future-proofed. ... For instance, if in the future a more accurate technology is developed to detect CSAM, there may be a perverse incentive for companies not to adopt this feature or delay it until either Ofcom updates the Codes or companies receive the necessary assurance from Ofcom that it would meet their requirements\". Specific suggestions made, including adding to measure \"or alternative proactive content moderation to combat CSAM which are at least equally effective”\\nAutomated content moderation (User to User) \\t\\tGoogle has better technologies than keyword detection and codes should allow this to be used instead\\t\"Keyword detection can be circumventable and is not always sustainable.\" Google uses alternate measures which it considers better. It gives examples.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928889274597168,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Codes should enable these better technologies to be used instead. Specific wording suggested.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9731797575950623,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See Google\\'s ansewr to Q12 too. Automated content moderation (User to User) \\t\\tGuidance on private/public overly broad and unclear\\tWelcomes guidance, but \"overly broad and unclear approach in stating “The fact that there are access restrictions on a service does not necessarily, by itself, mean that content on that service is communicated ‘privately’. Ofcom would still expect a service provider to consider how many individuals in the UK are able to access the content…”. We think clear access restrictions, which mean content communicated on a service is intended to be shared with a limited group of individuals, should always mean the relevant content is communicated ‘privately’. On a number of our services, users apply access restrictions precisely because they regard that content as private and want it to be shared with a restricted group.\" Otherwise significant adverse implications on users’ expectations of privacy and freedom of expression\\nAutomated content moderation (User to User) \\t\\tDetails on what need to do to ensure CSAM hash matching works well and costs involved\\tGoogle systems work well because do not apply each technology in isolation. \"We have fine tuned the system and matching thresholds to limit false positives and have built additional safeguards that reduce errors, which may include [CONFIDENTIAL: human verification of the accuracy of a hash, human review before content is reported, and human review of appeals made by end-users. The process is not simple and doing this correctly, accurately, and in a privacy preserving manner takes dedicated time, resources, and manpower.]\" Also need reliable hash database. [CONFIDENTIAL: \"We obtain hashes from a variety of highly trusted sources ... we also review purported CSAM hashes that have not been reviewed before independently by Google to confirm its accuracy ... We give partners access to CSAI Match fingerprinting software and an API to identify matches against our database of known abusive content.\" Response describes steps taken to on-board partners and describes costs to partners (eg legal costs, human reviews)\\nAutomated content moderation (User to User) \\t\\tDetails on what need to do to ensure CSAM hash matching works well and costs involved\\tGoogle systems work well because do not apply each technology in isolation. \"We have fine tuned the system and matching thresholds to limit false positives and have built additional safeguards that reduce errors, which may include [CONFIDENTIAL: human verification of the accuracy of a hash, human review before content is reported, and human review of appeals made by end-users. The process is not simple and doing this correctly, accurately, and in a privacy preserving manner takes dedicated time, resources, and manpower.]\" Also need reliable hash database. [CONFIDENTIAL: \"We obtain hashes from a variety of highly trusted sources ... we also review purported CSAM hashes that have not been reviewed before independently by Google to confirm its accuracy ... We give partners access to CSAI Match fingerprinting software and an API to identify matches against our database of known abusive content.\" Response describes steps taken to on-board partners and describes costs to partners (eg legal costs, human reviews)\\nAutomated content moderation (User to User) \\t\\tDetails on what need to do to ensure CSAM hash matching works well and costs involved\\tGoogle systems work well because do not apply each technology in isolation. \"We have fine tuned the system and matching thresholds to limit false positives and have built additional safeguards that reduce errors, which may include [CONFIDENTIAL: human verification of the accuracy of a hash, human review before content is reported, and human review of appeals made by end-users. The process is not simple and doing this correctly, accurately, and in a privacy preserving manner takes dedicated time, resources, and manpower.]\" Also need reliable hash database. [CONFIDENTIAL: \"We obtain hashes from a variety of highly trusted sources ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929007291793823,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'we also review purported CSAM hashes that have not been reviewed before independently by Google to confirm its accuracy ... We give partners access to CSAI Match fingerprinting software and an API to identify matches against our database of known abusive content.\" Response describes steps taken to on-board partners and describes costs to partners (eg legal costs, human reviews)\\nAutomated content moderation (User to User) \\tKeyword detection for fraud\\tMeasure overly prescriptive in recommending keyword detection, which isn\\'t a very good technology\\t\"concerned that Ofcom’s suggestion to use fuzzy keyword matching would lead platforms to adopt an approach that is less effective than the uses of other technologies they have developed ...\" Response has confidential details of why keyword detection didn\\'t work well in the past. See also Google\\'s answer to Q12\\nAutomated content moderation (User to User) \\tTerrorist content hash matching\\tHash-matching for terrorist content most reliable when used within platform (ie content assessed by own platform). GIFCT hashes problematic\\t\"Hash-matching technology is technically possible for terrorist and violent extremist content. It is most reliable where it is used to detect reuploads of violative content (i.e. content that has previously been determined by a platform to be violative of their policies).\"   Various issues with using GIFCT hashes, including these \"hashes ... are not necessarily illegal, but rather are violative of individual companies’ voluntary (and varying) content policies\"\\nAutomated content moderation (Search) \\t\\tCSAM URL measure should not require search services to reinstate URLs after CSAM removed\\tDraft code requires search services to reinsert URLs which previously had CSAM and where it has been removed. This amounts to a \"must carry\" provision.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927901029586792,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Services shoudl be allowed to determine reinstatements.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9400489926338196,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Suggested wording change. Automated content moderation (Search) \\t\\tCSAM warning message measure too prescriptive and method proposed not most effective\\tTriggering warning messages via keywords [CONFIDENTIAL not effective because \"CSAM keywords go stale quickly and our algorithmic triggering is much more effective, given the dynamic nature of the web.\" Illustration included in response]. \"Also, ... code requires services to include links in the warnings to resources designed to help users refrain from committing CSEA offences that are freely available through a reputable organisation dedicated to tackling child sexual abuse. This seems overly prescriptive, giving little room for innovation/adaptation.\" Measures should give more flexibility.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934799671173096,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Some wording changes suggested. User reporting and complaints (U2U and search) \\tAppeals\\tFollowing reversal of decision on appeal, not workable to restore search result exactly to where it was and not always appropropriate\\t\"The draft Code states that if a service reverses a decision on appeal, the search content should be restored to its previous position. However, search results are dynamic and the ranking is always changing\".': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9909432530403137,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Hence current wording unworkable. Also \"even if the content is not determined to be illegal, it might be policy violative and the service provider should still retain the ability to demote or delist\". Suggested wording amendments. User reporting and complaints (U2U and search) \\tAppeals\\tShould not need to amend guidance and any automated technology for isolated appeal, only if systemic issue. Code says if appeal successful, \"relevant moderation guidance should be amended; and any automated technology should be amended to prevent similar issues. However, we feel this should not apply to individual cases but rather should be based on an aggregated assessment (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9365304112434387,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'a spike in successful appeal rates). An individual false positive is not necessarily indicative of a systemic issue\"\\nUser reporting and complaints (U2U and search) \\t\\tShould not be required to do with fewest number of clicks, as can have unintended consequences and be worse for consumers\\t\"On the provision regarding the number of clicks to submit an appeal or complaint, Google does extensive UXR testing to ensure our flows are as user friendly as possible. In our view, the appropriate metric should not be “as few number of clicks as possible” but how intelligible a reporting flow is to users. Focusing solely on the number of clicks creates unintended consequences such as the poor design of the user interface that would in fact discourage reporting or dramatically increase the number of erroneous user reports.\" Includes suggested wording changes. User reporting and complaints (U2U and search) \\tAppeals for demotions\\tSearch appeals should be limited to delisting and not demotion, or else gaming by people who want higher ranking, unrelated to online safety. Also can be issues with notifying of moderation action\\t\"As currently drafted, the draft Codes could allow every webmaster whose site is not listed as the first result to have the right to file an appeal.\" Risk of gaming. \"...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929234981536865,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'much of the way Google Search limits the risk of problematic content appearing in search results ... is closely tied into Search’s core mechanisms for assessing the overall quality of content, a concept that encompasses but goes well beyond the kinds of content risk at issue here. If that leads to a conclusion that Search’s core quality signals are “proactive technology” whose application webmasters can appeal, that will create a serious structural issue for Google. Search appeals should therefore be limited to delisting, rather than ranking.\" Also [CONFIDENTIAL can only inform webmasters of moderation action if they are registered, and don\\'t want to notify owners (eg in CSAM context) where migth jeopardise criminal proceedings]. Potentially issues of personal data with informing webmasters. Includes suggested changes. User reporting and complaints (U2U and search) \\tAppeals\\tAllowing appeals for all content removed is not appropriate\\tFor example, [CONFIDENTIAL:  \"... where mass removals have taken place due to machine-spam, we do not believe there is a freedom of expression or other justification for offering appeals\"]\\nUser reporting and complaints (U2U and search) \\tAppeals\\tIt will be unwieldy and disproportionate for all elements of a service to be complained about and appealed\\tCONFIDENTIAL examples given to illustrate issue include (amongst other things) for YouTube: \"Creators play an important role in moderating comments. They control whether comments are enabled or not, can remove any comments under their videos for any reason, can edit comments under their videos, can create block lists for words or phrases permissible in comments under their videos, and can block specific users from commenting on their videos. As such, YouTube cannot and does not offer assurances that comments will remain on the platform, whether or not they comply with our content policies or the law.\" Freedom of speach issues with comments much smaller. Amendement suggested. User reporting and complaints (U2U and search) \\tAppeals\\tNot always possible to restore user content found on appeal not to be illegal\\t\"Codes require restoration of user content to the position it would have been in had the content not been judged to be illegal ... There may be situations where content restoration is not possible (e.g. because the content is ephemeral/time-sensitive by nature or technical limitations in the design of the platform mean that it is not reasonably feasible to “restore” content).\" Response includes suggested amendment. User reporting and complaints (U2U and search) \\tTrusted flaggers\\tTrusted flaggers should be required to include details of why content is illegal when reporting.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933280944824219,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'For both Search and U2U\\t\"... the Code should require trusted flaggers to include details of why the content is illegal, and not just report the content alone, in order to distinguish the process from a user flag. Further, platforms should be able to assume that where Government, regulators or other trusted flaggers report allegedly illegal content, that they have carried out basic publicly-available checks e.g. FCA-authorised entities, and can provide such evidence to the service\"\\nUser reporting and complaints (U2U and search) \\tComplaints\\tNot necessary or proportionate to always provide user with ability to submit supporting material when complanining\\t\"Codes can be read as contemplating that all complaints mechanisms should enable users to provide supporting material ... There may be situations where it is not necessary or proportionate to provide users with the ability to submit supporting material ... For example, many Google products enforce policies regarding “obscene and profane content”-- a requirement to enable users to provide supporting documents in addition to relevant text-based information when complaining about the removal of such content is likely to be disproportionate ...\". Wording change suggested. User reporting and complaints (U2U and search) \\tAppeals\\tShould not specify factors to include in prioritisation framework for appeals\\tGoogle \"consider it is appropriate to require a prioritisation framework, but there should not be any requirement to include specific factors in the framework, as these may not be applicable to all harms and all products (see further Q18 and 19).\"\\nUser reporting and complaints (U2U and search) \\tAppeals\\tDetailed wording change to appropriate action for complaints requirement\\t\"Following success appeals, we propose action is taken “where necessary to avoid similar errors in future”. \"\"First, it is not necessary to include that language ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929224252700806,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'which reads more clearly without it (because the point is covered by the proviso that the technology “does not cause the same content to be taken down again”). Second, the requirement ... does not take account of the fact\\nthat it is not possible to avoid all errors in content moderation at scale\"\". Suggest removing this wording from both Codes.\"\\nUser reporting and complaints (U2U and search) \\tAppeals\\t\\'spam\\' type complaints likely to work out how to get around system\\t\"we also foresee a large number of non-genuine “spam” complaints. In our experience, bad actors seek to exploit notice and complaints systems to obtain feedback that will enable them to circumvent detection systems. Recognising this risk, the Digital Services Act appropriately includes an exception for “deceptive high volume commercial content”.\" Amendments suggested. User reporting and complaints (U2U and search) \\tAppeals\\tSmall change wanted to clarify wording\\tSuggestion: \"Amend the Codes to clarify that for “all other relevant complaints” services should deal with the complaint in accordance with the most relevant category of recommendations in 5D - 5G.\" Explanation of why in response. Terms of service and Publicly Available Statements\\t\\tExpress recognition that description of ACM in TOS should not be so detailed it can be exploited\\t\"We recommend Ofcom includes an express recognition that platforms should only be required to provide a level of detail regarding their automated technology (or other measures that address priority illegal content) that does not jeopardise the effectiveness of those measures.\"\\nDefault settings and user support (U2U)\\t\\tSupport intention of grooming measures\\t\"We support the intention to provide support to child users so they are protected from encountering illegal harms such as CSAM and grooming.\"\\nDefault settings and user support (U2U)\\t\\tYouTube sometimes described as social media network in consultation which is incorrect\\t\"... in a number of places the draft Codes seem to incorrectly reference YouTube as a social media network, whereas in fact it is a Video Streaming Platform and - like many other U2U services that will be in scope - has different functionalities than social media and would therefore have a different level of risk of harm. For instance, YouTube does not have a direct messaging function nor network expansion prompts.\"\\nRecommender system testing (U2U) \\t\\tRecommender system testing measure overly burdensome and may have perverse resutls\\t\" ... we believe the testing requirements are overly prescriptive and it is unclear how these requirements would actually mitigate risk of illegal content. ... not in line with Act, and not beneficial from a user safety perspective, for the Code to introduce quasi-risk assessments for recommender system design changes\". Too burdensome.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9935429096221924,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Risk that \"platforms are incentivised to only carry out off-platform testing prior to launch. In most cases, this would not be as robust as on-platform testing ... \".': [{'label': 'POSITIVE',\n",
       "               'score': 0.5505724549293518,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Suggested amendments in response. Recommender system testing (U2U) \\t\\tRecommender systems can benefit online safety\\t\"Our recommender systems help connect viewers to high-quality information and minimise the chances they’ll see problematic content. However, where content is judged to be illegal, we do not allow it on our services and we remove it.\" Also arguments that recommender systems can be benefial for online safety in Google\\'s answer to Q2 and Q34. Enhanced user control (U2U) \\t\\tEnhanced user controls measures not proportionate for all types of U2U services\\tSerious concerns with proposals. \"provisions are too prescriptive and … may not be the best means for individual platforms to meet the underlying harms\". Doesn\\'t fit with user empowerment duties in the Act: \"Ofcom is effectively going beyond the scope of the Act\". If nevertheless Ofcom introduces, should be in Phase 3 of work. \"to the extent the provisions remain in some form, we recommend that Ofcom should include clear proportionality provisions, to reflect the fact that not all elements of the Enhanced User Controls are appropriate for all types of U2U platforms\". Long confidential practical example relating this to YouTube, for which detailed requirements would \"require significant product work for minimal gain to YouTube users\". Enhanced user control (U2U) \\tUser verification scheme\\tMeasure overly prescriptive and may means schemes not set up or withdrawn\\tRecommend deleting various requirements about establishing how they verify, people being able to update user profile and requirement to describe functions to users. Concerned about services not having enough flexibility when they have different verification/labelling schemes, not allowing users to update their profiles and unclear on rationale for part of it. Examples relate to how this would work on YouTube. User access to services (U2U)\\t\\tConfidential outline of how user blocking works \\tCONFIDENTIAL: YouTube uses combination of signals to black. Removal approach highly accurate, but appeals process, and continually iterate to respond to evolving threats. \"Unless there are exceptional circumstances, intentional sharing of CSAM results in permanent block from YouTube. ... To mitigate the risk that lawful content is erroneously classified as CSAM by automated systems, services should try not to over-index on the accuracy of individual protections. This is partly because each protection has its own accuracy and needs to be assessed independently. Generally, Google does not apply each technology in isolation, minimising this risk]\"\\nUser access to services (U2U)\\t\\tConfidential outline of how user blocking works \\tCONFIDENTIAL: YouTube uses combination of signals to black. Removal approach highly accurate, but appeals process, and continually iterate to respond to evolving threats. \"Unless there are exceptional circumstances, intentional sharing of CSAM results in permanent block from YouTube. ... To mitigate the risk that lawful content is erroneously classified as CSAM by automated systems, services should try not to over-index on the accuracy of individual protections. This is partly because each protection has its own accuracy and needs to be assessed independently. Generally, Google does not apply each technology in isolation, minimising this risk]\"\\nUser access to services (U2U)\\t\\tConfidential outline of how user blocking works \\tCONFIDENTIAL: YouTube uses combination of signals to black. Removal approach highly accurate, but appeals process, and continually iterate to respond to evolving threats. \"Unless there are exceptional circumstances, intentional sharing of CSAM results in permanent block from YouTube. ... To mitigate the risk that lawful content is erroneously classified as CSAM by automated systems, services should try not to over-index on the accuracy of individual protections. This is partly because each protection has its own accuracy and needs to be assessed independently. Generally, Google does not apply each technology in isolation, minimising this risk]\"\\nService design and user support (Search)\\t\\tCONFIDENTIAL: difficult to exclude the risks that a user might \"encunter\" illegal content\\t[CONFIDENTIAL: proposed measure \"poses a risk to Google Search’s autocomplete feature, as it is difficult to exclude the risks that a user might “encounter” such content. For example, it may be difficult to exclude that the content is possible, but the more common situation is appropriately contextualised, lawful material].\"\\nCumulative Assessment\\t\\tsignificant concerns about assumption that risk is directly proportionate to size\\t\"We have significant concerns around Ofcom’s apparent assumption that risk is directly proportional to the size of a service without also taking into account other factors. This is most evident in i) the approach to evidence needed for the risk assessments; ii) the approach to assessing impact of harm; iii) the suggested governance measures; and iv) the approach to the significant change trigger for reviewing risk assessments.\"\\nCumulative Assessment\\t\\tsignificant concerns about assumption that risk is directly proportionate to size\\tSee Google\\'s answer to Q45\\nStatutory tests\\t\\tConcerns about codes being overly prescriptive, insufficient consideration of freedom of expression and users\\' rights of privacy\\tConcerns about: some measures being too narrow and presciptive (see also answer to Q12); need greater consideration of freedom of expression; impact on users\\' rights of privacy. On the latter, Google \"would like to ensure that the online safety regime is compatible with requirements under data protection laws (such as UK GDPR). In particular, we would welcome clarification in the Codes and guidance that services are not required to collect or process additional personal data from their users in order to comply with the requirements of the Act\" On privacy, see also the ansewr to Q7. Automated content moderation (User to User)\\t\\tOfcom shouldn\\'t require E2EE services adopt client side scanning. This is b/c it would compromise the security of the service (noting various pieces of evidence). It would introduce vulnerabilities that the OSA prevents Ofcom from requiring. \"The Guidance should also clarify that with respect to that risk, Ofcom will not require E2EE services to adopt measures such as client-side scanning. This is required because the duty to adopt proactive measures does not apply when such adoption is not technically feasible without compromising the security of a service. There is more than ample evidence that client-side scanning would indeed compromise the security of E2EE services, as shown in particular by this report, Bugs in Our Pockets, produced by noted computer security experts. They point out that a client-side scanning requirement to address the risks posed by CSEA would introduce the type of security vulnerabilities that the OSA prohibits Ofcom from requiring. Other research, including a study by the European Parliament Research Service regarding the proposed EU CSAM regulation confirms that currently there are no technically feasible means of allowing access to content on systems that are encrypted end-to-end without compromising the security of the system as a whole. Client-side scanning can be insufficiently accurate at detecting CSEA, result in false positives, and be vulnerable to “poisoning attacks” on hash databases used in detection efforts. The draft Guidance properly restricts the use of hash matching to detect CSEA to content that is publicly communicated, but encryption is missing as a factor to consider when determining whether content was communicated publicly or privately.\"\\nAutomated content moderation (User to User)\\t\\tOfcom shouldn\\'t require E2EE services adopt client side scanning. This is b/c it would compromise the security of the service (noting various pieces of evidence). It would introduce vulnerabilities that the OSA prevents Ofcom from requiring. \"The Guidance should also clarify that with respect to that risk, Ofcom will not require E2EE services to adopt measures such as client-side scanning. This is required because the duty to adopt proactive measures does not apply when such adoption is not technically feasible without compromising the security of a service. There is more than ample evidence that client-side scanning would indeed compromise the security of E2EE services, as shown in particular by this report, Bugs in Our Pockets, produced by noted computer security experts. They point out that a client-side scanning requirement to address the risks posed by CSEA would introduce the type of security vulnerabilities that the OSA prohibits Ofcom from requiring. Other research, including a study by the European Parliament Research Service regarding the proposed EU CSAM regulation confirms that currently there are no technically feasible means of allowing access to content on systems that are encrypted end-to-end without compromising the security of the system as a whole. Client-side scanning can be insufficiently accurate at detecting CSEA, result in false positives, and be vulnerable to “poisoning attacks” on hash databases used in detection efforts. The draft Guidance properly restricts the use of hash matching to detect CSEA to content that is publicly communicated, but encryption is missing as a factor to consider when determining whether content was communicated publicly or privately.\"\\nAutomated content moderation (User to User)\\tPublic Private Guidance - Encryption\\tThink our public private guidance should be amended to reflect that \"encryption\" is a factor in determining whether content is communicated privately. The Guidance be revised in Annex 9 to make it clear that encryption is a factor in determining whether content should be regarded as having been communicated privately\\nAutomated content moderation (Search)\\t\\tSEFF broadly agree, highlighting the need for smaller services to use automated tools to avoid bias. There needs to be more transparency around larger services that already use automated content moderation mechanisms. And it\\'s important for smaller services to start using automated methods as humans can be subject to bias or personal beliefs. User reporting and complaints (U2U and search)\\t\\tSEFF appreciate the requirement to provide timelines to deal with reports/complaints. SEFF appreciate the requirement for platforms to have a timeline in which to investiage and respond to complaints, and appreciate that this timeline should be realistic to avoid incorrectly dismissing reports in the need for speed or to keep stst numbers low. They give the example of X who provide notifications on the process.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934343099594116,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Though do note the clinical tone of these messages, which could be improved. Enhanced user control (U2U)\\t\\tLocal authorities will need to engage with US authorities which will likely be ineffective. SEFF make the point that big platforms will still be run from the US, and so Northern Irish law enforcement / civil groups will still need to apply to US authorities for information relating to offending account - this doesn't go far enough to tackle harm related to terrorism in Northern Ireland. Specifically, there needs to be more information about how civil society can participate.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922839999198914,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Enhanced user control (U2U)\\t\\tIn favour of voluntary verifcation with some caveats. Broadly in favour of voluntary verification, but with some caveats / concerns.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9821877479553223,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"For instance, although subscription type verification provides a traceable route to a bank account and therefore individuals' details, it doesn't create a safe space online. Also, paid verification can give a illusion of safety to users, but in reality it does not stop the verified account from posting harmful content. A voluntary verification should be available to all users on all platforms so that authorities can track individuals. User access to services (U2U)\\t\\tSEFF agree but think we should go further - so that users cannot easily create new accounts to return to the platform. SEFF agree that services should remove accounts if they infer it is operated by/for a terrorist group. But think it should go further so that it is not easy for users to set up new accounts. Email address / IP address / bank details should be flagged to negate the risk of offenders returning with a new account. SEFF believe preventing access to a service once illegal harm has taken place is the best way of eradicating online harm. Cumulative Assessment\\t\\tSEFF believe the burden on larger services to be proportionate as they should take a deeper accountability. SEFF believe there is a deeper accountability for large services to take safety and security of their users into account. They represent more people and therefore have a higher risk of harm arising. Recommender system testing (U2U)\\t\\tDisagrees changes to recsystem should be considered a significant change\\tOfcom has also proposed that a “significant change” to the service should trigger an obligation on service providers to perform a new risk assessment. We consider that, where a service provider has concluded that its recommender system does not materially affect the risks posed on its service, then that should be sufficient in ensuring that it has an up-to-date understanding of the risks it poses. In such circumstances, additional risk assessments would be disproportionate to the low risks involved\\nGovernance and accountability  \\t\\tChallenges segmentation of specific governance measures\\tThe proposed governance measures can be highly resource-intensive. To ensure they are proportionate and consistent with the purpose of the Online Safety Act - to make services safer and require their providers to identify, mitigate, and manage risks of harm to users (as set out in section 1) - the measures in the Codes of Practice should be logically connected to and targeted at the risks of harm posed by a service. As explained further in response to question 11.3 below, we recommend the following proposals for service providers should only apply if a service is either “multi-risk” or “large and multi-risk”, as relevant, not simply by virtue of a service being large: to (a) conduct an annual review of risk management activities; (b) have a written statement of responsibilities; (c) track evidence of new and increasing online illegal harms; (d) have a code of conduct regarding protection of users; and (e) have a system of staff compliance training. Governance and accountability  \\t\\tSupportive of not requiring external audit providers\\tAirbnb supports Ofcom’s provisional conclusion that it would not be proportionate to require service providers to engage external audit providers to review the measures they have in place. In Airbnb’s experience, internal audit functions frequently act independently from the remainder of the business and have a detailed understanding of the risks posed by the service. They are therefore able to provide effective auditing of measures taken to mitigate risks posed by the service, along with making proportionate proposals on additional measures that should be taken. If a service provider prefers to engage an external third-party to audit the measures they have in place, then our proposed approach would be to not preclude them from doing so. Approach to the Codes\\t\\tDisagrees that larger platforms should apply more measures\\tTo ensure the Act functions as effectively as possible, the obligations placed on service providers must be proportionate to the risks of illegal content on their service. While some larger platforms may have the resources to maintain more complex and costly compliance regimes, it would not be proportionate for the availability of resources alone to be used to determine whether obligations apply, as this has no direct bearing on the risk posed by the service. Approach to the Codes\\t\\tDisagree with definition of multi-risk\\tAs explained in response to question 11.4, we consider that Ofcom’s definition of a “multi-risk” service is too broad and disproportionately exposes lower risk service providers to onerous obligations. We consider it would be more proportionate to the aims and intentions of the Act that the most onerous measures should apply to services that identify high risks of priority harm. Approach to the Codes\\t\\tDisagree with definition of multi-risk\\tWe are concerned that Ofcom’s definition of a “multi-risk” service is too broad and exposes inherently low-risk service providers to disproportionately onerous obligations. At present, there is little to no differentiation between a service provider which assesses a particular risk as “medium” and a service provider which assesses the same risk as “high”. We therefore consider that in order to be “multi-risk”, a service should pose at least one high risk of residual harm and recommend that Ofcom provide clearer guidance on the distinction between a medium or high risk in the Risk Assessment Guidance. We note in response to question 9.2 our concerns with the approach the guidance currently takes (e.g. in the “illustrative risk matrix” provided at paragraph A5.7)\\nApproach to the Codes\\t\\tSuggesting four changes to the definition of large services + agreement of it not being linked to resources\\tFirst, a service provider should only be subject to the additional obligations proposed for “large” platforms where it both surpasses the quantitative measure (e.g. user number threshold), and satisfies a qualitative assessment based on the type and degree of risks its service poses. This would ensure that the recommendations are proportionate and target services that present a meaningful risk of priority harms. Second, Ofcom should confirm, in paragraphs A11.7 to A11.11 of the Code of Practice, that a “user” should only be counted for the purposes of calculating whether the “large service” threshold is met, where that user faces a realistic risk of coming into contact with illegal content on the service. This would ensure the recommendations in the Code of Practice are proportionate and would align with the purpose of the Act, as set out in section 1. [gives example].\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.7847344279289246,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Third, Ofcom should provide guidance on the calculation of user numbers. Any figures on how many people visit the site (without logging in or transacting) are unreliable because it is not technically possible, with complete accuracy, to de-duplicate site visits or identify bots. Fourth, Ofcom should re-evaluate the application of measures that bite primarily because of the size of a service (in particular 3A, 3C, 3E, 3F, 3G, 4B, 4C, 4D, 4E and 4F, which apply to all large services, regardless of whether any high, or even medium, risks have been identified). As described further in response to question 11.4, Airbnb considers that certain of these measures should only apply to multi-risk services which pose a high risk of at least one priority illegal harm. Finally, we agree with Ofcom that the definition of a “large service” should not be linked to a service provider’s resources and number of employees; for lower risk services, this could lead to disproportionately burdensome compliance obligations, considering the risks posed to users. Approach to the Codes\\t\\tDisagree with definition of multi-risk\\tWe are concerned that Ofcom’s definition of a “multi-risk” service is too broad and exposes inherently low-risk service providers to disproportionately onerous obligations. At present, there is little to no differentiation between a service provider which assesses a particular risk as “medium” and a service provider which assesses the same risk as “high”. We therefore consider that in order to be “multi-risk”, a service should pose at least one high risk of residual harm and recommend that Ofcom provide clearer guidance on the distinction between a medium or high risk in the Risk Assessment Guidance. We note in response to question 9.2 our concerns with the approach the guidance currently takes (e.g. in the “illustrative risk matrix” provided at paragraph A5.7)\\nContent moderation (User to User) \\t\\tSummary of their thoughts on conmod\\tAirbnb: (a) welcomes the inclusion of content moderation measures in the Code of Practice; (b) considers Ofcom should aim to strike an appropriate balance as to the specificity of its proposals for content moderation obligations; (c) recommends that Ofcom explicitly confirm that the content moderation measures do not require general monitoring, apart from where this is explicit; and (d) considers that keyword detection technology has limited effectiveness and can lead to a high number of false positives\\nContent moderation (User to User) \\t\\tAgrees with measures, but asks for more specificity\\tAirbnb welcomes the inclusion of content moderation measures in the draft user-to-user Code of Practice. It is critical that the recommendations in the Code strike the appropriate balance between ensuring services understand their compliance obligations under the Act, while not being overly prescriptive.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9913676381111145,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Service providers should be able to determine precisely how to operationalise the recommendations in the way that is most appropriate and proportionate for its service and the risks it may pose.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9959879517555237,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See also our response to question 14.1. Airbnb agrees that the meaning of “swift”, in relation to a service provider’s obligation to take down illegal content, is highly dependent on the context and the material that is to be removed. While we understand that it is not Ofcom’s intention to introduce general monitoring obligations, Airbnb strongly recommends that Ofcom explicitly confirm that the content moderation measures do not require general monitoring, apart from the automated content moderation obligations triggered in relation to CSAM and fraud. Content moderation (User to User) \\t\\tFeels ACM CSAM measures should be more targeted\\tAirbnb supports Ofcom’s overall approach of limiting automated content moderation requirements to the use of hash matching, URL detection, and keyword detection against CSAM and certain types of fraud. However, we consider that these measures should be targeted at a very limited range of services that are at genuinely high risk of carrying relevant illegal content\\nContent moderation (User to User) \\t\\tRecommending redrafting fraud measure\\tAirbnb supports Ofcom’s conclusions that keyword detection should not be required to identify a broader range of fraud offences (e.g. illegal financial promotions and investment scams). As noted by Ofcom, the terminology used for financial promotions is used in many other contexts and this can heavily impact the degree of accuracy and effectiveness of keyword detection technology. However, Airbnb considers that the recommendations at §§4.45 - 4.47 are too prescriptive.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922077059745789,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"As currently drafted, we are concerned that service providers may choose to meet the minimum requirements to enjoy the ‘safe harbour’ provided by the Code of Practice, rather than pursuing an alternative, more robust, and more effective approach. Airbnb proposes that the measures set out in the Code to tackle fraudulent behaviour are redrafted to enable service providers greater flexibility while retaining the benefit of the safe harbour. For example the Code could clarify that Ofcom regards the recommended measures as one appropriate means of compliance, but that service providers may adopt other equally or more effective methods to combat the relevant types of fraud. User reporting and complaints (U2U and search) \\t\\tAsking for more detail to be included in the trusted flaggers measures\\tWe support Ofcom’s proposal that service providers should have policies about the prioritisation of content for review, having regard to whether the content “has been flagged by a trusted flagger”, and that service providers which have identified a medium or high risk for fraud should “establish and maintain a dedicated reporting channel for fraud, for trusted flaggers”. We would consider it crucial to the proper functioning of the “trusted flaggers” system for Ofcom to explicitly state in the Code of Practice that, while services “should acknowledge receipt of each relevant complaint and provide the complainant with an indicative timeframe for deciding the complaint”, this does not create a legal obligation on them to follow-up with a trusted flagger to share the outcome of a report/complaint or to disclose further information about the user(s) or content involved. Recommender system testing (U2U) \\t\\tFeel the measure is disproportionate\\tAs noted in our response to question 9.3, Airbnb agrees that risks can arise with recommender systems on certain services. However, this is highly dependent on the content that is being recommended. It would be disproportionate to impose requirements on a service provider to collect safety metrics when implementing changes to its recommender system where that service has (a) evaluated the effects of its recommender system, and (b) concluded that it does not materially affect the likelihood or impact of the risks posed by illegal content\\nRecommender system testing (U2U) \\t\\tDisagrees changes to recsystem should be considered a significant change\\tOfcom has also proposed that a “significant change” to the service should trigger an obligation on service providers to perform a new risk assessment. We consider that, where a service provider has concluded that its recommender system does not materially affect the risks posed on its service, then that should be sufficient in ensuring that it has an up-to-date understanding of the risks it poses. In such circumstances, additional risk assessments would be disproportionate to the low risks involved\\nGovernance and accountability\\t\\tOrganisations should be trained on approach and understanding why they need to be compliant. We believe that organisations, in addition to being trained on approach must also receive training on understanding why they need to be compliant to counter bias. Approach to the Codes\\t\\tAgree that onerous measures applie to large and/or medium or high-risk services. Satisfied with the proposal for the most onerous measures to be applied to large and/or medium or high-risk services. Pleased to see recognition that small services will need to have more onerous measures applied where they are high risk for grooming/CSAM. Approach to the Codes\\t\\tAgree with the definition of large services. Agree with the definition of large services. Approach to the Codes\\t\\tAgree with the definition of multi-risk services. Agree with the definition of multi-risk services. Content moderation (User to User) \\t\\tRecommend a time target for taking down CSAM \\tRecommend there be a time target for the taking down of CSAM. The longer the content is accessible, the more harm it causes, and the child is revictimised each time an image is viewed. This has a significant impact on a victim's recovery journey. Suggest any CSAM reported should be removed within 24 hours at the latest. Content moderation (User to User) \\t\\tPlatforms should pre-screen at the point of upload. Platforms should pre-screen at the point of upload to ensure CSAM does not appear on the platform in the first place. Would like to see this reflected in future proposals. Automated content moderation (User to User)\\t\\tPlatforms should pre-screen at the point of upload. Platforms should pre-screen at the point of upload to ensure CSAM does not appear on the platform in the first place. Would like to see this reflected in future proposals. Content moderation (User to User) \\t\\tAll moderators (whether voluntary or non-voluntary) should receive training and materials. Current proposals are for volunteer moderators not to receive training and materials. For grooming/CSAM all moderators should have sound knowledge and understanding of CSA. This proposal increases the risk of unconscious bias and could lead to illegal content not being taken down. Content moderation (Search)\\t\\tAgree with the proposals. Agree with the proposals. Automated content moderation (User to User)\\t\\tUser number thresholds should be constantly reviewed as perpetrators seek out the most private places. Understand the reasons behind user number thresholds, but should be constantly reviewed as services that are high or medium risk for image-based CSAM regardless of size need to implement ACM, or we risk creating spaces for people to disseminate CSAM freely without detection. Perpetrators seek out the most private places where they are least likely to be detected, e.g. E2EE environments. Smaller services that do not use ACM wil become high risk, desirable places for perpetrators to operate. Automated content moderation (Search) \\t\\tUser number thresholds should be constantly reviewed as perpetrators seek out the most private places. Understand the reasons behind user number thresholds, but should be constantly reviewed as services that are high or medium risk for image-based CSAM regardless of size need to implement ACM, or we risk creating spaces for people to disseminate CSAM freely without detection. Perpetrators seek out the most private places where they are least likely to be detected, e.g. E2EE environments. Smaller services that do not use ACM will become high risk, desirable places for perpetrators to operate. User access to services (U2U)\\t\\tAnybody sharing known CSAM should be permanently blocked from the service. Anybody sharing known CSAM should be permanently blocked from the service. There should be a zero-tolerance approach for sharing this content due to the significant harm caused to victims. Governance and accountability\\t\\tDoesn’t appear to be any mechanism for external enforcement - leaves an open question as to how effective these Codes will be in practice\\tWhile the suggested Codes of Conduct appear appropriate, there doesn’t appear to be any mechanism for external enforcement e.g. from Companies House or a Regulator. This leaves an open question as to how effective these Codes will be in practice. We would be keen to understand how these will be monitored and reviewed, and the process for that.'\\nGovernance and accountability\\t\\tOfcom and the UK Government should ensure the organisation is sufficiently resourced to deliver these measures \\t[Codes measures] will only be as effective as the organisation overseeing them. Ofcom and the UK Government should ensure the organisation is sufficiently resourced to deliver these measures and provides regular transparent reporting on its regulatory activity and progress in relation to ensuring (as much as is feasibly possible) that providers are adhering to the Codes of Practice.'\\nGovernance and accountability\\t\\tScottish Gov broadly agreee with scope of governance and accountability measures but urge review of effectiveness in future\\t[Proposals feel like] a one size fits all proposal. Ofcom should review this approach once enacted to ensure it is effective and suitably bringing into scope all relevant services.'\\nGovernance and accountability\\t\\tProxy data on costs of implementation available from Data Protection Act 2018\\tProxy data on costs of implementation available from Data Protection Act 2018\\nApproach to the Codes\\t\\tScottish Gov agree with definition of large services if segmentation is regularly reviewed\\tYes, as long as Ofcom are regularly reviewing the size and risks associated with platforms, including small, new and emerging platforms regularly.'\\nApproach to the Codes\\t\\tScottish Gov agree with definition of multi-risk services if segmentation is regularly reviewed\\tYes, as long as Ofcom are regularly reviewing the size and risks associated with platforms, including small, new and emerging platforms regularly.'\\nContent moderation (User to User) \\t\\tOfcom should consider further guidance to encourage transparency re standards/performance of measures\\tThese proposals are acceptable in principle, but Ofcom should also consider additional guidance to encourage publication of standards and performance to the public so users can have confidence and choice as to whether they wish to engage with a platform.'\\nContent moderation (User to User) \\t\\tOfcom should consider signposting support when suicide/self-harm content is removed\\tHave Ofcom considered what, if any, support or signposting tech firms should provide to people who have a post removed when it is related to self-harm or suicide content? Although, we recognise that this content can be harmful and is removed for good reasons, this kind of content is often shared by people who are vulnerable and having their posts removed can be upsetting. '\\nContent moderation (User to User) \\t\\tMore info needed on how Ofcom will ensure services seek further advice/support to share with users when reporting content\\tHow will Ofcom make sure that companies reading this Guidance are aware of appropriate signposts to seek further advice/support to share with individuals who raise concerns about content? For example, references to support, such as through the Revenge Porn Helpline.'\\nAutomated content moderation (User to User)\\t\\tScottish Gov agree with measure but want more consideration of implementation barriers for non-profits\\tWe agree with the proposals. Ofcom should consider potential challenges around the affordability and programming of these tools for small community/voluntary organisations. .'\\nUser reporting and complaints (U2U and search) \\t\\tOfcom should consider guidance on ensuring complaint process is accessible to all\\tOfcom should consider the development of child friendly complaints processes to empower users under 18. Complaints procedures need to be easily accessed by all users regardless of age, disability etc. and providers should be open about what action may be taken in response to complaints.'\\nUser reporting and complaints (U2U and search) \\t\\tTrusted flaggers should be considered for areas other than fraud, including CSAM\\tWe note the provision for ‘trusted flaggers’ to use a dedicated reporting channel for fraud due to the currently available evidence. Ofcom should regularly review the success of this approach, and consider its potential utility to report CSAM, with ‘trusted flaggers’ comprising Police forces, NCA, NCSC etc.'\\nTerms of service and Publicly Available Statements\\t\\tAssumption that language of policies should be suited to legal age of 14 is flawed: language should be drafted to be accessible to a much younger age group\\tWe note that there has been some attempt to consider if policies are readable by children. However the assumption of language/style fit for legal age for use (often 14 years) is flawed. The industry is well aware that many of the most vulnerable users are under 14. Policies should be drafted for a much younger age group to protect those most at risk (accepting that users may technically be underage'\\nDefault settings and user support (U2U)\\t\\tOnus is too much on children to take action; measures should be more proactive in targeting bad actors\\tThe proposals are helpful. However the onus remains on the child to block/mute unwanted contact from an adult. The platform technology is sufficiently sophisticated to run searches to identify adult users who persistently make unsolicited contact with children and block them. This control needs to be strengthened and more responsibility put on operators.'\\nUser access to services (U2U)\\t\\tFurther consideration to measure requiring accounts sharing CSAM to be blocked\\tWe are supportive of Ofcom giving further consideration to a measure recommending that users that share CSAM have their accounts blocked and consider this to be a proportionate move, given the severity of the level of harm caused by CSAM.'\\nUser access to services (U2U)\\t\\tOfcom should take a risk-based approach to CSAM account blocking measure\\tIt is difficult to be specific  [re evidence to support measure] but it would be helpful for Ofcom to consider a risk-based decision making framework to encourage consistency across all platforms. This would need to be considered on a case by case basis, dependent on the facts and circumstances of each case. '\\nService design and user support (Search)\\t\\tScottish Gov agree with measure and recommend evidence from IWF/Lucy Faithfull foundation re warnings on U2U services\\tWe agree with the proposals for warnings for those seeking out CSAM online on search services. It would be interesting to consider the findings from automated tools developed by the IWF and Lucy Faithfull Foundation that pushes CSAM searchers on Pornhub to seek help for their online behaviour when these are available. In March 2022 alone, their chatbot appeared more than 170k times for those seeking CSAM on Pornhub.'\\nGovernance and accountability \\t\\tAgree with proposals\\tIt is a reasonable ask that organisations carry out risk  assessments of products and/or services which have the  potential to cause harm, or be used to cause harm, even if  this is not the aim of the product or service. Governance and accountability \\t\\tWould like Ofcom to consider the impact  of regulation on low-risk services such as ours. Evri currently offers a service to consumers and retailers  which allows them to upload a video message to be sent on a parcel code to be viewed by parcel recipient. This can only be accessed by the intended recipient and has clear traceability to both the video and parcel sender and the recipient (no anonymity). But as a user-to-user service, would be captured.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.992377758026123,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Governance and accountability measures would introduce cost and resouerce - feels disproportionate given nature of the service. Ofcom should consider impact on low risk services\\nGovernance and accountability \\t\\tWould need to cost any measures to moderate con�tent or provide a takedown service if we were required to  do this. Would need to cost any measures to moderate con�tent or provide a takedown service if we were required to  do this. Governance and accountability \\t\\tDo not think this measure would apply to our service as we consider it a small and low risk service. Do not think this measure would apply to our service as we consider it a small and low risk service. We would need to cost the additional processes and responsibilities to roles within our organisation. Governance and accountability \\t\\tProposals do not feel proportionate for such a low risk, low reach, and highly traceable service as Evri video\\tDo not think these proposals should apply to Evri video. It does not feel proportionate or reasonable for such a low risk, low reach, and highly traceable service. Governance and accountability \\t\\tAgree as long as the guidance of what companies/organisations need to consider is clear and reasonable. Agree as long as the guidance of what companies/organisations need to consider is clear and reasonable. Governance and accountability \\t\\tAgree\\tAgree risk profiles are clear\\nGovernance and accountability \\t\\t\"Understand that retention of records relates to risk assessment but this is not made clear in the consultation and we had to seek further clarity from \\nOfcom. \"\\tUnderstand that retention of records relates to risk assessment but this is not made clear in the consultation and we had to seek further clarity from Ofcom. Would be helpful for any statement of regulations to be clear which records need to be retained and in what format. Governance and accountability \\t\\tDo not agree -  for low usage and low risk services we think the record keeping time period could be reduced. Do not agree -  for low usage and low risk services we think the record keeping time period could be reduced. Content moderation (User to User) \\t \\tAgree but the requirements should be spelt out more clearly in future statements or final regulations. The wording in the consultation is not clear and we initially thought moderation involved checking content prior to it being made available, which would be a significant burden. In light of this clarification,  do agree with the proposals, but the requirements should be spelt out more clearly in future statements or final regulations. User reporting and complaints (U2U and search) \\tReporting and complaints\\tAgree that consumers should have access to an easy to use, free complaints service\\tAgree that consumers should have access to an easy to use, free complaints service. Outlined Evri video approach to complaints. Terms of service and Publicly Available Statements\\t\\tYes, the terms of service should be easily accessible and include provisions for user to user services\\tYes, the terms of service should be easily accessible and include provisions for user to user services\\nRecommender system testing (U2U)\\t\\tSeems sensible illegal content dissemination should be included within recommender system testing, ensuring any changes to the system does not increase illegal content outreach. Seems sensible illegal content dissemination should be included within recommender system testing, ensuring any changes to the system does not increase illegal content outreach. Not applicable to Evri video\\nUser access to services (U2U)\\t\\t\" If we believe our services \\nhave been abused for any reason (most likely fraud) we can block names and email addresses, but this \\nwould not prevent someone from setting up another account \"\\t If we believe our services have been abused for any reason (most likely fraud) we can block names and email addresses, but this would not prevent someone from setting up another account. To date have not had any complaints about Evri video. Cumulative Assessment\\t\\tOfcom should be cautious about extending regulation to low usage, low risk services which have a lower probability of causing harm\\t\"Ofcom should be cautious about extending regulation to low usage, low risk services which have a lower probability of causing harm. It is right that the highest burden of regulation should be \\nplaced on those services which have the widest reach and pose the highest risk of causing harm. \"\\nAutomated content moderation (User to User) \\t\\tApply these measures to services with over 700k monthly users\\t\"We note that Ofcom have recommended the most impactful measures in protecting UK\\nconsumers from illegal harms for large and multi-risk services only. In the draft illegal\\ncontent Codes of Practice, Ofcom has proposed to define a large online service as one with\\nover 7 million UK users per month (11.51). While Which? recognises the need to target\\nservices with the widest reach, our evidence suggests that online services with high risk of\\nfraud such as online dating apps will be excluded from these obligations. Which? believes that Ofcom should consider alternative criteria or thresholds for services\\nthat have a high risk of fraud but have less than 7 million UK users per month. Which? notes that Ofcom has included recommendations for smaller online services that\\nhas been proposed for large online services in relation to automated tools for the\\ndetection of Child Sexual Abuse Material (CSAM) (4G, 4H). This extends the obligations to\\nservices with over 700,000 UK users per month.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9967145919799805,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Building on the CSAM proposals, Which? recommends that a similar distinction is made\\nfor fraud detection. Ofcom already recognises that online services with 700,000 to 7\\nmillion UK users per month have the potential to impact a significant number of users if\\nharm is present on their service. In its risk assessment guidance (table 6), Ofcom states\\nthat these services should be considered medium risk. This ensures that these services\\nwith a high risk of fraud are not overlooked in the regulatory framework. By extending certain fraud obligations (such as direct reporting channels to trusted\\nflaggers) to services with over 700,000 monthly UK users, Ofcom can ensure that these\\nservices are subject to obligations similar to those placed on larger services, without\\nimposing an undue burden on the smallest services.\"\\nUser reporting and complaints (U2U and search) \\t\\tApply these measures to services with over 700k monthly users\\t\"We note that Ofcom have recommended the most impactful measures in protecting UK\\nconsumers from illegal harms for large and multi-risk services only. In the draft illegal\\ncontent Codes of Practice, Ofcom has proposed to define a large online service as one with\\nover 7 million UK users per month (11.51). While Which? recognises the need to target\\nservices with the widest reach, our evidence suggests that online services with high risk of\\nfraud such as online dating apps will be excluded from these obligations. Which? believes that Ofcom should consider alternative criteria or thresholds for services\\nthat have a high risk of fraud but have less than 7 million UK users per month. Which? notes that Ofcom has included recommendations for smaller online services that\\nhas been proposed for large online services in relation to automated tools for the\\ndetection of Child Sexual Abuse Material (CSAM) (4G, 4H). This extends the obligations to\\nservices with over 700,000 UK users per month.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928016662597656,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Building on the CSAM proposals, Which? recommends that a similar distinction is made\\nfor fraud detection. Ofcom already recognises that online services with 700,000 to 7\\nmillion UK users per month have the potential to impact a significant number of users if\\nharm is present on their service. In its risk assessment guidance (table 6), Ofcom states\\nthat these services should be considered medium risk. This ensures that these services\\nwith a high risk of fraud are not overlooked in the regulatory framework. By extending certain fraud obligations (such as direct reporting channels to trusted\\nflaggers) to services with over 700,000 monthly UK users, Ofcom can ensure that these\\nservices are subject to obligations similar to those placed on larger services, without\\nimposing an undue burden on the smallest services.\"\\nApproach to the Codes\\t\\t\"Calculate large online services through its group capacity, revenue\\nand reach\"\\t\"Ofcom has assigned the most onerous measures to online services that they have\\ndetermined to be large according to their reach. It has justified this on the basis that they\\nhave the financial capacity to absorb costs (11.53-60). The proposed approach does not\\nconsider the capability of online services that fall below the current 7 million users per\\nmonth that have the financial means by which to undertake these measures. Ofcom’s judgements on size are based on a single service rather than the capacity of the\\norganisation. Ofcom’s proposed approach could incentivise organisations to have multiple\\nsmaller services rather than a single larger service. For example, Match Group operates\\n12 different online dating services with an annual revenue of $3.4bn, none of the services\\nindividually appear to have sufficient user numbers to qualify as a large service. Ofcom considered defining large services based on annual revenue. It suggested that this\\nmay not always accurately capture access to resources.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9906834363937378,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Which?': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7859123349189758,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'does not believe that the\\ncalculation of size based on user numbers accurately captures access to resources either. We recommend that Ofcom apply its considered revenue threshold £10 - £50 million at a\\ngroup level in addition to individual online services that are considered high risk of fraud. In the event that a single service does not meet this threshold, their group revenue should\\nbe considered and this will ensure that sectors such as the online dating community\\nwhere fraud is prevalent are brought into scope\"\\nAutomated content moderation (User to User)\\t\\t\" Services should be required to use URL detection to detect possible\\nfraud\"\\t\"Suspicious URLs are a useful piece of data which can indicate potentially fraudulent\\ncontent. However, the presence alone of a suspicious URL is not enough to provide\\nreasonable grounds to infer that the piece of content is itself fraudulent. Which? believes that services should be required to use relevant data such as URLs to\\nproactively detect possible fraud on their services. Combining URL with other sources of\\ndata presents a picture of whether content is fraudulent. As a result we do not believe that\\nURL matches should lead to automatic takedown but instead should require that the\\ncontent is processed through the organisation’s content moderation system (as in A4.46). We recommend that services should be responsible for procuring an accurate feed of\\nlikely fraudulent URLs and other internet locator indicators. This should include having\\nestablished mechanisms for reviewing the accuracy of the locator information provided to\\nensure low levels of false positives (as in A4.53-A4.57). Given the low cost of the cheaper options available, fraudulent URL detection should not\\njust be a requirement for the largest services. Ofcom should consider recommending\\nFraudulent URL detection for services with over 700,000 users as with CSAM URL\\ndetection (A4.35).\"\\nAutomated content moderation (User to User)\\t\\t\"Ofcom should use its information gathering powers to generate an\\nevidence base on the use of new technologies (including artificial intelligence) to tackle\\nfraud.\"\\t\"Ofcom should, as an immediate priority, use its newly granted information gathering\\npowers to rapidly generate an evidence base on the use of machine learning to detect\\nillegal content including fraud. Which?': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9911544322967529,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'recognises the lack of publicly available data on the\\naccuracy of service’s ML systems. The absence of clear recommendations encouraging\\nthe responsible use of this type of technology, risks allowing sophisticated organised\\ncrime to continue to be one step ahead. It also risks that services are not being\\nincentivised to invest in this technology to continuously improve their detection and take\\ndown.\"\\nAutomated content moderation (User to User)\\t\\t\"Ofcom should create a standard in their codes of practice that online\\nservices use specific types of data to train their systems to prevent fraud\"\\t\"Ofcom can provide a technology neutral approach to improve content moderation systems\\nto prevent fraud by focusing on data. Ofcom has created a framework of indicators in the\\nICJG, and should use this to create a standard in the codes of practice for specific data\\nuse in the prevention of fraud.\"\\nAutomated content moderation (User to User)\\t\\t\"Add Specified Anti Fraud Organisations (SAFOs) to the list of trusted\\nflaggers\"\\t\"The current list of trusted flaggers misses organisations that have important data which\\ncould be useful for identifying fraud on services. The Serious Crime Act 2007 (order\\n2008) lists a number of Specified Anti-Fraud Organisations (SAFOs). SAFOs are trusted\\norganisations, recognised by the UK government, to enable and facilitate the sharing of\\ninformation across both the public and private sectors. Which? is aware that multiple SAFOs are keen to work with service providers to use their\\ndata to better detect fraud. Their inclusion ensures that the regulatory framework\\nleverages the knowledge and capabilities of organisations with a proven track record in\\nthis domain.\"\\nEnhanced user control (U2U) \\tFuture Codes\\tApply safety by design principles to all verification schemes\\t\"Ofcom proposes limiting these obligations to only large services that label user profiles\\nas notable or offer a monetised scheme. As Ofcom’s evidence (20.83-95) suggests that\\nthe harm comes from the poor design of these schemes it is unclear why this measure\\nshould apply only to schemes on large services. There is no evidence to suggest that a\\nverification scheme that failed to comply with Ofcom’s proposals would be beneficial to\\nusers at preventing rather than facilitating impersonation. Given this risk to consumers\\nand the lack of benefit Ofcom should apply these measures to all services that label user\\nprofiles as notable or offer a monetised verification scheme.\"\\nn/a\\tFuture Codes\\tLook into boosted content in the future\\t\"Ofcom\\'s proposals have not included any provisions to tackle the increased risk posed by\\nuser generated content that is promoted after receiving payment. The Online Safety Act\\n2023 definition of fraudulent advertising excludes regulated user-generated content. This\\nmeans that user generated content which acts like advertising (in that it is promoted to\\nusers on the basis of payment), also known as boosted content, is covered by the Illegal\\nContent Codes of Practice rather than the Fraudulent Advertising Codes of Practice. As a\\nresult any additional requirements placed on fraudulent advertising do not apply to boosted\\ncontent. Our primary concern is that bad actors could avoid due diligence checks and still\\nreach consumers at the same scale as paid for advertising.We believe it is essential to\\naddress this gap. Which? has previously raised concerns about how paid for boosted content could be used\\nto effectively promote fraudulent content to consumers. We urge Ofcom to revisit boosted\\ncontent within these Codes of Practice after publishing the Code of Practice for Fraudulent\\nAdvertising to ensure that the same protections that apply to prevent fraudulent advertising\\napply to boosted content.\"\\nApproach to the Codes\\t\\tThe Codes are inflexible, will quickly become outdated and risk inappropriately homogenising platforms\\' approach to compliance in a way that makes their T&S functions not fit for purpose. The Codes may lock in practices which quickly become outdated or fail to keep pace with evolving harms, because they incentivise compliance over innovation. The Codes may have the effect of homogenising platforms\\' approach to compliance in a way which does not reflect their varying designs, practices and business models, forcing platforms to adopt methods which are not relevant or effective. The requirement to and the process for justifying alternative approaches / deviation from the Codes measures is unclear and may disproportionately impact smaller, alternatively structured platforms. Approach to the Codes\\t\\tThe thresholds are too low / i.e. they disproportionately impact a large number of services which should not be categorised as multi-risk. The threshold for designation as a \"multi-risk\" service as only requiring presence of 2 of the fifteen illegal harms is too high and will disproportionately affect a large number of services. Approach to the Codes\\t\\t7 million monthly UK user threshold is arbitrary, too low, and does accurately reflect penetration of the UK market relative to some of the larger services. Rather than using an arbitrary figure derived from the EU DSA framework, UK should seek a definition which looks at the UK market\\'s unique characteristics (does not expand on this point or provide examples). The definition of large service should take account of different types of users. Distinquishes between logged in and logged out userrs who have very different user experiences on Reddit (logged in users have access to user empowerment and content creation tools that logged out users do not).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9912591576576233,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Definition should also take into account revenue and employee numbers.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9870175719261169,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Cites the Digital Trust & Safety Partnership\\'s SAFE Framework https://dtspartnership.org/wp-content/uploads/2021/12/DTSP_Safe_Framework.pdf. Approach to the Codes\\t\\tThe threshold for multi-risk services is disproportionately low. The threshold requirement that a service scores as medium risk for at least two illegal harms is too low and disproportionately wide I.e. it will capture most platforms. Suggests a higher threshold of medium risk for 5 or more harms. Approach to the Codes\\t\\tThe Codes are overly broad and risk homogenising services\\' approach to compliance and failing to keep up to date with the evolution of how illegal harms manifest online. \"Reddit\\'s community governance structure and trust and safety processes focus on data and behavioural patters and less on content and have proven to be effective. Cites Weisenthal Center\\'s 2023 Digital Hate and Terrorism Report Card which ranked Reddit as a\"\"top industry leader\"\" in combatting those harms https://www.digitalhate.net/inicio.php?year=_2023 \\nStates Reddit\\'s trust and safety features incorporate safety by design, and that expecting Reddit to adopt codes of practice which do not take account of that design would be disproportionate and reduce the efficacy of its procedures. States the codes disincentivise innovation which creates the risk that services will lose pace with the evolution of illegal harms online.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8311643600463867,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nApproach to the Codes\\t\\tthe cost ranges cary significantly and do not take account of companies who do not have resources to scale up a dedicated compliance function. States that the cost ranges of the measures vary significantly. States the cost estimate do not take into account \"opportunity cost\". States that smaller companies do not have large compliance teams on call and have to reallocate engineers, designers and other critical staff, taking them off projects which may be critical to business operations and result in reduced or deferred revenue. Approach to the Codes\\t\\tReddit has concerns that many of the codes measures are overly prescriptive, do not account for diversity of service types and are inflexible. Reddit has concerns that many of the codes measures are overly prescriptive, do not account for diversity of service types and are inflexible. Governance and accountability\\t\\tOF agrees with our governance measures and who we propose to apply them to\\tOF agrees with our governance measures and who we propose to apply them to\\nGovernance and accountability\\t\\tOF agrees with our governance measures and who we propose to apply them to\\tOF agrees with our governance measures and who we propose to apply them to\\nGovernance and accountability\\t\\tOF does have some evidence of the costs involved in 3rd party audit (both time and currency). Yes, although an audit must be independent and tailored to the specific risks associated with each online service. OnlyFans already instructs an independent third party, which we call a \"Monitor\", to assess and validate the design, implementation, and effectiveness of our compliance program. The appointment of the Monitor comes at a substantial financial cost and requires significant time from senior leaders at OnlyFans. Governance and accountability\\t\\tWe began the Monitorship process in August 2021 and, to date, costs are in excess of $3 million USD\\tWe began the Monitorship process in August 2021 and, to date, costs are in excess of $3 million USD\\nApproach to the Codes\\t\\tWe should consider definitions used in other regimes and aim to align. We ask that Ofcom consider the definitions already adopted by other legal and regulatory regimes since most online services operate internationally\\nApproach to the Codes\\tSegmentation \\tOF agrees that most onerous measures should apply to large / risky services. We should consider age of users / target users as a material factor in RPs. We agree and note that the age of users (and target users) should be a material factor when considering risk profiles\\nApproach to the Codes\\tLarge services defintion\\tOF thinks we should align the definition with DSA. As above, we suggest that the user base calculation is made consistent with the EU Digital Services Act and is based on active users. Approach to the Codes\\tMulti risk defintion\\tOF agrees with our definition of multi-risk\\tOF agrees with our definition of multi-risk\\nContent moderation (User to User) \\t\\tOF would welcome inclusion of a specific section dedicated to \"safety by design\". We would welcome the inclusion of a specific section dedicated to ‘Safety by Design’. Automated content moderation (User to User)\\t\\tOF is supportive of our ACM measures. It would also support a keyword detection measure for CSAM/CSEA risks. OF is supportive of our ACM measures. It would also support a keyword detection measure for CSAM/CSEA risks. Automated content moderation (User to User)\\tPublic Private Guidance\\tWant further guidance from Ofcom on public/private distinction\\tWe would welcome further clarification from Ofcom on when content would be considered to be communicated privately vs publicly. Automated content moderation (User to User)\\tPublic Private Guidance\\tWant further guidance from Ofcom as to whether its service could be considered public/private. We consider content on OnlyFans to be privately disseminated because we use a subscription-based model, our content is behind a paywall, and our users must open an account. However we note Ofcom’s comments in Sections A9.29-A9.31, which appears to challenge our view without providing definitive clarification. If Ofcom applies this definition it should also ensure that it captures private/direct messages sent through other social media platforms (including where end-to-end encryption applies). User reporting and complaints (U2U and search)\\t\\tYes OF supports these measures. Wants to know if Ofcom will provide a list of trusted flaggers for just fraud or also other harms. Yes. However we request clarification on the question of whether Ofcom will provide a list of trusted flaggers only with respect to fraud or whether Ofcom will provide a list of trusted flaggers for more (or all) harms. Terms of service and Publicly Available Statements\\t\\tYes OF supports these measures and would welcome the clarification that the age of the users / target users should determine what language is appropriate for a service\\'s ToS.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9944690465927124,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Yes and we would welcome clarification that the age of the users (or target group of users) will determine what language and style is appropriate for a service’s Terms of Service. Recommender system testing (U2U) \\t\\tIt is possible to use certain \"stop words\" to prevent illegal / harmful content appearing on the platform, and pop-ups to direct users to organisations that could help. \"Certain words, known as “stop words”, that could relate to illegal or harmful content are blocked from appearing on the platform. OnlyFans also generates a pop-up message in\\nresponse to defined terms to direct individuals towards resources such as StopItNow (Lucy Faithfull Foundation) and Redirection (Suojellaan Lapsia).\"\\nEnhanced user control (U2U) \\t\\tYes OF supports these measures. It agrees that the first two proposed measures should include requirements for how these controls are made known to users.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9932413101196289,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Yes OF supports these measures. It agrees that the first two proposed measures should include requirements for how these controls are made known to users. Enhanced user control (U2U) \\t\\tYes OF supports these measures. It agrees that the first two proposed measures should include requirements for how these controls are made known to users. Yes OF supports these measures. It agrees that the first two proposed measures should include requirements for how these controls are made known to users. Enhanced user control (U2U) \\t\\tOF noted that it does not categorise creator accounts so all creator account holders undergo identity verification\\tOF noted that it does not categorise creator accounts so all creator account holders undergo identity verification\\nUser access to services (U2U)\\t\\tOF agrees with our UA proposals\\tOF agrees with our UA proposals\\nUser access to services (U2U)\\t\\tOF permanently and irrovecobly bans users that share CSAM. Also report CSAM to NCMEC.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9907987117767334,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They take SG CSAM on a case by case basis. OnlyFans collects relevant user data for safety purposes. If a user attempts to post illegal or harmful content, we place them on a banned user list which logs details like email address, IP address, device ID and (for creators) government-issued ID. It is OnlyFans’ policy to permanently and irrevocably ban a user who shares CSAM. We also report all suspected CSAM to NCMEC. It is OnlyFans’ policy to consider self-generated CSAM (SG CSAM) on a case by case basis, following escalation to the Safety Advocacy Team. We believe that SG CSAM should be considered on a case by case basis when it comes to law enforcement reporting and user blocking. OnlyFans welcomes the inclusion of human review of all hash-matched CSAM and the provision of feedback to organisations which maintain hash lists to improve list quality and reduce false positives. Statutory Tests\\t\\tOF agrees that our codes are appropriate in light of matters we need to have regard to.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922245740890503,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"OF agrees that our codes are appropriate in light of matters we need to have regard to. Approach to the Codes\\t\\tImportant to consider target audience when deciding which measures should apply \\tReach / target audience is an important consideration. We would like to see any service which is considered popular amongst children and young people be considered higher risk, regardless of the size of the service, and therefore the most onerous measures would apply. Approach to the Codes\\t\\tImportant to ensure minority groups (e..g welsh speakers and children) are still protected online regardless of size of service. In relation to 11.52 “Part of the reason for recommending some measures only for large services is that the benefits of measures are likely to be greater for such services, because more users will be protected by the measure”. We need to ensure that minority groups are not less protected because of this – e.g. Welsh speakers should be afforded the same protections even if the service is by definition smaller. Similarly, whilst cost implications are important, this should not be used to justify not protecting children and young people online. Content moderation (User to User) \\t\\tSupport improved content moderation by services \\tWe fully support improved content moderation on services and increasing the level of accountability expected of providers. Content moderation (Search) \\t\\tSupport improved content moderation to protect children  \\tWe support placing greater expectations on providers to improve search moderation so that it will make it much harder for children and young people to access harmful material. Automated content moderation (User to User)\\t\\tSupport any measures to protect children from CSAM and agree that hash matching and URL detection are useful tools \\tWe would strongly support measures being put in place to protect children and young people from CSAM, both as victims and from viewing such content and agree that smaller providers deemed high risk should also deploy these technologies. We also agree that hash matching and URL detection are useful tools to help combat CSAM and support wider use of these technologies. Automated content moderation (Search) \\t\\tSupport any measures to protect children from searching and accessing CSAM\\tWe would strongly support any measures that can be put in place to protect children and young people from searching for and accessing CSAM. We encourage services to take effective action and regularly monitor and update the URL list to ensure its relevance. User reporting and complaints (U2U and search) \\t\\tEncourage services to have easily accessible, transparent and simple complaints processes\\tWe would encourage services to have easily accessible, transparent and simple complaints processes. Where complaints are upheld, services should ensure appropriate action is taken in a timely manner. Terms of service and Publicly Available Statements\\t\\tSupportive of Ofcom's proposals \\tWe would support all services ensuring that their Terms of Service and publicly available statements are easy to find and understand for all services users, including the youngest person permitted to agree with them. Default settings and user support (U2U)\\t\\tSupportive of Ofcom's proposals \\tWe fully support making changes to default settings for child users to reduce their exposure to online harms. We also encourage services proactively providing support to help young people make more informed choices about risk (whilst acknowledging that children and young people are not solely responsible for minimising risk they are exposed to). Default settings and user support (U2U)\\t\\tEncourage use of prompts\\tWe would encourage prompts at any relevant points in the user journey. Recommender system testing (U2U)\\t\\tEncourage servcies to collect safety metrics to evaluate effectiveness of measures to protect children \\tWe would encourage the collection of additional safety metrics so that services can assess whether proposed changes are likely to increase children and young people’s exposure to illegal content. Enhanced user control (U2U)\\t\\tSupportive of Ofcom's proposals \\tWe fully support efforts to provide children and young people service users with more control and understanding of the content they encounter, to enable them to make informed choices. User access to services (U2U)\\t\\tSupportive of services reducing CSAM by blocking accounts\\tWe would support services taking stronger action to remove and reduce CSAM online by blocking user accounts of those who are known to share CSAM. Service design and user support (Search)\\t\\tSupportive of services taking measures to protect children and young peopl \\tWe support search services taking stronger measures to prevent children and young people from being exposed to harm. Governance and accountability\\t\\tGovernance requirements must be flexible\\tWe believe that governance is key to platform safety.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.990047812461853,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'However, governance must be sufficiently flexible to accommodate different business types and recognise the diverse size and nature of technology companies. Governance and accountability\\t\\tOfcom should collaborate with global regulators on governance standards\\tMany platforms that Ofcom will be regulating, including X, operate cross-border and therefore, we encourage Ofcom to collaborate with global regulators to encourage global governance standards that combat user harm effectively and consistently. Content moderation (User to User) \\t\\tUpdate guidance to include objective criteria to combat unconscious bias\\tX strives to implement content moderation policies that actively prevent unconscious bias, fostering fairness and respect in the online community. To achieve this, X’s internal content review policies set objective criteria against which agents can judge potentially illegal content. X welcomes the detailed nature of Volume 5 Illegal Content Judgements Guidance (“Volume 5”) and Annex 10 Guidance on Judgement for Illegal Content (“Annex 10”), but due to the broad interpretation of the ‘state of mind’ or ‘mental element’, may lead to subjective decisions. Automated content moderation (User to User)\\t\\tOfcom should collaborate with stakeholders to create common automated moderation standards \\tGlobal multi-stakeholder programs and initiatives have successfully supplemented X\\'s automated systems combatting CSE and terrorist content; we are hopeful that similar, multilateral collaboration can be deployed to combat other types of harmful content. X urges Ofcom to collaborate with industry stakeholders and regulators around the world to create common automated moderation standards that are globally scalable to protect global users consistently. Governance and accountability\\t\\tThe proposals are inappropriate for the large number of personal, non-commercial websites which are currently in scope of the codes. The codes continual references to \"industry\" and \"businesses\" fail to recognise that a vast number of in scope services are personal, non-commercial websites, run by individuals. The language and framing of the codes in regard to \"industry\" needs to either a) be broadened to acknowledge the wider range of in scope services, or else b) explicitly exclude non-commercial operators from the codes. For example, the requirement to \"name a person accountable to the most senior givernance body for compliance\" cannot be applied sensibly to personal, non-commercial websites. Governance and accountability\\t\\tThe requirement to have risk management audited by a third-party wouly be disporoportionately costly for smaller services. Non-commercial webistes and services, run by individuals, would findi it expensive to have risk management auditing completed. Further, noting the estimated number of in scope services (100,000), is there the necessary capacity and capability to perform these risk management audits? Approach to the Codes\\t\\tThere is concern that the definition of medium risk services is too broad. The broad defintions of medium risk services would result in too many very small and lower risk services being burdended with the most onerous measures. The suggestion here is to focus only on the large and high risk services. Approach to the Codes\\t\\tSmaller services cannot easily track and count the number of its users. Defining the size of service according to the number of users requires the service to have the capability and capacity to count the number of its uerse - which may not be the case for smaller services. A suggestions here to measure the size of the service by the number of employees or the size of its revenue. Automated content moderation (User to User)\\t\\tWhy is the hash database simply not available to all - as simply and as cheaply as possible? There is a requirement within the appendix to restrict access to the hash database, but if all services in scope should access it, why not make it as simple and as easy, and therefore as cheap, to access as possible - for the sake of smaller services, operated by individuals. Automated content moderation (User to User)\\xa0\\t\\tWhy is the URL database simply not available to all - as simply and as cheaply as possible? There is a requirement within the appendix to restrict access to the url database, but if all services in scope should access it, why not make it as simple and as easy, and therefore as cheap, to access as possible - for the sake of smaller services, operated by individuals. User access to services (U2U)\\xa0\\t\\tThe suggested measures to block a user are not likely to be effective. Blocking by IP would be unfair because IP addresss do not identify individuals but a household, so people other than the targeted individual wouild be blocked. Also, IP addresses are frequently re-allocated. Cumulative Assessment\\t\\tThe consultation and the codes are framed in ways that relate to \"businesses\" and largely exclude non-commercial websites run by individuals. Are the codes intended to relate to non-commercial websites run by individuals? If so, it appears that the wording and framing of the questions need to be broadened to make them relevant and accessible for small, non-commercial websoites (question 45 wording itself, for example). Granted the documentation does refer to rules that \"apply to individuals who run an online service\", but this is not consistent throughout. Alternatively, if the codes are intended to regulate an industry, rather than an activity, could smaller, non-commercial websites be ruled out? Cumulative Assessment\\t\\tThe consultation and the codes are framed in ways that relate to \"businesses\" and largely exclude non-commercial websites run by individuals. Are the codes intended to relate to non-commercial websites run by individuals? If so, it appears that the wording and framing of the questions need to be broadened to make them relevant and accessible for small, non-commercial websoites (question 45 wording itself, for example). Granted the documentation does refer to rules that \"apply to individuals who run an online service\", but this is not consistent throughout. Alternatively, if the codes are intended to regulate an industry, rather than an activity, could smaller, non-commercial websites be ruled out? Governance and accountability\\t\\t\"Yes\"\\t\"Yes\"\\nGovernance and accountability\\t\\t\"Yes\"\\t\"Yes\"\\nGovernance and accountability\\tGovernance and Accountability\\tLivestreaming of animal cruelty. Live streaming of animal cruelty is a risk factor and the Scottish SPCA have been made aware of websites that offer animal abuse content for a fee available in the UK. Approach to the Codes\\t\\t\"Yes\"\\t\"Yes\"\\nApproach to the Codes\\t\\t\"Yes\"\\t\"Yes\"\\nApproach to the Codes\\t\\tBar mustn\\'t be set too low for services\\tIf bar is set too low there won\\'t be any meaningful change. Particular concern relating to smaller services, who can host some of the most harmful suicide content. Approach to the Codes\\t\\tMust embed safety by design\\tEmbed and incentivise a ‘safety by design’ approach to root out harmful content from the outset\\nApproach to the Codes\\t\\tToo much emphasis on the economic burden on companies\\tToo much emphasis on the economic burden on services, with the purpose of the OSA (making the UK the safest place to be online) not sufficiently reflected. Approach to the Codes\\t\\tDo not agree with segmentation on smaller services being excluded\\tDo not agree with decision to only apply additional measures to smaller services that have at least two illegal harms - lack of evidence to support this. If only harm is suicide content, that should still be addressed. Also argue there is no evidence to support claim that if there is only a single risk that ‘risk is more likely to be well understood across the organisation’, and say understanding of harm doesn\\'t necessarily mean service is taking action against it. Content moderation (Search)\\t\\tSupport proposal re. addressing moderation staff\\'s understanding of harm\\tSupport \"proposal that action may be needed to address gaps in moderation staff’s understanding of specific harms (13.158). This applies to suicide and self-harm content, where providers should be ensuring that moderation staff understand the nuance around self-harm and suicide language, provide users with personalised responses, and quickly identify and react to emerging trends\"\\nContent moderation (Search)\\t\\tSupport measure to provide crisis prevention information\\tSupport measure to provide crisis prevention information on search services but careful consideration of nuance needed, e.g. when suicide methods should be included in results. Current lack of consistency across industry. Content moderation (Search)\\t\\tMeasure to provide crisis prevention information shouldn\\'t just be for large search services\\tOfcom recognises that this measure re. crisis prevention info (22.83) is already in place on large and small services, so small services shouldn\\'t be excluded. This may encourage small services to stop this practice. Over-emphasis of economic approach to proportionality.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9968371391296387,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Safety shouldn't be viewed as stiffling innovation. Governance and accountability\\t\\tEvidence of new and increasing illegal content should be shared with trusted flaggers and statutory bodies\\tServices are required to track new and increasing illegal content (3E in annex 7) but should also be required to share this info with trusted flaggers and statutory bodies. Will help to prevent spreadng / access to new / less well known methods of suicide. Must be careful not to advertise methods that are identified. Governance and accountability\\t\\tServices' codes of conduct should include how to support users that post illegal content and are themselves vulnerable\\tServices are required to creat codes of conduct to protect users from illegal content (3F). This should  include an expectation from Ofcom that this will include how to support users posting illegal content who may themselves be in vulnerable circumstances or distress. Services should consider support to staff dealing with this content as part of this. Content moderation (User to User) \\t\\tThird party sources of support for content moderators\\tRequirement to support and train staff dealing with content moderation (4F) should highlight third part sources of support, e.g. Samaritans. Recommender system testing (U2U) \\t\\tSegmentation of  on-platform testing of recommender systems\\tRequirements for safety metrics for on-platform testing of recommender systems shouldn’t only apply to large / medium services or those with at least two priroity harms. Suicide and self-harm content often appears on services dedicated solely to this topic, so they would be excluded. Recommender system testing (U2U) \\t\\tRequirement re. dedicated reporting channel for trusted flaggers should include info sharing with agencies\\tThe requirement that platforms have a policy for establishing a dedicated reporting channel for trusted flaggers (3E) should include an approach to how intelligence of new and emerging harms arising from harmful suicide or self-harm content will be shared with relevant agencies. They suggest the NCA, and highlight need to inadvertantly publicise any info. Recommender system testing (U2U) \\t\\tSupport inclusion of suicide prevention information but more clarity needed\\tThey support the inclusion of suicide crisis prevention information (7C) but more info needed on  meaning of ‘general queries regarding suicide’. Say such information is needed for people making searches for online challenges or high profile deaths by suicide, and searching for methods of self harm.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9901555776596069,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Any sign posting should be 24/7 support services. Governance and accountability\\t\\tSupport the governance and accountability measures, argues the requirement must, however, be distinct from the enforcement power which enables a senior manager to be held liable for compliance with a confirmation decision. Support these proposals as necessary measures, and the proposal that all services should have a person accountable to the most senior governance body for compliance will help ensure regulatory compliance is consistently considered - however, this requirement must be distinct from the enforcement power which enables a senior manager to be held liable for compliance with a confirmation decision. The senior manager(s) held liable in these cases must be selected based on who has authority for the matter(s) covered in the confirmation notice – for example, content moderation, reporting, privacy settings. Governance and accountability\\t\\tSome of the proposed governance and accountability measures should  be extended to apply to small services as well as large services, and, as a minimum, should apply to small services with specific or multiple risks. Some of the proposed governance and accountability measures should  be extended to apply to small services as well as large services, and, as a minimum, should apply to small services with specific or multiple risks. Without these processes, small services will be ill-equipped to systematically identify, manage and report on risk (and small services are not immune to risk/small services can grow rapidly). Two measures which should be applied to small services: Measures 3E: Tracking evidence of new and increasing illegal harm; Measure 3A: Annual review of risk management activities. Governance and accountability\\t\\tMeasures 3E and 3A are important and without extending these requirements to small risky services, governance and accountability will be limited and risks undermining the whole regime. Measures 3E and 3A are important and without extending these requirements to small risky services, governance and accountability will be limited and risks undermining the whole regime. 3E is important as services will not be able to adapt their risk mitigation strategies to growing threats if they do not have an up to date understanding of how their service is being used to facilitate illegal harms - they need to be proactive and not wait until harm has taken place. Small services not exempt from most of the relevant measures in the Codes. 3A is important as services need to review their risk management activities to understand what is working and where they need to be strengthened - the online threat landscape is constantly shifting. The detail of this annual review would be proportionate to the size of the service, as it is likely that smaller services will have more streamlined governance systems, and so it is not an unreasonable burden to extend this measure\\nGovernance and accountability\\t\\tUrge Ofcom to reconsider the involvement of independent third-parties in monitoring and assurance for large multi-risk services- external review can help ensure that risk management systems fully meet the requirements of the regulation\\t\"Significant risk that companies will not put the strongest measures in place to tackle harm and protect users. Evidence from whistle-blowers has highlighted that tech companies are often unwilling to review internal data and tackle risk; large services repeatedly prioritised profit\\nand user engagement over children’s safety; some services were publicly hostile about the Act’s aims. External review can help ensure that risk management systems fully meet the requirements of the regulation; can help improve transparency in regulatory regimes (gives example from water sector). Third-party auditing will be particularly valuable for services choosing to implement their own measures, rather than follow the Codes. As this recommendation is for large multi-risk services, it is reasonable to expect that they will have the resource to fund a third-party review.\"\\nGovernance and accountability\\t\\tExecutive remuneration is often tied to delivering positive outcomes - encourage Ofcom to consider how they can shift senior management’s attention to safety outcomes. Executive remuneration is often tied to delivering positive outcomes - encourage Ofcom to consider how they can shift senior management’s attention to safety outcomes. This is the case in other sectors such as water.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9968506693840027,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approach to the Codes\\t\\tOfcom’s proposed measures must be considerably more ambitious, particularly for risky services, in the future. \"Many of the platforms which we are most concerned with would already be considered compliant when measured against these Code proposals - Ofcom’s proposed measures must be considerably more ambitious, particularly for risky services, in the future. Engagement: support the fast publication of this consultation, however it is disappointing that there was not an opportunity for earlier engagement. In future, independent experts and civil society must be engaged much earlier in the process. Ofcom must actively engage children and young people and survivors of online CSEA in the development of future Codes - there must be formal mechnisms in place, entirely unreasonable to expect these groups to respond to such detailed and lengthy policy documentation\\nEvidence and ambition: welcome the approach of Volume 2, however, the scale of harm demonstrated is not reflected in the ambition of the Codes. The structure of the Codes means, for the most part, services will only need to implement generic measures which, in many cases, they will already have in place.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8560409545898438,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Few major changes that the largest social media platforms will need to make. Concerned with the high evidential bar which has been set for proving efficacy, this approach has a bias towards industry interests. Also could curb innovation - if platforms are deemed as compliant by only implementing the Code measures. In future Codes we would welcome Ofcom setting outcomes that services should meet to help reduce illegal offences on their service; it should not always be necessary to include the means for achieving this. Safety by design: should be a greater focus on challenging services to build platforms which are safer in their design, not mitigating harm after the fact e.g. greater friction in offending pathways, recommender systems.\"\\nApproach to the Codes\\t\\tSmall services seeking to grow rapidly may initially overlook safety, and the Codes of Practice will do little to rectify this if they are not initially identified as risky. Recognise the need for proportionality and the logic of focusing on large/and/or risky services - however, small services seeking to grow rapidly may initially overlook safety, and the Codes of Practice will do little to rectify this if they are not initially identified as risky. Response to Vol 3 gives more detail - these recommendations would not result in a disproportionate burden on smaller services but ensure there is a minimum level of safety activity in place which it is reasonable to expect any platform to consider. Approach to the Codes\\t\\tSeven million users sets a high bar for a large service - once the full regulatory regime is in operation,  would expect that measures which are currently only expected for larger platforms to also apply to smaller ones. \"Seven million users sets a high bar for a large service - once the full regulatory regime is in operation,  would expect that measures which are currently only expected for larger platforms to also apply to smaller ones. \"\\nApproach to the Codes\\t\\tRecommend that Ofcom develops a new category of ‘large services for children’ in future Codes and applies key child safety measures to these platforms. 7 million – roughly 10% of the UK population. This proposal overlooks services with a low adult but high child user base. If a service is used by around 10% of the child population (1.4 million), this would make it a large service for children, but it would fall well below the proposed definition. Vital that those services which are most popular amongst children have to identify and mitigate risks (particularly of CSEA). Two platforms which may not currently be classed as large, but would be large for children, are Kik and Yubo, and both pose a risk for child safety (gives some stats on this)\\nApproach to the Codes\\t\\tRecommend that companies are required to track their user base on a monthly or quarterly basis. Clarity should be provided regarding how often services need to review their user-base to determine when they have become a large service.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934015870094299,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Volume 4 does not provide any specific requirements. Clear expectations for tracking growth are important for ensuring that companies comply as large services once they meet the 7 million threshold. Ofcom should also track the impact of the decision to use a 12-month average for measuring userbase. There are circumstances where this could mean that a service rapidly grows and reaches 7 million users, but then falls below this threshold again within 12 months.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9901684522628784,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Ofcom must ensure that this metric is effectively capturing all services with a significant reach and impact. Approach to the Codes\\t\\tAgrees with definition of multi-risk\\tAgrees with definition of multi-risk - more than one illegal harm on a platform indicates that it is used by different types of bad actors, requiring a more complex and comprehensive response. Approach to the Codes\\t\\tOne of our primary concerns is the lack of requirements for private and end-to-end encrypted (E2EE) services. Private messaging: Recognise that there are limitations imposed by the Act. Private messaging is the frontline of online grooming - data from ONS shows that 74% of approaches to children by someone they do not know online first take place via private messaging (and outlines further evidence and testimony). As far as the limits of the Act allows, future Codes must introduce stronger requirements for private messaging.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933839440345764,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is discussed further in answer to Q31. Ofcom should also introduce non-binding guidance for private messaging and end-to-end en-crypted service which provides detail on how and when Ofcom will look to use proactive tech-nology notices to deal with CSEA content. Approach to the Codes\\t\\tLarge and risky services should have tools in place to detect suspicious patterns of activity on their platform - this practice should be recommended in future Codes to ensure that services are proactively identifying dangerous actors, and not relying on children to report illegal activity\\tDetecting suspicious patterns of activity: Large and risky services should have tools in place to detect suspicious patterns of activity on their platform. For example, on Facebook and Instagram, Meta uses machine learning tools to detect patterns of abuse; Whatsapp uses automated technology to proactively scan unencrypted information for suspected CSAM sharing. This practice should be recommended in future Codes to ensure that services are proactively identifying dangerous actors, and not relying on children to report illegal activity. This will be particularly important for tackling perpetrator networks who share CSAM, where reporting tools will be futile. Approach to the Codes\\t\\tIn future Codes, Ofcom should assess existing systems and address how services can identify perpetrators and limit the risk of children being groomed across multiple services. Cross-platform risk: One common grooming tactic is for offenders to redirect conversations from public spaces to more private channels, includ-ing end-to-end encrypted environments. Abusers also speak to children simultaneously on platforms. Having multiple accounts across different platforms enables offenders to evade attempts by children to block them or stop engaging (NSPCC gives some quotes of calls to Childline). Government expected services to consider cross-platform risk, but there are currently limited examples of how services can tackle cross-platform risk. The announcement of Lantern, a cross-platform signal sharing programme to tackle grooming and financial sextortion of young people, appears to be a leading example of how services can collaborate and share information to prevent abuse. In future Codes, Ofcom should assess existing systems and address how services can identify perpetrators and limit the risk of children being groomed across multiple services. Approach to the Codes\\t\\tIt is vital that further measures are added in the next iteration of Codes which directly target perpetrator activity. Targeting perpetrator behaviour: A common feature of online grooming is perpetrators creating fake profiles (gives quotes/testimony). To reduce the risk of children be targeted by these profiles, services must tackle the creation of fake profiles – and particularly profiles where adults are claiming to be children. When recommending age verification and age assurance methods, Ofcom should consider not only the importance of accurately identifying children, but also the importance of identifying and blocking spurious accounts. Future Codes must also target the facilitation of abuse, e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928184747695923,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'the collation of legal images of children at scale. Content moderation (User to User)\\t\\tAgree with the proposals but would like to see some strengthened / extended. \"Effective content moderation is an important safety tool, though is largely reactive, so it is\\ncritical that systems are as accurate and efficient as possible. Automated systems have a critical role to play, but systems should be supported by appropriate human input. Online services should be expected to quality assure their moderation systems and dial up or down the role that human and automated moderation plays as appropriate.\"\\nContent moderation (User to User)\\t\\tSupport 4B: Setting internal content policies; and 4F. Provision of training and materials to moderators but urge that they are also applied to small services with a specific risk. We support these proposals, however we urge that they are also applied to small services with a specific risk. Clear internal policies and up to date training and support for moderators are essential for effective content moderation and decision-making on small services tackling a specific illegal harm. Only having one illegal harm on a service does not mean that it is simple to moderate (the ICJG recognises that there are a number of complexities in grooming, e.g. its gradual nature).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9932589530944824,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This challenge will only be exacerbated by emerging technologies e.g.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9933440685272217,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'virtual environment multi-user spaces, such as in VR strip clubs. Without a robust content moderation policy and targeted training, moderators are likely to face significant challenges in identifying this harm, posing a major risk to children’s safety. It is proportionate to expect that any service that has a medium or high risk of an illegal harm implement content moderation systems and processes. Content moderation (User to User)\\t\\tStrongly agree with A4.17 (the provider should resource its content moderation function so as to give effect to its internal content policies and performance targets) - but this measure should be expanded to require services to have regard for the results of their risk assessment when resourcing their content moderation functions. Strongly agree with A4.17 (the provider should resource its content moderation function so as to give effect to its internal content policies and performance targets). Dangerous impact of significant cuts to content moderation team has recently been demonstrated by X (gives example).This measure should therefore be expanded to require services to have regard for the results of their risk assessment when resourcing their content moderation functions. For example, services with a significant risk of CSEA must have individuals within their content moderation teams with the expertise and experience to moderate this harm. Welcome the recognition that services should also consider the languages used by their user-base. Content moderation (Search)\\t\\tAgree with our proposals. Two main risks to children posed by search services: accessing dangerous illegal material, such as illegal self-harm and suicide content; and perpetrators can search for and access CSAM through search engines. Support these proposals as well-targeted and proportionate measures.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928644895553589,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Automated content moderation (User to User)\\t\\tAgree with our proposals, but future Codes must give much greater weight to the important of these technologies\\tFuture Codes must give much greater weight to the important of these technologies – either by recommending services implement specific tools or by requiring them to develop their own. This approach must be supported by human input\\nAutomated content moderation (User to User)\\t\\tStrongly support the proposals for CSAM hash matching and CSAM URL detection\\tHash matching and URL detection: these tools are critical for ensuring CSAM can be proactively detected, removed, and reported. Fundamental to protecting the rights of these victims; preserves privacy; reduces risk of re-victimisation. CSAM URL detection is particularly important for tackling the way perpetrators use social media to network and direct other perpetrators to sites hosting CSAM (and use of coded language and legal content of children). Negligible risk to user privacy for non-victims, negligible false positive rate (gives evidence). Currently, CSAM hash matching tools are used by many large platforms where content is not end-to-end encrypted. We urge Ofcom to monitor the availability of these tools and expand the types of service this measure applies to - should also be applied to smaller services (700,000 users) with a medium-risk of CSAM (rather than just a high-risk)\\nAutomated content moderation (User to User)\\t\\tMajor gap that this Code of Practice does not contain any measures to support the proactive detection of unknown or ‘new’ CSAM.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9931771755218506,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Detecting \\'new\\' CSAM: Known CSAM represents just a fraction on online CSAM, outlines data from NCMEC. Huge number of children who are victims of CSA will be overlooked under the current measures. Reinforced by the challenges posed by the rapid development of generative-AI to generate hyper-realistic CSAM - risk that this will become widely available and overwhelm moderation teams. Many platforms are not using specific tools for detecting new CSAM, but there are examples of good practice e.g. Google uses artificial intelligence to identify new material (also gives example from Meta, and lists some other new tools including SafeToWatch). Rights implications of CSAM being shared online must be given significant weight in decisions about proactive technology. The next Code of Practice must require both large and risky services to deploy tools which automatically detect and remove all forms of CSAM, both known and unknown material. Automated content moderation (User to User)\\t\\tGiven its high-risk nature, we would expect a future CSEA Code of Practice to set out measures requiring companies to tackle CSEA in livestreaming. Livestreaming: detecting and disrupting CSEA on livestreaming is another area where automated technologies have a critical role to play. Testimony on the impact on livestreaming. IWF have raised that the material they remove online is often taken from livestreams where children have been groomed, coerced, and blackmailed. Industry efforts are currently limited, but there are cases where technology is effectively being used to protect children in livestreaming, e.g. Yubo. would expect a future CSEA Code of Practice to set out measures requiring companies to tackle CSEA in livestreaming. Automated content moderation (User to User)\\t\\tIn future codes, services should be required to utilise a range of tools, including default settings, human moderation, and automated tools, to both prevent and disrupt grooming. Keyword detection: Automated content moderation technologies should also be implemented to detect and disrupt grooming - the current measures rely on changing settings on children’s accounts, which place the burden on children(and chlildren often dont know they are being abused). These measures also don’t disrupt the grooming process. Keyword detection and machine learning can help to signifi-cantly speed up the process and inform triaging and human moderation. Gives two examples, Swansea University and Roblox, the latter of which uses filtering tech to assess all text chat on the platform. Services should be required to utilise a range of tools, including default set-tings, human moderation, and automated tools, to both prevent and disrupt grooming\\nAutomated content moderation (User to User)\\t\\tIn the guidance, a specific threshold or range for what constitutes a ‘substantial section of the public’ should be provided. \"Whilst this guidance is a helpful start in making this distinction, significant further clarity is required. Of the three key factors, measure A – ‘the number of UK individuals able to access the content’, is particularly opaque, so it is unlikely that providers will use a consistent definition. A specific threshold or range for what constitutes a ‘substantial section of the public’ should be\\nprovided, and a number of case studies and examples should be provided (as in the ICJG). Gives exampes of large group chats on Snapchat and Whatsapp, and community features. Gives a testimony quote \"\\nAutomated content moderation (User to User)\\t\\tConcerned that this guidance [Annex 9] classes metadata as private content\\tMetadata plays a key role in services’ efforts to detect and disrupt bad actors on their platforms (gives example of Meta and Whatsapp). Considering these platforms use or will continue to use metadata whilst end-to-end encrypted, we urge Ofcom to reconsider designating this content as private. This risks significantly undermining efforts to tackle CSEA on private messaging, and will limit the steps that private communication platforms can be asked to take to tackle illegal activity on their platform. Automated content moderation (User to User)\\t\\tAnnex 9 does not set out different standards of private communication – for example, the difference between non-encrypted, encrypted, and end-to-end encrypted messaging services - this is inconsistent with Ofcom’s approach in other documents (Summary and Consultation at a Glance)\\tEncryption. Annex 9 does not set out different standards of private communication – for example, the difference between non-encrypted, encrypted, and end-to-end encrypted messaging services. Whilst we support this, we note that it is inconsistent with Ofcom’s approach in other documents. In the Summary and Consultation at a Glance, end-to-end encrypted services are specifically refer-enced as being exempt from certain measures alongside private communications. it is vital that end-to-end encryption is not given specific carve outs or special status within the Codes and wider regulatory regime. The Act’s tech neutral approach is fundamental to future proofing and ensuring that there are no incentives for services to encrypt in order to evade regulatory responsibilities. We urge that Ofcom removes the specific references to end-to-end encryption and instead consistently uses the public / private distinction. Automated content moderation (Search)\\t\\tAgrees with our proposals \\tAgrees with our proposals \\nUser reporting and complaints (U2U and search)\\t\\tAgree with our measures but would like to see some measures strengthened and additional measures included.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9903545379638672,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'At present, reporting is under-used and ineffective.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9739531874656677,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Children are disillusioned with the efficacy of reporting tools (gives evidence). Reporting tools have significant limitations as a child protection measure (children often unaware of grooming dynamics, and doesnt capture offender-to-offender interactions eg CSAM sharing. Needs to be balanced with wider measures. User reporting and complaints (U2U and search)\\t\\tRecommend an additional reporting measure is introduced: All services should be required to provide clear and accessible information about what the reporting process is, and what the consequences will be for the reporter and the user/content being reported, at the start of the process. All services should be required to provide clear and accessible information about what the reporting process is, and what the consequences will be for the reporter and the user/content being reported, at the start of the process. Insights from Childline show that children can be put off reporting (gives insight from Thorn and testimony quotes). This new measure would reduce a key barrier to reporting for children and young people. Improving transparency is a reasonable expectation for services\\nUser reporting and complaints (U2U and search)\\t\\tSupport measure 5B. Having an easy to find, easy to access and easy to use complaints system\\tSuggestion that users should be able to provide contextual information (A5.4 (d)) is particularly important. Young people consistently raise that despite material being illegal, it is not taken down when reported (gives examples). Crucial that there is space to explain why an image must be re-moved to speed up the process of taking it down and ensure that children do not need to make repeated reports and complaints. User reporting and complaints (U2U and search)\\t\\t5C. Appropriate action – sending indicative timelines: strongly recommend that this measure is changed to include a requirement that users are updated on the outcome of their report. Strongly disagree with the decision not to recommend that services update users on the outcome of their reports and complaints. This is crucial for children: vital that they know if action has been taken against the perpetrator; otherwise unsure if they need to take further action or if the issue has been resolved (gives Childline testimony); reinforces opaque nature of reporting. We strongly recommend that this measure is changed to include a requirement that users are updated on the outcome of their report - as a minimum, for users that make a report about CSEA. Alternatively, the measures regarding default settings to children could be amended to include a requirement that children are always updated on the outcome of a report. Critical that this change is made\\nUser reporting and complaints (U2U and search)\\t\\t5I. Dedicated reporting channels - strongly recommend that in future Codes a new measure is included that providers with a medium or high risk of CSEA establish and maintain a dedicated reporting channel for trusted flaggers. We strongly recommend that in future Codes a new measure is included that providers with a medium or high risk of CSEA establish and maintain a dedicated reporting channel for trusted flaggers. The NSPCC operate a Trusted Flagger process - there have been some challenges with this process. Requiring the creation of a dedicated reporting channel in the Codes and setting out key principles would be highly valuable for reducing the burden on trusted flaggers and ensuring their reports are meaningfully actioned. This is crucial for ensuring that moderation is informed by external experts and that high priority material can be identified and removed. Recommend considering the Guidance for Trusted Flagger Programmes produced by UKCIS to inform future measures\\nDefault settings and user support (U2U)\\t\\tUrge that Ofcom uses its information gathering and supervision powers to better understand the range of ways that platforms can disrupt grooming so that a wider range of technical solutions can be introduced that take the burden off children. \"Very welcome that it will be more challenging to identify and contact children, especially via messaging services - at present, far too easy for children to be contacted by strangers online. No silver bullet - these settings will need to work alongside other proactive detection measures and tools. Making these options default but with the option to turn them off rightly empowers children, but also means the burden is on them. We urge that Ofcom uses its information gathering and supervision powers to better understand the range of ways that platforms can disrupt grooming so that a wider range of technical solutions can be introduced that take the burden off children. Effective age-checking will be fundamental for these tools to work - young people’s panel immediately questioned how well they would work in practice (cited Ofcom’s own research on age of users). Vital that Ofcom ensures future Codes set out rigorous measures for how platforms improve their age assurance processes (will also help tackle the creation of fake profiles). Platforms must also be required to consider any steps they can take to identify existing accounts which have put the incorrect age upon signing up. \"\\nDefault settings and user support (U2U)\\t\\t7A. Safety defaults for child users - support these measures, but young people\\'s panel said there are a number of benefits to being able to see the lists of child users they are connected with\\tListed A7.2 measures. Support these measures as ways to make it more challenging for perpetrators to quickly identify large groups of children - network expanding prompts are particularly risky for children (gives testimony quote). However, young people’s panel raised that there are a number of benefits to being able to see the lists of child users they are connected with. There are trade-offs children will have to make. Example of the limits these settings may have in the eyes of children and the importance of taking a holistic approach to tackling grooming. Default settings and user support (U2U)\\t\\tA7.3 If the service has direct messaging, the provider should implement default settings - these were identified by the most important by the young people\\'s panel, and raised the important of seeing who a user is before accepting a message (which could be an extra recommendation)\\tDefault settings were identified as the most importantby the young people’s panel - who highlighted the importance of being able to more easily reject interactions that they do not want, and wanted this to be built into the messaging functionality, and ensure they do not have to see the messages before doing this. These settings would effectively address these needs. Our panel also raised the importance of being able to see who a user is, before deciding whether to accept a message from them - this could be included in the Code as an extra recommendation for direct messaging. Default settings and user support (U2U)\\t\\t7B Support for child users -it would be valuable for companies to have guidance to help them in developing effective and age-appropriate information. Ofcom could produce this guidance themselves or commission it. The success of these support messages will be determined by how they are designed, which will likely differ - services must engage directly with children or representative groups in order to develop messaging. Young people’s panel emphasised that whilst these sorts of messages would be helpful, at the moment this information tends to be buried. There are benefits of not including specific requirements for this support information in the Codes, however, it will be complex to develop and the tone needs to be right (especially the recommendation that services should provide information to child users about the risks of disabling a default setting). Ofcom could produce guidance themselves or commission it. Young people’s panel also made a number of suggestions about this support information: reassurance that you can change settings back, sharing updates about new settings, key messages not able to scroll/skip. Services should do user testing. When reviewing a service’s compliance with this measure, Ofcom should not just check whether this information is technically available, but whether it is accessible and child-centred. Default settings and user support (U2U)\\t\\tWe recommend that in future Codes of Practice, platforms are prompted to develop tools that enable client-side nudity detection and mean potentially explicit images are blurred. We recommend that in future Codes of Practice, platforms are prompted to develop tools that enable client-side nudity detection and mean potentially explicit images are blurred, with the user deciding whether they want to view, delete, or report the image before viewing it. This setting is used by Bumble and Apple.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9752943515777588,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Way to promote innovation and raise bar for best practice. Default settings and user support (U2U)\\t\\tFuture codes: turning off visible tags and making children\\'s bios not visible to users they are not connected with \\t\"Children have highlighted that they would like to be less discoverable online. Two key changes that would support this:\\nTagging:  services should turn off visible tags, so  other users cannot view the accounts of users tagged in content shared by children. Information in bios: Platforms which have free text bios often contain personal information about a user, potentially including where they live, their age, and their interests. This information should not be visible to users that a child is not connected with. Young people\\'s panel also suggested there should be a default setting which means children do not receive notifications about accounts that they are not connected with.\"\\nDefault settings and user support (U2U)\\t\\t\"Young people’s panel suggested that prompts or support information at the stage of creating an account might help children consider the risks of putting in the wrong age. \"\\t\"Young people’s panel suggested that prompts or support information at the stage of creating an account might help children consider the risks of putting in the wrong age (which is very simple to do) and the protections being recognised as a child affords. \"\\nDefault settings and user support (U2U)\\t\\tWe recommend that, for children’s accounts with at least one default setting turned off, services set ad-hoc prompts encouraging the child to review their settings\\tFor children’s accounts with at least one default setting turned off, services should set ad-hoc prompts encouraging the child to review their settings and providing information about how to turn safety settings back on. This takes the onus off the child. Could be periodic, or when the child engages with a new feature, or has a certain reach. Recommender system testing (U2U)\\t\\tThis proposal should be extended to apply to all large, multi-risk services, regardless of whether they already have on-platform testing of recommender systems in place. This proposal should be extended to apply to all large, multi-risk services, regardless of whether they already have on-platform testing of recommender systems in place. The cost of this would be proportionate. Without undertaking robust safety testing, services will not be able to fully understand the impact of changes to their recommender systems. Research and evidence clearly demonstrate that there is a connection between algorithms designed without user safety in mind and harm to children (outlines research from Centre for Countering Digital Hate and Molly Rose Foundation, and testimony from Childline). Children’s accounts with indicators of vulnerability are particularly likely to be recommended potentially illegal suicide and self-harm content, as well as eating disorder content. Facebook also recommends potential offenders to similar sites. If large services do not already have testing systems in place, then that is an indication of weak safety governance. It is not an acceptable reason for continuing not to test, and the Codes should rectify this. Enhanced user control (U2U)\\t\\tAgree with the proposals in this section, particularly around user blocking, muting, and disabling comments, though are concerned some of the analysis significantly overstates the benefits. Agree with the proposals in this section, particularly around user blocking, muting, and disabling comments, though are concerned some of the analysis significantly overstates the benefits to users, and particularly to children. The analysis in the section around grooming (that blocking/muting can help child users protect themselves online) misrepresents the nature of online grooming/CSA - fake profiles are deceptive and misleading and mean that children are more likely to trust them. Gives evidence/testimony from Childline.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9920159578323364,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Not opposed to measures but they shoudl not should not enable services to evade implementing safer design features, proactive detection tools, and strong moderation systems. Enhanced user control (U2U)\\t\\tRecommend that services are required to make these controls known to users, and that this should be informed by where these measures will be most useful. Given children’s apathy to reporting systems generally, they are unlikely to be proactive in seeking out other similar tools without being prompted. Young people’s panel raised that blocking tools are often hidden and not straightforward to use. Would be particularly useful to be notified about blocking / muting options in direct messages. We recommend that services are required to make these controls known to users, and that this should be informed by where these measures will be most useful to users. User access to services (U2U)\\t\\tOfcom should use their information-gathering powers to understand how regulated services are approaching blocking users who have shared CSAM to inform future iterations of the Code. Recognise the challenges of blocking users who have shared CSAM, but there needs to be repercussions. Ofcom should use their information-gathering powers to understand how regulated services are approaching blocking users who have shared CSAM to inform future iterations of the Code. Information from tech companies on the efficacy of blocking tools and their current practices would help the development of appropriate, well-targeted recommendations. Systems which block IP addresses and report users to law enforcement should be considered. Cross-platform cooperation would enable services to share signals about accounts which have shared CSAM. See also Q16\\nUser access to services (U2U)\\t\\tCriteria for blocking a user for sharing CSAM should be developed This guidance should consider the age of the offender, the nature of the CSAM, the intention, and whether it was a repeat offence. \"Criteria for blocking a user for sharing CSAM should be developed This guidance should consider the age of the offender, the nature of the CSAM, the intention, and whether it was a repeat offence. Age: children should be treated with greater leniency, may be sharing out of shock or humour. Also should be no penalties for sharing their own images. Intention: cases where adults share CSAM out of outrage. Service should implement blocking procedure here but users should be able to appeal their ban where there is reasonable evidence.\"\\nUser access to services (U2U)\\t\\tMore harm is done by not safeguarding children than by false positives, and blocking measures must therefore not be disproportionately weakened\\tAutomated systems are overwhelmingly more likely to detect genuine CSAM than false positives, especially for known CSAM (noted NCMEC false positive rate). To manage risk, Ofcom could recommend that services ensure human moderators are involved in cases where a user might be banned from a service (with key features disabled in the interim). Implementing accurate automated detection systems, supported by efficient reporting channels, will reduce the risk of law-abiding users being negatively impacted\\nService design and user support (Search)\\t\\tStrongly support the proposals to provide users with warnings / support information if they are searching for CSAM or suicide content (7A and 7B) - ervices should be required to share these new terms with other services, or with a central provider\\tStrongly support the proposals to provide users with warnings / support information if they are searching for CSAM or suicide content (7A and 7B). The requirement that services develop and maintain an appropriate list of terms commonly used to search for CSAM provides a potential opportunity for cross-platform collaboration.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9925500154495239,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We would expect services to identify new terms and in future Codes, services should be required to share these new terms with other services, or with a central provider, to ensure that efforts are consistent and comprehensive. Signposting to appopriate organisations should be done with the consent of and in collaboration with the organisation being signposted to. Ofcom should also consider how income generated from fines in the future could be used to help fund these support organisations. The provision of suicide crisis prevention information must be appropriate for both child and adult users. Cumulative Assessment\\t\\tAgree that these measures are certainly not overburdensome; there are areas where the requirements should be extended.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7460393309593201,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Number of areas where further measures should be applied. Analysis of police FOI data indicates that over 150 different apps, games and websites were used for grooming. It is highly unlikely that all of these will be classed as large platforms. It is therefore vital that smaller platforms with a risk of child sexual abuse are held accountable. The Act recognises the importance of proportionality, but its core aims (set out in Section 1) do not distinguish between services. This will only be possible if reasonable safety measures are imposed on small services that pose a specific risk to children’s safety online, and is not something we think is fully delivered in this first Code. Cumulative Assessment\\t\\tAgree that the plans for large services are not overburdensome, and in future, services must be required to implement new features and tools that mark a step change in how they protect childre\\tAgree that the plans for large services are not overburdensome. We note that many of the measures proposed in the Codes are already widely used by large services e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9812896847724915,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'hash matching, reporting and blocking functionalities, and published terms of service. Alone, these measures will be insufficient in tackling CSEA online. In the future, services must be required to implement new features and tools.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9381365776062012,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We have recommended such measures throughout our response, e.g. more proactive tools to detect and disrupt abuse, making changes that will target perpetrator behaviour, and being more ambitious with requirements for private messaging. Default settings and user support (U2U)\\t\\tMost important settings were not receiving direct messages from strangers, also support location sharing setting. Most important settings were not receiving direct messages from strangers (most common issue experienced online, this would allow greater control). They also support location sharing setting (these are often unclear). Not being able to be recommended other people to connect with has its own trade-offs. Could even help safety to see who friends are connected to. Default settings and user support (U2U)\\t\\tSafety and support info needs to be presented in an engaging and straightforward way\\tSafety and support info needs to be presented in an engaging and straightforward way. Don’t just share when settings are being changed. Includes table of how effective and safety and support info should be presented (how it should be written, how best to share, other opps). Default settings and user support (U2U)\\t\\tRaised some gaps in these recommendations\\tQuestioned effectiveness given age-checking systems are weak. Pointed out it is common to get notifications on activity from accounts they\\'re not connected with, which promotes interactions with strangers. Image blurring was suggested\\nEnhanced user control (U2U)\\t\\tWant reporting and blocking to be more prominent in direct messaging\\tWant reporting and blocking to be more prominent in direct messaging - can be particularly inaccessible and convoluted. Also encouraging to get a message outlining next steps when you do block an account\\nEnhanced user control (U2U)\\t\\tWould like to know action taken after a report\\tWould like to know (or have option to know) action taken after a report, would encourage them to use the function again. User access to services (U2U)\\t\\tCaveats regarding CSAM measures should be clearly stated and narrow in nature\\tThe current reading of some of the risk mitigation measures relating to sharing CSAM material (such as the example given of how accounts may be blocked) were not as unequivocal as they should be. Where there are reasons for such caveats they should be stated and narrow in nature.\\'\\nApproach to the Codes\\t\\tCodes should take a greater \\'safety-by-design\\' approach\\tWe consider clause of the Act an overarching requirement, to provide a service that is safe by design, and feel that was lost in the detail of the code. By referring to clause one, it will remind regulated companies that Ofcom can and should take action on poor or negative outcomes regardless of compliance with the codes of practice. Desire to avoid a situation where service providers can persistently be non-compliant and unpunished, or constantly be ‘compliant’ but their products are no safer.\\'\\nApproach to the Codes\\t\\tMore emphasis needed on how systems and proceses will stop the spread of illegal content (e.g. by stopping it being recommended)\\tOn systems and processes, a fundamental outcome of the regime must be that platforms are not recommending illegal content to users. This must be unequivocally clear as an outcome. Platforms and the regulator will have to determine what data is being pulled and try to figure out how to stop it. There does need to be more emphasis on the syste ms and processes doing the job of routinely preventing the spread illegal material which should mean that it is not incumbent on the regulator to prove each piece of material is illegal but rather that the flow of material is not allowing illegal material through.\\'\\nApproach to the Codes\\t\\tToo much emphasis on efficacy of measures, rather than focusing on process-based recommendations which would allow for services to iterate measures until they become effective\\tOn best practice for risk mitigation, we were encouraged by your goals to access information relating to mitigation measures that might be tested internally at a platform but not adopted. We felt that whistleblowers who have left the sector would also be a good source of information on this topic. However, as set out in point 5 of this document, we are concerned that Ofcom is overly intent on proof that a particular measure is effective, rather than setting in train process measures that offer the opportunity for a company to iterate until they have reached the desired goal.\\'\\nGovernance and accountability\\t\\tSupport of internal governance and monitoring recommendation. \"With the support future recommendations are provided: \\n* Accountable person is senior in service/platform, \\n* For larger/more risky this senior person is also accountable to a Committee.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927493333816528,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '* Committee is accontable to ministers. When a rise in illegality is detetected the Service/Platform should:\\n* Inform law enforcement. * Take proactive steps to counter the increase.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9958717226982117,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Further recommendation regarding an external audit to scruntinise efforts and effectiveness of them. Recomendation that failure to comply with meaningful action is taken. \"\\nGovernance and accountability\\t\\tRecommendation of including an emphasis on the causes of fraud beyond financial loss. The imapact of fraud is wider than the obvious financial loss. Pathways to serious criminality and criminalisation of children and the vulnerable needs to not go unnoticed. The impact and pathway generated from \\'money mule\\' fraud is worth unpacking. Public distrust of e-commerce and the impact that has on the UK\\'s adoption of online transactions is raised as a concern for future prosperity. Governance and accountability\\t\\tSingle word answer: \\'Yes\\'.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9972768425941467,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Single word answer: 'Yes'. Governance and accountability\\t\\tSingle word answer: 'No'. No\\nGovernance and accountability\\t\\tSingle word answer: 'No'. No\\nApproach to the Codes\\t\\tRecommendation Ofcom consults with Ofcom to ensure CoP's meet requirements policing and counter fraud partners. Recommendation Ofcom consults with Ofcom to ensure CoP's meet requirements policing and counter fraud partners. Approach to the Codes\\t\\tAgreement in principle but with caveats.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9963181018829346,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"Medium and High Risk assessments should factor in the offence and harm type. Request for \\'onerous measures\\' to be defined. \"\\nApproach to the Codes\\t\\tSingle word answer: \\'Yes\\'.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8888131380081177,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Single word answer: \\'Yes\\'. Approach to the Codes\\t\\tSingle word answer: \\'Yes\\'. Single word answer: \\'Yes\\'. Approach to the Codes\\t\\tRecommendation for advised timelines of complaints procedure. \"Recommendation for advised timelines of complaints procedure. Obligation to act quickly upon reciept of information from Trusted Flaggers and Dedicated Reporting Channel. Requirement to feedback confirmation action was taken. The two year feedback cycle shortened to a year. \"\\nApproach to the Codes\\t\\tSingle word answer: \\'No\\'. No\\nContent moderation (User to User) \\t\\tSwiftly to be defined in relation to content removal and recommendations that assurances will be provided that violative content and those that use a service illegality are removed. \"Recommendations:\\n* Obligation to act quickly upon reciept of information. * Requirement to feedback confirmation action was taken. * Obligation for services to proactively identify linked content and take proactive steps to remove. * Removal of User who posted removed content. * Violative content and User to be shared with police for intelligence purposes.\"\\nContent moderation (Search)\\t\\tAgreement with proposals. \"Agreement that illegal content should be prevented from being presented to users during search. Agreement that this needs to be monitored and moderated, and that sufficient resource is applied to achieve this. \"\\nAutomated content moderation (User to User)\\t\\t\"Agreement with key word search to identify content. Recommendation that this obligation is taken up across the board.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9887697696685791,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Banning of users from service if found to have committed Fraud.\"\\t\"Recommendations:\\n* Obligation to act quickly upon reciept of information. * Joint up approach between services so fraudsters cannot jump from one to the other, especially smaller ones. * Obligation for services to proactively identify linked content and take proactive steps to remove. * Removal of User who posted removed content and enforcement of a ban from the service for a substantial period of time.\"\\nAutomated content moderation (User to User)\\t\\tSingle word answer: \\'No\\'. No\\nAutomated content moderation (User to User)\\t\\tSingle word answer: \\'No\\'. No\\nUser reporting and complaints (U2U and search) \\t\\tAgreement that all services should have an easyily identifiable process for users to report. \"Recommendations:\\n* Established time limit set to deal with complaints.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992493748664856,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '* Use of Trusted Flaggers. * For Fraud - The City of London Police are made aware of the result of all action taken against fraudulent content/users. \"\\nTerms of service and Publicly Available Statements\\t\\tToS accessible to all.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.991134762763977,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'ToS accessible to all. Terms of service and Publicly Available Statements\\t\\tSingle word answer: \\'No\\'. No\\nDefault settings and user support (U2U)\\t\\tSingle word answer: \\'Yes\\'. Yes\\nRecommender system testing (U2U) \\t\\tSingle word answer: \\'No\\'. No\\nEnhanced user control (U2U) \\t\\tAggreement with recommendation relating to the Online Fraud Charter. \"Recommendation:\\n* Adoption of Online Fraud Charter\\'s recommendations that users should be given an option to limit their engagement [REMOVE - with / INSERT - to] with fellow users who have undergone a level of verifictaion. * Lack of ID verification poses a threat. Romance Fraud for example. * Verification would also positively impact Online Shopping fraud. \"\\nEnhanced user control (U2U) \\t\\tYes – this should be promoted to users and also to the parents of children and young persons. Yes – this should be promoted to users and also to the  parents of children and young persons. Enhanced user control (U2U) \\t\\tVerification would have benefits across the board when tacking Fraud. \"Statement:\\n* Online Shopping Fraud occurs on Facebook Marketplace and eBay. * Voluntary verification would possibly impact this.\"\\nUser access to services (U2U)\\t\\tAggreement with recommendation relating to Fraud. \"Recommendation:\\n* Extend to all users that share illegal content including Fraud. * Block and banning those that exploite services to target others. * Measures to prevent banned users from creating accounts in different names.\"\\nUser access to services (U2U)\\t\\tSingle word answer: \\'Yes\\'. Single word answer: \\'Yes\\'.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924173355102539,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Service design and user support (Search)\\t\\tSingle word answer: 'Yes'. Single word answer: 'Yes'. Governance and accountability\\t\\tThere is broad agreement with the proposed G&A measures. Clarity is required on how Ofcom will ensure that services risk assessments are fit for purpose. It is essentail that animal cruelty content is amongst the illegal harms, as set out, to avoid a low risk score, thus failing to reflect the importance of tackling animal cruelty. Services must consult with credible  and professional sources with detailed understanding of the Animal Welfare legislation. Extensinve research by Social Media Animal Cruelty Coalition (SMACC) identified that platforms frequently failed to remove such content, breacing their own policies. Robust and specific sanctions, including reductions in senior manager renumeration, will be needed to address such inaction and establish clear accountability. Approach to the Codes\\t\\tFailure of seocial media platforms failing to implement their own policies. Social media platforms, as documented by the Social Media Animal Cruelty Coalition (SMACC), frequently fall short in enforcing their own policies consistently. To address this, there is a critical need for enhanced training for moderation teams and the establishment of systems for continuous policy monitoring and review. Additionally, platforms should actively engage with experts to address illegal harms and develop effective approaches for handling harmful content\\nApproach to the Codes\\t\\tHighlights gaps in illegal hams measures\\tGMCA responded from perspective of Crim Commissioner remit - running through a series of harms, they challenge the focus of a number of harms and suggest ways in which they could be improved\\nGovernance and Accountability\\tDefining High/Low risk services\\tRisk that illegal activity moves to smaller services/ dark web\\tGMCA responded from perspective of Crime Commissioner remit. They say the large service/high risk approach may be proportionate but this does not address the risk of illegal harms manifesting on smaller services or on the dark web. Governance and accountability\\t\\tAs a low risk VSS, the accountable person requirement is disproportionate and goes further than EU DSA. We strongly agree with the majority of the proposals in relation to governance and accountability measures, as they relate to large low-risk vertical search services (specifically, the decision to exempt such services from many of the measures). No evidence of  large VSS pose lower threat to users due to functionalities.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9964995384216309,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Concurs with Skyscanners own evidence. The recommendation that all services name a person accountable to the most senior governance body for compliance with illegal content duties and reporting and complaints duties goes beyond what is required of online platforms and intermediary services under the EU’s Digital Services Act. The measure therefore imposes a disproportionate burden on low-risk services which are in scope of both pieces of legislation (but are not a VLOP under the DSA). Approach to the Codes\\tDefinition of large services\\tAppreciates EU DSA alignments but does not believe definition of large service is nuanced enough\\tWe do appreciate the attempts to align this definition with the EU’s in the Digital Services Act (DSA). but only when the corresponding proposal in the DSA is itself proportionate and effective. When it comes to defining large services, we believe that the more important consideration is whether the accompanying obligations that are attached to being defined as a large service are themselves proportionate. In this particular case, we recognise that the definition of a large service comes with less onerous obligations than VLOP/VLOSE designation under the DSA. That said, we would also note that the EU market is much larger than the UK market, and that reaching over 10% of the population of the EU market is likely to be indicative of a large, multinational firm operating across multiple markets. Such firms are likely to be significantly bigger and better resourced than a firm that reaches 10% of the population only in one market, and so it may not always be appropriate to follow the EU’s definition of what constitutes a large service\\nApproach to the Codes\\tMulti-risk definition\\tMulti-risk definition is disproportionate for smaller services\\t\"Multi-risk services identified as medium or high risk for just one kind of illegal harm would be disproportionate, with relatively few online safety benefits. if a service is at medium or high risk for only one kind of illegal harm, it is more likely to be well understood and addressed already by the service, compared to if a service has “multiple areas of risk.”  Ofcom then goes on to suggest an alternative would be to set the definition “at a high level” (without stating what it considers that level would be), but that this would mean the benefits from measures would be smaller due to being applicable to fewer services. Skyscanner argue that there are many other options between those two extremes which would better balance proportionality and online safety benefits than the option being proposed by Ofcom. That a service found to be medium risk from just two kinds of illegal harm could face many of the same obligations as a service found to be high risk for 5/6/7+ kinds of harm is, to us, clearly disproportionate. They suggest alternative approaches to definition of multi risk\"\\nApproach to the Codes\\tOfcom Cost assumptions\\tThe estimated labour costs in Annex 14 are too low and opportunity cost should be reconsidered\\t\"Skyscanner argue that the estimated labour costs are still too low, even the higher ones. The high gross annual wage estimates for professional occupations are significantly lower than costs, as are the estimates for software engineer. Ofcom should give due consideration to the opportunity costs imposed by these measures, particularly for low-risk services. Resource dedicated to complying with measures that do little to increase the safety of UK consumers online (when applied to low-risk firms) is resource that cannot be dedicated to innovation, product development, or hiring new staff. These costs should be given due consideration by Ofcom.\"\\nContent Moderation (Search)\\t\\tWould welcome updated definitions and wording to reflect VSS\\tWe would, however, welcome updated definitions and wording to reflect the fact that VSS do not “deindex” or “downrank” content, due to the fundamentally different way in which they operate compared to general search services. Given our control over the search results we present, for example, we already have a robust policy of removing any content that is deemed to be illegal or in breach of our terms of service, when we become aware of it. We believe an update to the guidance, which makes clear that other forms of removal beyond deindexing are acceptable, would aid clarity. User reporting and complaints (U2U and Search)\\t\\tCloser allignment to EU DSA is welcome but do not agree with process for user complaints and the administrative burden placed on small services. \"Given the existence of similar provisions in the EU’s DSA, Ofcom should ensure that these duties are closely aligned to avoid services having to make multiple changes across the two jurisdictions to achieve the same objectives. We disagree with the judgement of Ofcom that at least two separate processes for users (a report function for illegal content and a broader complaint function) is necessarily needed (evidence in the response) We also believe that the requirement to acknowledge receipt of all relevant complaints and provide indicative timelines for their resolution is an unnecessary administrative burden on low-risk services, when they will already be under a separate obligation to respond promptly. We are also slightly concerned that the requirement for a reporting function for illegal content that is “clearly accessible in relation to that search content” could lead to an overly prescriptive approach that could undermine the usability of search services. (Evidence in response) Ofcom should make clear that they do not expect reporting functions to be immediately next to each individual piece of search content, which would clutter the page. \"\\nGovernance and accountability\\t\\tArgues our governance principles are not designed for non-profit organisations\\t The Foundation is concerned that the proposals neither consider platforms that are governed by communities nor do they sufficiently factor in the resource limitations of nonprofits. The proposed Governance and Accountability framework does not appropriately account for the role of effective community self-governance, because it is modelled primarily on centralised approaches to trust and safety. The framework’s implicit focus on centralised platform governance is evident in Ofcom’s suggestion that all \"multi risk services\" must have \"A Code of Conduct that sets standards and expectations for employees around protecting users from risks of illegal harm.\" The obligation to \"track evidence of new kinds of illegal content\" is another example of an underlying assumption that services systematically monitor and remove content in a centralised manner. Community self-governance is essential for Wikipedia and the other Wikimedia projects to continue to function and provide accurate educational content that is free and open to use for everyone. Because the framework does not adequately consider community-led governance mechanisms, it does not benefit platforms like Wikipedia. Governance and accountability\\t\\tDisagrees with the proposal on governance and accountability measures\\tdisagrees with the proposal, since it does not sufficiently take into account public interest projects delivering public goods, especially those with volunteer community self-governance models. Governance and accountability\\t\\tImplementing auditing costs to meet requirements is prohibitive for a non-profit org\\tThe possibility of financing any external audit requirements. For the Foundation, such a mandate would be an example of where there is “potential for significant costs” (as Ofcom acknowledged on page 6 of Volume 3). If the proposed fees imposed by potential auditors in the context of the EU Digital Services Act (DSA) are any indication, then we can expect audit fees to be in the six-to-seven figure range. Payments to the auditors would go hand-in-hand with corresponding internal costs to support the audit. Investments would also be required to cover detailed training, design, and consultancy work required to make these UK-specific. Complying with multiple external audits across jurisdictions like the EU, UK, and others is an existential risk for a nonprofit, and diverts sizable resources away from their public interest mission. Governance and accountability\\t\\tTying remuneration to outcomes would undermine the purpose of the organisation\\tThe Foundation, as a nonprofit organisation, will not practise bonus-related performance and remuneration management for its staff. Moreover, it would oppose introducing such practices mandated for public interest platforms under government regulation.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9934269189834595,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The reason is that introducing these incentives might undermine the very alignment with self-governing community partners, which is based on a shared educational and public interest mission that delivers the beneficial societal outcomes upon which the public has come to rely. Approach to the Codes\\tDefinition of large service\\tDisagrees with defintion of large service due to requirements to track user which is against business principals and be difficult to address\\t\"In summary, wikimedia disagrees with: the large services definition would require the Foundation to build new monitoring tools as well as change how we operate in relation to our users. We pride ourselves on not tracking users.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8341398239135742,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'For this reason, among others detailed in this consultation response, we do not know the size of our active UK userbase, and can only share an approximation of 6,000. Further caveats behind this sum include the fact that it refers to the amount of “active” contributors (defined as people who made five or more edits per month), and is limited to “active” editors on the English language version of Wikipedia. This is an issue that we also raised, and for which the European Commission provided us with accommodation, in its DSA implementation.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9828444719314575,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nApproach to the Codes\\t\\tCodes of practice are disproportionate burden\\t\"The Foundation has raised substantial concerns about Ofcom’s proposed approach to risk profiles in response to the relevant volume as it relates to public interest platforms and, by implication, naturally has severe concerns about the proposed Code(s) that will be developed on that basis. The Foundation has concerns about the proposed approach to make all platforms subject to the Code(s) on a baseline irrespective of the actual, empirically-founded risks that they might entail. Doing so would be a disproportionate burden on small and medium enterprises as well as public interest platform providers with a low structural risk profile. \"\\nApproach to the Codes\\t\\tApplying measures because they are \\'large\\' is impractical and disproportionate\\tApplying measures because they are \\'large\\' is impractical and disproportionate\\nApproach to the Codes\\t\\tDisagrees with definition of large service\\tThe numerical approach (7 million users) to defining large services does not appear to distinguish between users who are content contributors and users who are merely readers. As can be seen in our publicly available statistics, even the largest public interest project that the Foundation hosts, English language Wikipedia, has only around 6,000 active contributors in the UK. This is due to the complexity of defining what it means to be a “user” of a Wikimedia platform—which we have explained, above, in this consultation response. “Users” of projects can be divided into two different types: 1) those who are passive readers; and, 2) those who are active content contributors. Another complicating factor is determining where these users are based: Wikipedia is language specific and, therefore, many readers and contributors of content in the English language will not necessarily be based in the UK. Approach to the Codes\\t\\tDisagrees with the current version of codes\\tIn summary - proposed governance arrangements do not take into account public interest platforms. active monitoring of ongoing UGC activity as well as intrusive age verification merely by virtue of the number of users is required,  imposing unreasonable burdens on their providers as well as new harm risk vectors on their users. Services that are large services, of medium or high risk of one or more specified harms, and with at least one specified functionality, to offer enhanced user control measures. At least one of these required controls is completely unworkable in the case of Wikipedia\\nApproach to the Codes\\t\\tDisagrees with ofcom\\'s proposed costs definition\\tWe are concerned that the proposed costs defined in 11.30(c) do not factor in opportunity costs. As the Foundation has outlined to Ofcom before, in the context of a nonprofit platform, the failure to adequately consider these opportunity costs can pose severe human rights risks to users and staff. The Foundation therefore remains unpersuaded by the reasoning put forward in 23.31 as potentially applied to public interest platforms. Content moderation (User to User) \\t\\tDisagrees with the proposals\\tMirroring the argument put ofrward in Question 1, vol 2, The proposal does not take into account public interest platforms that host educational content, freely available to everyone, which is created and curated by self-governing communities. It is important for the proposal to do so because the content in question is based on publicly available policies that these communities transparently enforce themselves. Automated content moderation (User to User)\\t\\tFavours consistent CSAM hash matching, but unconvincned by proposals\\t\"The Foundation favours consistent CSAM hash matching practices against established industry corpuses in all public UGC platform contexts. This practice works for an organisation like ours, which is perhaps unique in its wide reach-to-resourcing ratio as a nonprofit as well as particularly low risk profile (see the 2023 OECD report on the subject).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9791736602783203,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'While we remain unpersuaded that the proposals are practically feasible in the context of encrypted services without disproportionate negative rights impact imposed on the public, we want to be clear: Any requirements around keeping internet users safe from harm should protect end-to-end encrypted (E2EE) communications. These requirements should refrain from discouraging or prohibiting the use of E2EE communications, or from de-incentivizing platforms and other service providers from offering them to safeguard the privacy and safety of their users. \"\\nAutomated content moderation (User to User) \\tPublic/Private\\tThe distinction between public/private is inapplicable to the platform\\tThe conceptual distinction made in Annex 9 appears to be generally inapplicable in the context of the Foundation’s platforms and services because of the transparent and public interest nature of the Wikimedia projects and platform architecture—where content published by users on the platforms is necessarily accessible both publicly and globally. Automated content moderation (User to User)\\t\\tStronger alignment with industry standard established by EU terrorism legislation\\tThe Foundation strongly favours alignment with the industry standard established by the relevant EU terrorism-related legislation. Fragmentation of regulation in this place makes enforcement less effective and less likely that practicable implementation will be consistent across both for-profit and nonprofit providers\\nUser reporting and complaints (U2U and Search)\\t\\tThe guidance is unworkable in current form, recommends the DSA approach. The Foundation is concerned that the guidance at 16.124 requires urgent attention, since—read in the light of paragraphs 12.80(a), 16.21, and 16.22—it is not workable. We worry that Ofcom’s commitment to proportionality and international regulatory compatibility would be seriously undermined, and respectfully refer Ofcom to the EU DSA and other international laws, which show a way forward here. In addition, the process would be cost prohibitive.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9930539727210999,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They propose following the DSA approach. Terms of service and Publicly Available Statements\\t\\tThe proposals do not reflect the practives of public interest platforms\\tThe Foundation remains concerned that the basic design vision underpinning the proposals excludes the reality of public interest platforms such as ours, which do not unilaterally set policy for their platform, but co-create foundational policies such as the Terms of Use (ToU) and Privacy Policy with the self-governing volunteer communities. This is done in transparent consultations that are open to anyone belonging to these communities. It is these self-governing communities who develop, implement, and operate local policies in pursuit of their specific educational aims and to complement the broad global framework agreed upon. Default settings and user support (U2U)\\t\\tDisagree with proposals due to negative impact educational content and children who use platform\\tThe Foundation remains deeply concerned about the implied assumption underpinning the proposals: The requirement to build a surveillance infrastructure as outlined in responses 9) and 12) would mandate that public interest platforms such as the Foundation obtain user data they are not currently tracking. Given that the Terms of Use mandate hosting freely available educational content, and also that the vast majority of all visitors (i.e., passive readers) to our platform never create an account, it is not evident whom such settings would benefit or which kinds of risk it would mitigate in the context of Wikimedia projects. These proposals would require Wikimedia to collect and store more data on children who contribute to or access content made available by free and open knowledge projects like Wikipedia. This would diminish children’s privacy on our projects and could diminish their ability to exercise their right to seek, receive, and impart information online, among other human rights. Enhanced user control (U2U)\\t\\tOppose the proposals because they fail to account for the platforms approach to content vreation\\tThe Foundation respectfully and strongly opposes the proposals, since they do not take into account the collaborative approach to content creation and curation that the self-governing volunteer communities have successfully employed for more than two decades. Ofcom’s proposals are tailored to social media settings, in which they might well be both appropriate and proportionate, but do not take into account the back-and-forths that take place publicly on Wikipedia “Talk” pages. Wikipedia’s “Talk” pages are effectively self-regulated volunteer environments. Debates there often take weeks to discuss and agree on the specific language to be included into the related Wikipedia articles serving the public. Such discussions cannot successfully occur if, for example, malign actors aiming to undermine the integrity of these self-governing editorial processes can prevent contributors from scrutinising and developing arguments and counter-arguments to proposals\\nEnhanced user control (U2U)\\t\\tOpposes proposals due to the risk to the platforms collaborative knowledge environment\\tThe Foundation strongly opposes these proposals due to the significant risk they pose to the collaborative knowledge production processes playing out on Wikipedia “Talk” pages and comparable, effectively self-regulated collaborative environments. User access to services (U2U)\\t\\tDisagrees\\tRefers to response given in Q 22 and 26\\nUser access to services (U2U)\\t\\tArgues that users can be blocked but public interest platform has better mitigations through platform architecture\\tA user can be blocked based on all three factors identified in question 41). However, none of these blocks ultimately prevent a determined malign actor from returning to any platform, be it a regularly constructed for-profit service of any kind or a radically transparent public interest such as our own, which hosts freely accessible educational content. The most effective manner to limit concerning activities is to construct platform architecture that limits incentives such as a publicly transparent platform\\nUser access to services (U2U)\\t\\tCannot take a stance on this position \\tAs outlined in answer 14), even very large public interest platforms have only a small number of active content contributors. As the OECD noted in its 2023 “Transparency Reporting on child sexual exploitation and abuse online” report, the National Center for Missing & Exploited Children (NCMEC) registered 29 reports by the Foundation in 2022 and eight the year prior. Final human adjudication of automated systems outputs were in line with appropriate rights impact assessments, and reporting to NCMEC was proportionate for such contexts. The Foundation opposes the line of reasoning outlined in 21.7 and 21.83 if applied to low-risk public interest platforms—consistent with our answers to 9), 13), and 41). The Foundation acknowledges that large higher risk business model providers—such as social media companies and their huge volumes of concerning content (noted by the OECD in the same report)—cannot practically rely on such double-evaluation standards. However, our organisation has no first-hand experience with what is practically feasible for providers facing challenges of such magnitude and, hence, is in no position to take a stance on this matter. Governance and Accountability\\t\\tOn independent assurance, DTSP promotes its own framework as best practice\\tDTSP highlights the linkages between its best practice framework and the proposals around independent assurance and accountability. Approach to the Codes\\t\\tWhile in agreement the methodology risks burdening smaller companies based on the risks facing them\\tAlthough DTSP agrees with the principle of proportionality, and that more onerous measures should be tiered in their application, in our view the Ofcom methodology risks overburdening smaller companies based on assumptions about the risks facing certain types of services. Attention to product design and other mitigating details could help redress this deficiency. Approach to the Codes\\t\\tArgues for common understanding of key terminiology across organisations\\t\"It is important to have a common understanding of key terminology across organizations responsible for trust and safety online, including industry actors as well as partners in government and civil society. To this end, DTSP released an inaugural edition of its Trust & Safety Glossary of  This is the first industry effort by technology companies, representing various products, sizes, and business models, to develop a common trust and safety lexicon. The Trust & Safety Glossary of Terms consists of more than 100 terms across four categories:\\n● content concepts and policies;\\n● common types of abuse;\\n● enforcement practices; and\\n● trust and safety technology.\"\\nGovernance and accountability\\t\\t\"Meta broadly supports this proposal and recognises the importance of building a culture that prioritises safety (as noted in para 8.48 of Volume 3) and of the importance of having engaged\\nsenior management oversight of risk.\"\\tMeta broadly supports \\'Name a person accountable to the service’s most senior governance body for compliance with illegal content duties and reporting and complaints duties.\\' [They go on to explain what they\\'ve set up to propose this and explaining they go further.]\\nGovernance and accountability\\t\\tAgrees with proposal but asks for more flexibility and confidentiality\\t\"Senior managers names and statements of responsibilities created: Meta is supportive of the proposal to require written statements of responsibilities for certain senior members of staff. However, as a large, multi-faceted international organisation and in recognition of the multidisciplinary nature of online harms, the final proposal should maintain sufficient flexibility to adapt to the reality of a variety of organisational designs that may change over time.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9948111772537231,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Organisational structures do not always remain static and responsibility may be shared across multiple individuals and teams.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9964576363563538,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We would therefore wish for the proposal to remain sufficiently flexible to allow for this and guard against it becoming more prescriptive. Further, given the sensitive nature of many online risks which we manage, we would be keen for Ofcom to be very clear on retaining the confidentiality of the statements of responsibilities and of the related names. It should also be noted that the implementation and maintenance of such statements at a global, matrixed company such as Meta will likely require significant additional investment in human resource structures and processes. The estimate of an average time investment to create a statement of ‘a few days’ (para A8.74) is in our view too short\"\\nGovernance and accountability\\t\\tDisagree with time and costs of implementing the measure\\t\"Senior managers names and statements of responsibilities created: \\nIt should also be noted that the implementation and maintenance of such statements at a global, matrixed company such as Meta will likely require significant additional investment in human resource structures and processes. The estimate of an average time investment to create a statement of ‘a few days’ (para A8.74) is in our view too short\"\\nGovernance and accountability\\t\\tExplain how they\\'re continuing to tracj evidence of new illegal harms\\tWe explained aspects of our approach to identifying emerging risks in our response to the 2022 Illegal Harms Call for Evidence (as referenced in Volume 3, para 8.117) , and our risk intelligence team continues with its work on reviewing escalations across internal teams. We intend to continue to leverage our internal processes to meet this requirement.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9967561364173889,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As stated above in response to Question 3, IGRC will also be involved in identifying risks that may emerge and ensuring these are reported internally. Governance and accountability\\t\\tAsk for discretion on how to draw up the Code of Conduct\\tA Code of Conduct has the benefit of providing clarity to relevant Meta staff of their duties and obligations under UK law, and this is to be welcomed. We would advocate for discretion on how to draw up the Code to maintain flexibility and ensure we can align its contents with existing internal and external Codes of Conduct with which Meta complies. Governance and accountability\\t\\tSupport the requirement of training and ask for discretion as to how it\\'s done\\tMeta recognises that effective training is an important part of a compliance framework and already delivers extensive staff training across our business. We support the proposal that relevant staff involved in the design and operational management of the service receive training on the service’s approach to compliance with the illegal content safety duties and the reporting and complaints duties, but, as with the Code of Conduct, would advocate for discretion and flexibility on the form and manner in which this training is delivered\\nGovernance and accountability\\t\\tAdvocate for a proposal that is flexible when it comes to the annual review of risk management activities\\tAnnual review of risk management activites: This proposal is complementary to Meta’s commitments under the EU’s DSA and is supported. Meta supports and encourages Ofcom’s aims at harmonisation with other regulatory regimes around online harms. However, we would like the opportunity to comment on this further should Ofcom put forward its own review template. We would advocate for a proposal that is flexible and affords providers with sufficient discretion as to the design and operation of such a review. Governance and accountability\\t\\tSupport proposal about the internal monitoring and assurance function\\tHave an internal monitoring and assurance function to provide independent assurance that measures taken to mitigate and manage the risk of harm to individuals identified in the risk assessment are effective on an on-going basis, reporting to an overall governance body or audit committee. We support this proposal. See our detailed comments earlier in this response on how, in anticipation of developing global regulatory regimes around online content, a three lines of defence model of risk management has been established at Meta, including a new Integrity Governance, Risk & Compliance Function (IGRC). As part of our three lines of defence model, we also have an internal audit function which provides comprehensive and independent assurance on the effectiveness of governance, risk management, and internal control activities\\nGovernance and accountability\\t\\tBroadly support segmentation of governance measures, but refer to question 12 about lack of clarity of what defines a service\\tWe broadly support these proposals. However, please also see below our response to Question 12, in which we explain our concerns regarding the lack of clarity with regards to what constitutes a “service” and where the boundaries of a service should lie. In our view, it is not entirely clear from the OSA and the consultation documents where the boundaries of a service are drawn. Governance and accountability\\t\\tDisagrees with potential future measure of an independent third-party\\t\"This does not appear to be a proportionate requirement. The OSA already provides adequate levers in the regulatory toolkit to enable Ofcom to get the information it requires to further its objectives in ensuring online safety for UK users, namely:\\n● the ability to issue an audit notice (OSA Sch 12 para. 4) and related powers in that\\nparagraph;\\n● the ability to instruct a skilled person to produce a report;\\n● other information gathering powers, which may in practice be used to obtain information pertaining to Meta’s compliance with its risk assessment and safety duties. In many cases, providers of regulated services will have existing internal processes which would negate the need for an additional independent audit requirement, as follows:\\n● Meta already carries out effective internal audits;\\n● Meta has established an Integrity Governance, Risk and Compliance function that will\\nperform a continuous cycle of monitoring, testing and improvement work, producing\\nrelevant materials that can be provided, as appropriate, to Ofcom during the supervisory\\nrelationship. Lastly, the EU’s DSA already requires Meta to conduct an annual independent audit and to\\nproduce an audit implementation report. This report will be made public and will therefore be available to Ofcom. [gives further detail on the DSA audit report]. Importantly, a potential cost to Meta of such a measure is the opportunity cost associated with removing trust and safety experts from front line risk mitigation work, to manage and respond to an independent auditor. We are seeing from the DSA independent audit process that is currently underway that the work involved is very significant. As such, our view is that the costs of this measure would outweigh the benefits, and that it would be at odds with the principle of proportionality that underpins the OSA’s requirements for providers.\"\\nGovernance and accountability  \\t\\tDo not support the measure to tie remuneration for senior manager to positive OS outcomes\\tWe recognise that remuneration and incentives are powerful drivers of compliant behaviour, but financial remuneration is not tied to performance in the tech sector in the same way as it is in financial services, for example. Establishing a causal link between a single decision and a suboptimal outcome will be very difficult at Meta, given our scale and complexity. The nature of online safety risks and the process of their management is wholly different, more nuanced and more complex as compared to risks seen in the financial services sector. [provides examples of how it is different, and how U2U services can already be incentivised to manage wrongdoing because of its affects on other revenue streams]. Moreover, the suggested approach does not accord with the principles of intermediary liability that have been in place since at least 2002, when the e-Commerce Directive was implemented into UK law by the Electronic Commerce (EC Directive) Regulations 2002, and which remain in force following Brexit. The preponderance of illegal harms is determined in significant part by external events, over which senior managers and intermediary service providers have no control. Meta monitors these events and has systems in place to respond accordingly. Meta is continually working to update its controls in response to the evolving threat landscape, including in monitoring evolutionary behaviours by bad actors regarding the misuse of our services. While our risk management practices are intended to identify and mitigate such risks on an ongoing basis, it is not always possible to prevent such misuse in real time, and as such, tying remuneration to online safety outcomes that may be determined in significant part by external events is not reasonable or proportionate. IIn short, whereas in the financial sector failures (and therefore remuneration) are linked to negative impacts directly caused by employees, in tech the harms are caused by users and not employees. This degree of separation from the employee to the cause of the harm makes linking remuneration in the way suggested unfair, disproportionate and illogical. Governance and accountability\\t\\tStrongly support capturing existing good practice, and acknowledgement of the advantages of aligning with other regulators. Support that CoPs be based on proportionality\\t\"We strongly support:\\n● Ofcom’s aim to capture existing good practice within industry, set clear expectations, and work to raise standards of user protection over time, especially for services whose existing systems are nascent or under-developed. Platform integrity is as important as it is\\nchallenging in practice. We constantly invest in existing and new technologies/tools and\\nprocesses to reinforce the integrity of our services and with years of expertise in this\\ndomain, we want to share our experience and learnings to level the playing field in this\\ndomain. ● Ofcom’s acknowledgement of the advantages and aim of alignment with other content\\nregulations, such as the DSA. This aligns the UK with global industry standards and best\\npractices framework, e.g. similar efforts in the EU and Australia. ● Ofcom’s commitment that the CoPs have to be based on proportionality and that\\nexpectations for services need to be clear\"\\nApproach to the Codes\\t\\tSpeak of problematic divergences from other regulations\\tWe noticed some potentially problematic divergences from other regulations. In our view, it is essential to develop regulatory models which are workable within the full spectrum of other, relevant regulations for online services (such as privacy regulation) and the global regulatory environment, in order to avoid fragmentation of the technological landscape. As such, CoPs should avoid setting up dual regulatory regimes. We set out more details on this in our responses to Questions 13 to 47 below. Approach to the Codes\\t\\tPropose a number of points to \\'help support proportionality\\'; first is to clarify on how boundaries of a service will be drawn\\t\"a) Clarity on boundaries of a service - and in case such guidance is not forthcoming, will be developed at a future date or those boundaries are drawn broadly - flexibility should be built in as to what proposed measures are required for different parts of such services that have different risk classifications. In our view, it is not clear from the Act or the consultation where the boundaries of a service are intended to lie. As an example, it is not clear whether Facebook (as accessed via, e.g., the Facebook App) is intended to constitute a single service or whether, e.g., “Facebook Dating” and “Facebook Marketplace” - which are accessible within the Facebook App but provide different service offerings and have different risk profiles - would amount to distinct, individual services. [META PROVIDES THE BENEFITS OF EACH INTERPRETATION]  We propose that Ofcom provides clarity on how the boundaries of a service will be drawn and, where that results in multiple products/parts of the service with different risk profiles being grouped into a single service, allowing the different parts of the service to be subject to proposed measures in line with their own size / risk level. \"\\nApproach to the Codes\\t\\tPropose a number of points to \\'help support proportionality\\'; second is that \\'more options to constitute a \\'safe harbour\\'\\tCompliance with measures recommended in the CoPs will effectively give providers a ‘safe harbour’ regarding their compliance with the Act. However, most providers will already have a range of safety and integrity measures in place, which will vary depending on the design and underlying integrity systems of the service. Against that backdrop, we propose that Ofcom, rather than providing a single set of recommended measures that will constitute a safe harbour, suggests or allows for a range of options which would meet the objectives of the Act, to reflect the flexibility and variation among services. We are aware that the Act permits providers to deviate from Ofcom’s recommended measures, but providers that do so lose their ‘safe harbour’ and bear responsibility for proving their alternative measures are equally effective. If Ofcom were to provide greater flexibility in its recommended measures, this would make it easier for providers to build on their existing online safety measures, rather than feeling that they need to spend resources on replacing an established and effective safety framework with the CoP-recommended version in order to benefit from the safe harbour. It would encourage an ongoing flexibility of approach to user safety, which is essential in an environment of constant technical evolution. Approach to the Codes\\t\\tPropose a number of points to \\'help support proportionality\\'; third is clarity as to how the ‘safe harbours’ available under the CoPs will apply to the relevant duties of the OSA\\t\"We note that s.49 OSA gives providers ‘safe harbours’, in that it states that providers will be treated as complying with a relevant duty if they take the measures recommended in a CoP for the purpose of compliance with that duty. We also note the index of recommended measures at pp.6-10 of Annex 7, which includes a column indicating which duty or duties a particular measure relates to. While this is a helpful reference, there are aspects of the index that we think could\\nbe clarified. [GOES THROUGH A NUMBER OF DETAILS ASKING FOR CLARIFICATION]\"\\nApproach to the Codes\\t\\tE2EE\\tProposed measures and protections in case of E2EE remain a concern, please refer especially to the responses to Questions 1, 12, 16, 18 and 20. Approach to the Codes\\t\\t\"The draft CoPs make a flawed assumption by focusing at times only on size as a proxy\\nfor harm.\"\\t\"The Act tasks Ofcom when determining the proportionality of various safety steps to focus on the factors ‘size and capacity’ of the service “and” the service’s own risk assessment (s. 10(10) OSA). While some of the measures in the draft CoPs apply at a proposed threshold of both risk level and size, others apply only at a proposed threshold of size and for others risk still remains a determining factor. The draft CoPs equate high reach with high risk, implying that services with the largest user base are higher-risk or that measures will have greater impact by means of higher reach. Using size as a proxy for harm is a simplistic assumption as harm varies between services and is not necessarily related to the size of the service. Frequently, potentially hateful or dangerous narratives that emerge on our platforms were first developed and spread on smaller services or those with less sophisticated integrity measures. In addition, larger services are also subject to more frequent and in-depth scrutiny by their users, the media, regulators and other stakeholders. This scrutiny, and the associated reputational exposure, gives larger services a further incentive to be at the forefront of tackling online safety risks, in addition to the core goals of protecting user safety and the integrity of their services. We also note the tacit responsibility of more mature companies to support the development of the whole ecosystem - e.g., by participating in industry forums, sharing best practices and risk mitigation tools, and taking other such steps to help less mature companies to improve their internal systems. Ofcom should consider a more equitable, risk-based approach that addresses risks where they appear.\"\\nApproach to the Codes\\t\\tApplication of measures to \\'large services\\' also dependent on how services are defined + refer to category 1 services\\t\"We also refer to our concern regarding the lack of clarity as to where the boundaries of a\\nservice are drawn, as expressed in our response to Question 12 above. If the boundaries of a service are to be drawn broadly, Ofcom’s recommended measures should allow for flexibility as to the measures that should be implemented on different parts of a service which have different risk classifications. We note that the same concerns will also apply when considering how to define and draw the boundaries of Category 1 services, which we understand will be addressed in another consultation phase.\"\\nApproach to the Codes\\t\\tThink the equating large service with high risk is flawed, but agree with some boundaries of our current definition\\t\"Please see our response to Question 13 on how, in our view, equating high reach with high risk is flawed. However, if a threshold is to be set for ‘large’ services:\\n● We agree that it should be aligned with thresholds in other content regulations, such as\\nthe DSA, i.e. average monthly active users equating to approximately 10 percent of the\\nrelevant population. ● Thresholds and methodology for calculations should be clearly defined. We refer to our\\nconcern set out in our response to Question 12 on the lack of clarity on where the boundaries of a service are to be drawn.\"\\nApproach to the Codes\\t\\tThink further proportionality should be considered in the definition of multi-risk\\t\"We understand Ofcom defines a “multi-risk” service as a service which is high or medium risk for at least two kinds of illegal harm, based on the provider’s most recent risk assessment. Based on this definition, it follows that a service which is high or medium risk for two kinds of illegal harm will be subject to the same stringent measures as a service that is high or medium risk for all 15 kinds of illegal harm. While we appreciate that it is difficult to define where to draw the line for ‘enhanced’ measures further proportionality could be considered. We understand Ofcom defines a “multi-risk” service as a service which is high or medium risk for at least two kinds of illegal harm, based on the provider’s most recent risk assessment. Based on this definition, it follows that a service which is high or medium risk for two kinds of illegal harm will be subject to the same stringent measures as a service that is high or medium risk for all 15 kinds of illegal harm. While we appreciate that it is difficult to define where to draw the line for ‘enhanced’ measures further proportionality could be considered.\"\\nApproach to the Codes\\t\\tACM and E2EE\\tIn addition, we share the below specific remark on private messaging: We welcome Ofcom\\'s clarification that the automated content moderation proposals it brought forward in the consultation are limited to content communicated publicly, where it is technically feasible to implement them, and that they will not apply to private communications or end-to-end encrypted communications (‘e2ee’). We consider this appropriate given the potential for some of these proposals to compromise users’ ability to trust in the privacy and security of their private messages - as well as the regulatory landscape applying to private messaging services which we expand upon in our answer to Question 21\\nApproach to the Codes\\t\\tCosts may be higher\\tWe appreciate the difficulties in estimating costs. As a general remark, we note such may vary from service to service, e.g.': [{'label': 'POSITIVE',\n",
       "               'score': 0.997027575969696,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'may be higher than set out in Annex 14. Content moderation (User to User) \\t\\tAppreciates the two options provided\\tAll services: Having a content moderation function that allows for the swift take down of illegal content (ref 4A). Meta appreciates Ofcom offering two options as a proposed measure. We refer to our answer to the related Question 49 below for details. Content moderation (User to User) \\t\\tSupport the measure\\t\"All ‘multi-risk’ or large U2U services: Setting internal content policies (ref 4B)\\nWe support Ofcom’s recommendation that the provider should set and record internal content policies setting out rules, standards and guidelines on what content is permitted on the service and what is not; and how policies should be operationalised and enforced. As regards the policy approach to illegal content, please see our comments on this point in our response to Question 49 below.\"\\nContent moderation (User to User) \\t\\tAgainst the inclusion of this requirement in the draft COP - more details in the response itself\\t\"All ‘multi-risk’ or large U2U services: Performance targets (ref 4C)\\n[Meta provides three detailed reasons as to why they do not want to include it. See response for details but in summary:]\\na) The OSA does not prescribe specific turnaround times for the removal of illegal content and instead provides a duty to operate a service using proportionate systems and processes designed to (a) “minimise” the length of time for which any priority illegal content is present; and (b) where the provider is alerted by a person to the presence of any illegal content, or becomes aware of it in any other way, “swiftly” take down such content. b) Moreover, it is unclear how accuracy in decision making is to be defined and assessed.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9925290942192078,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Forinstance, would a decision be considered accurate if it’s not appealed? Would it be considered accurate if it’s appealed and the content is restored? These are complex decision making processes in areas where there are a wide variety of different applications and outcomes. c) In contrast, taking an approach that does not involve setting prescriptive internal performance or accuracy targets, but that rather emphasises the general requirement for content to be reviewed swiftly, aligns with other regulations, such as the DSA. To this end, we strongly advise against including this requirement in the draft CoP. To the extent Ofcom decides to maintain the internal target proposals, we suggest further guidance by Ofcom particularly with regard to how such targets should operate as against other signals guiding prioritisation in content moderation system\"\\nContent moderation (User to User) \\t\\tHave significant concerns around the practical implications as to the details of \\'virality\\' and \\'severity\\', the remainder of Ofcom’s proposed factors, the interplay of the factors, applying these factors to review of content for illegality, and conflicts with other proposed measures. \"All ‘multi-risk’ or large U2U services: Prioritisation (ref 4D)\\nFor review whether content violates provider policies we support the criteria of “virality” and “severity” in application as above, but, we also recognise there are significant and concerning practical implications as to the details of these factors, the remainder of Ofcom’s proposed factors, the interplay of the factors, applying these factors to review of content for illegality, and conflicts with other proposed measures. [gives more in-depth details, but summary is:]\\nGiven that we would in the first instance be assessing content for violations of our policies, not for illegality, it would not be practical to prioritise on the basis of illegality and the results of our illegal content risk assessment. We suggest that the measure instead gives providers the flexibility to assess severity in a way that works for their moderation process which often operate most effectively at a global scale (as the underlying systems can continually evolve and be trained on a larger set of content and data). It would not be practical for us to take into account the likelihood of content being illegal, for the same reasons set out above. We suggest that the measure instead gives providers the flexibility to prioritise on other grounds aside from illegality - for example, the likelihood that content violates their policies. In any event, where reports from trusted flaggers are concerned, in most cases those reports are processed via separate, dedicated channels that don’t operate in tandem with content moderation systems. [Give further details of concerns around conflicting parameters in our measures, and concerns around the conflict with PoC]We therefore suggest that providers are granted flexibility to prioritise in a way that is appropriate for their service and in light of a wider range of relevant factors, and that any more prescriptive requirements are introduced at a later point, once all relevant consultations have been completed\"\\nContent moderation (User to User) \\t\\tStress the difficulty in applying the resourcing measure. \"All ‘multi-risk’ and large U2U services: Resourcing (ref 4E)\\n[say to refer to responses to 4B and 4C above around internal content policies and performance targets] + \\nIn addition, we stress that the implementation of this measure in practice needs to be\\nproportionate. For example: it is reasonable for providers to have appropriate backup strategies for unexpected surges in report volume, but even with such strategies in place, not every eventuality can be planned for, and some situations may require additional ad hoc measures that will take time to implement. Also, while we agree with the goal to have appropriate resourcing in place and cater for particular needs of a UK user base, we also stress that the correct staffing of language support worldwide is a complex planning exercise; even on a per region level depending on the amount of languages spoken in the region. [Meta sets out some strategies they have found useful, which Ofcom may wish to consider reflecting in the measures. Goes into more detail in the document, but in summary: using a mix of reviewers + allowing for triaging where more or less language support is needed]\"\\nContent moderation (User to User) \\t\\tAgree although ask for more flexibility in what the training covers - not necessarily the kinds of illegal harm we set out\\t\"All ‘multi-risk’ or large U2U services: Provision of training and materials to moderators (ref 4F)\\nWe support Ofcom’s recommendation to ensure moderators are appropriately equipped. To that end we support appropriate measures, such as distinct training based on the nature of the particular moderator’s work. We refer to our responses above regarding measures 4A and 4B, which are referred to in measure 4F. In addition, we stress that the implementation of this measure in practice should be proportionate to achieve its purpose. As outlined in our response (see our responses to Questions 8 and 49), the content prohibited by providers’ policies is not necessarily identical to the types of illegal content covered by the OSA. Given this, where content moderators work on reviewing content for violation of the provider’s policies, they should receive training on such policies. This may cover content that to some extent overlaps with particular kinds of illegal harm, but will not necessarily cover all forms of illegal harm. Conversely, content moderators who review content to determine whether it constitutes a specific kind of illegal harm should receive training on such harm. For the latter, providers should have flexibility to implement illegal harm-specific training in a way that is appropriate for their service and reflective of the particular risks of illegal harm that apply to their service. [Meta sets out some strategies they have found useful, which Ofcom may wish to consider reflecting in the measures. Goes into more detail in the response, but in summary: building review teams for policy violations that include a diverse range of backgrounds + human reviewers who review content alleged to be illegal receive distinct training based on the particular nature of their work.\"\\nUser reporting and complaints (U2U and search) \\t\\tSupport measure that services could oprtate a combined reporting and complaints function\\t\"We support Ofcom’s view that services could operate a combined reporting and\\ncomplaints function for most users and most types of complaints, provided that there is at\\nleast one other means for users to communicate these issues to the service, so that\\ncomplaints can be made about issues with the reporting function itself (Volume 4, paras\\n16.15-16.16).\"\\nUser reporting and complaints (U2U and search) \\t\\tRefer to their answers to questions 18 above as to inernal targets and turnaround times\\t\"The measures set out at 4A to 4F in Annex 7, which we address in our response to\\nQuestion 18 above, include measures relating to the handling of user reports and\\ncomplaints. The comments we set out in relation to those measures are also relevant here - for example, our comments above in relation to proposed internal targets and\\nturnaround times apply similarly to the measures recommended at 5C, 5D and 5E, as\\nthese relate to timelines and the taking of appropriate action in response to complaints.\"\\nUser reporting and complaints (U2U and search) \\t\\tIn relation to 5B - Agree that reporting functions should be clearly accessible, but there is a practical question about how this would work in relation to “affected persons” - ie. People who are not necessarily users\\t\"The OSA requires affected persons to be able to make reports and complaints, per\\nsections 20 and 21.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992364764213562,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'In s. 10(5) OSA, an affected person is defined as covering persons\\n“other than a user of the service in question” who fall within certain categories - i.e.,\\naffected persons are not users. We note that providers do not necessarily allow non-users access to all content on their\\nservices [Meta provide examples and reasons]. As such, affected persons will not necessarily be able to see an item of content that they may wish to report or complain about, and so may not be able to access a reporting / complaint tool directly from that content. We therefore strongly suggest that ‘accessible’ functions and tools for reporting / complaints are not limited to those accessible directly from the relevant item of content, but are deemed to include other functions and tools accessible to non-users - for example, reporting by way of a Help Center form.\"\\nUser reporting and complaints (U2U and search) \\t\\tIn relation to 5B - asking for flexibility as to the number of steps as a service may be accessed differently on different devices. \"We also note that other elements of measure 5B, e.g. the recommendation that “the\\nnumber of steps necessary (such as the number of clicks or navigation points) to submit (i) a relevant complaint using the reporting function or tool; and (ii) any other kind of\\nrelevant complaint are as few as is reasonably practicable”, will need to be interpreted\\nflexibly to account for differences between services and different ways services can be\\naccessed. For example, the space available on the user surface of a mobile phone app is\\nlikely to be more limited than the user surface of the same service accessed via a desktop\\nbrowser, and users may be used to finding reporting options in different places depending on how they access the service. It would therefore be important for providers to have flexibility in how they meet this recommendation, to account for differences between services and between different methods of access to a service.\"\\nUser reporting and complaints (U2U and search) \\t\\tIn relation to 5C - outline two challenges [first challenge here, second in next row]. First challenge around spam report. Firstly, service providers regularly encounter users or non-users who abuse the reporting system by “spamming” high volumes of reports with no merit or submitting coordinated bot reports, and act to the detriment of those genuinely reporting harmful or illegal content. We suggest that measure 5C should include an exception for reports identified as spam, to reduce the resource impact for providers of dealing with such reports and allow more resources to be dedicated to dealing with legitimate reports. There are objective criteria that may be used to identify spam reports, which we are happy to discuss with Ofcom separately (we have not set these criteria out in this public response, in order to prevent misuse by bad actors). Secondly, as regards the proposal for indicative time frames for sharing a response, we note that it is likely to be difficult to provide even an indicative timeframe for responding to reports / complaints, as the time needed for a proper assessment and response will be highly variable and is often difficult to identify before having reviewed the report \\nUser reporting and complaints (U2U and search) \\t\\tIn relation to 5C - outline two challenges, first in row above. Second is about providing timeframes\\t\"Secondly, as regards the proposal for indicative time frames for sharing a response, we\\nnote that it is likely to be difficult to provide even an indicative timeframe for responding\\nto reports / complaints, as the time needed for a proper assessment and response will be\\nhighly variable and is often difficult to identify before having reviewed the report / complaint. For example, the prioritisation and timeline of review for various types of reports cannot happen instantaneously as it depends on multiple factors within the content moderation system including how other content is to be prioritised against other reported content in a prioritisation “queue” or system. While we appreciate Ofcom’s clarification in Volume 4 para. 16.101 that these timeframes are not binding deadlines, we are concerned that providing users with timeframes on submission of their report / complaint, which timeframes may not subsequently be met, will lead to more frustration for users and incentivise providers to respond faster (at the\\nexpense of appropriate prioritisation and proper assessment of the report / complaint) in\\norder to stay close to the indicated timeframe. See our comments in response to Question 18 regarding internal performance targets, which apply similarly here. In addition, an indicative time frame does not align with the proposed measure 4D for\\nprioritisation. Prioritisation is an ongoing process, the response time for a report would\\nthen vary depending on what reports/complaints come in, e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9913191199302673,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'an initial estimate may be\\noverhauled a few minutes later when reports with higher priority or, for example, when\\nparticular spikes occur during certain high profile events or incidents come in and which\\nmay be hard to plan for. We also note, with reference to Ofcom’s aim of harmonising with other content regulations, that the DSA does not include an equivalent requirement of indicative time\\nframes.\"\\nUser reporting and complaints (U2U and search) \\t\\tIn relation to 5I - question around if a service has a DRC for all illegal content, whether setting up for just fraud is proportionate/required\\tproviders may already have dedicated reporting channels for trusted flaggers, which enable a range of reports, including fraud-related reports. For example, Meta has existing, dedicated reporting channels where government agencies and non-governmental organisations can report harmful or illegal content on Facebook or Instagram that may violate our terms and policies or that they consider to be in breach of local law, including fraud. These reporting channels are distinct from standard in-product reporting (which is open to all users), can only be used by the onboarded organisations, and are staffed by escalations specialists who triage reports and route them to expert teams for expedited analysis, including more in-depth investigations where appropriate. Some of the recommended trusted flaggers by Ofcom are already onboarded to our dedicated channels and use this process. Where such channels already exist, we would question the need for providers to set up a second and separate channel dedicated to fraud reports. This is all the more relevant given that when content is reported to our teams via dedicated channels for trusted flaggers, content is not merely reviewed against one specific policy area but across all potentially applicable terms/policy violation types. Such an approach would be hampered by the existence of a separate, distinct channel. User reporting and complaints (U2U and search) \\t\\tIn relation to 5I - question around engagement with all organisations\\t we note that where a provider has a reporting channel for a wider range of trusted flaggers than those listed in the measure, it may not be practical for the provider to commit to engaging with all of those organisations to understand their needs with respect to the channel. While we appreciate that Ofcom’s recommended measure only refers to such engagement with reference to the seven trusted flaggers listed in the measure, we propose that providers should have the flexibility to limit such engagement to these organisations, rather than providers being required to engage with all organisations onboarded as trusted flaggers. This could be clarified in the wording of the measure. Terms of service and Publicly Available Statements\\t\\tFeel our requirements are too extensive, and argue they should remain short and clear\\tThese measures, which reflect the OSA’s terms of service (‘ToS’) provisions, require extensive information to be included in the ToS. Our experience suggests that providers should have the flexibility to put this information in separate documents or locations (which are incorporated into the ToS by reference) in order to improve readability and clarity for users. Meta takes great care to ensure that all of its communications with users, including its terms and conditions, and any updates or changes thereto, are set out in clear, plain, intelligible, user-friendly language for users of all ages. In this context, we believe that terms of service should remain short and clear. Default settings and user support (U2U)\\t\\t\"Propose that Ofcom provides additional examples of recommended measures that\\nwill give providers greater flexibility to adopt protections that work for their service while\\nremaining within the COPs ‘safe harbour’.\"\\t\"In general, we agree with the proposal that by default enabling stricter privacy protecting options improves the safety of teen users on services. However, different protective measures may be appropriate for different services, depending on the nature, risks and user base of the service, and we would advocate for providers to have flexibility in deciding which protective measures to adopt in order to account for these differences. Additional flexibility would also allow providers to implement protections in a way that takes account of their existing safety measures (which will be particularly important for providers who have already invested substantially in protections for child users) and will allow protections to evolve as necessary to reflect the fast pace of change in children’s experience online. We therefore propose that Ofcom provides additional examples of recommended measures that will give providers greater flexibility to adopt protections that work for their service while remaining within the COPs ‘safe harbour’. For context, and as examples of measures that providers may take to protect teen users of their services, we refer to these previous articles (Facebook private settings, Instagram private by default) and this very recent article on message settings for teens on Facebook and Instagram, which outline examples of default settings and other protective measures that we have implemented in the past, and most recently for teen users on Facebook and Instagram. In our experience, these measures are geared towards achieving the same goals targeted by Ofcom’s recommended measures (i.e., to reduce the risk that teen users might inadvertently be exposed to inappropriate interactions and content which could result in grooming). We think that the measures we have taken so far are proportionate to mitigate this risk for these services, but we continue to monitor this and adjust these measures to reflect developments on our platforms.\"\\nRecommender system testing (U2U) \\t\\t\"Suggest avoiding setting prescriptive requirements in the CoPs, especially if they apply only\\nto providers that are already testing their systems. This would have the opposite effect of that intended, by encouraging providers which did not invest in this space to continue to do so to avoid regulatory pressure.\"\\tWe would recommend that providers are given flexibility as to how they approach recommender systems development and testing, and - in particular - that providers are able to use different strategies to detect and address changes in prevalence in harmful content that may stem from ranking changes, without thereby falling outside the CoPs ‘safe harbour’ (rather than being limited to a launch-by-launch AB test approach if they wish to remain within the ‘safe harbour’). Cumulative Assessment\\t\\tDo not believe that high reach services are automatically more risky for users per se, and hence it would be disproportionate to impose obligations only on the basis of size. Refer to their feedback on Questions 12 to 16\\tAs discussed in our response to Question 13 the draft CoPs set out a ‘differentiated’ approach to apply the most onerous measures in the proposed CoPs only to services which are large and/or medium or high risk. As mentioned, while we believe there is merit in varying recommended measures to take account of the varying risks and capabilities of different services, we do not believe that high reach services are automatically more risky for users per se, and hence it would be disproportionate to impose obligations only on the basis of size. We refer to our feedback on Questions 12 to 16. Approach to the Codes\\t\\tOn E2EE being able to track illegal content\\t\"In assessing the risks of a service to implement and maintain proportionate mitigations, Ofcom ought to make explicit that encrypted services cannot and are not required to track evidence of illegal content on their services through the content itself, instead of the current expectation outlined in Volume 3: “[Proposal for all U2U services to} Track evidence of new kinds of illegal content on their services, and unusual increases in particular kinds of illegal content, and report this evidence through the relevant governance channels.”\\nWhile we can and do track evidence of particular harms using unencrypted parts of our service and through user reports (and put in place comprehensive mitigations to address these harms), it is not possible in an E2EE service to assess the content of messages (as above) or ‘take down’ specific content (as per the content moderation proposals in volume 4). Private messaging providers should not be required to conduct content moderation or monitoring; rather, they should design the service in ways that prevent harm and have appropriate mitigations in place to respond to bad actors.\"\\nContent moderation (User to User) \\t\\tFeel all conmod measures are at odds with E2EE services\\t\"As it stands, proposals for content moderation functions for all services in Volume 4 stand at odds with the nature of E2EE services. Ofcom’s consultation document suggests a service needs to: “Have systems or processes designed to swiftly take down illegal content of which it is aware.”\\nAs Ofcom will be aware, WhatsApp’s approach to safety is not based on content moderation but rather on preventing harm through product design, as outlined in our introduction to this response. We appreciate the numerous statements made by Ofcom that the regime will not focus on the presence of illegal content but rather on the systems and processes to identify and mitigate the harms arising from it. Ofcom should amend this proposal to clarify that E2EE private messaging services are not required to monitor and / or moderate the content of people’s private conversations and can still be compliant. Even when we are made aware of illegal content (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9895433187484741,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'through user reports), we cannot access users’ chats to “take down” violating content. This would not be in line with user expectations around the privacy of their personal communications.\"\\nAutomated content moderation (User to User) \\t\\tGive suggestions on CSAM and ask for more flexibiliy on fraud fuzzy word detection\\t\"Overall we agree with the recommendations made by Ofcom in Measure 4G in relation to the use of Perceptual Hashing techniques for detecting CSAM material at the time of upload as well as for retroactively detecting such content on the public surfaces of our platform and removing it. From our own experience and tailored to our integrity systems, we believe that the approach developed below is one of the most efficient solution to date:\\n● photo-matching technologies and machine learning to proactively detect, remove, and\\nreport child exploitative content on public surfaces of Facebook and Instagram, as well as\\naccounts that engage in potentially inappropriate interactions (e.g., “grooming”);\\n● targeted solutions focused on prevention, including in-app advice and safety notices to\\nhelp educate young people about interactions with people they meet online and remind\\nthem about available self-remediation tools such as blocking, reporting, muting, and\\nmore;\\n● technologies to find accounts that have shown potentially suspicious behaviour and stop them from interacting with young people’s accounts; and\\n● reporting tools to report content in violation of Meta’s policies against child exploitation on Facebook and Instagram. Regarding the proposal 4I to use keyword detection such as fuzzy keyword detection for fraud, we would suggest allowing for more flexible options in this space as fuzzy keyword detection can be useful in some contexts but has shown limitations over time such as excessive false positives.\"\\nAutomated content moderation (User to User) \\t\\tSpeaking of the measures within E2EE\\t\"With regards to detection for certain types of harm, private messaging services can and should utilise available information such as behavioural signals from metadata, content from public and/or unencrypted parts of online platforms, and content made available via user reports, to rigorously pursue detection of illegal content like CSAM and terrorism, including proactively wherever possible. As we have detailed in previous engagements with Ofcom, WhatsApp has a comprehensive approach to identifying and enforcing against certain harms without accessing message content. We appreciate Ofcom’s position that automated scanning of E2EE messages won’t be mandated, as these technologies are technically infeasible to implement in a way that is compatible with E2EE. We note and agree with those who point to the independent Rephrain consultation of the Government funded Safety Tech Challenge, which concluded that:\\n● The confidentiality of E2EE users’ communications cannot be guaranteed when all\\ncontent intended to be sent privately is monitored pre-encryption. ● There are highly concerning incentives to repurpose scanning technologies. ● Transparency, disputability and accountability proved to be problematic in most of the\\ntools. We would also welcome wider industry consultation with Ofcom on what proposals are\\n\"\"proportionate\"\" measures for a service provider to take in tackling illegal content in E2EE services.\"\\nAutomated content moderation (User to User) \\t\\t\"potential for conflict between what is considered by Ofcom to be content \"\"communicated publicly\"\" on the basis of the criteria laid down in Annex 9 - and the technology measures thereby proposed in the Illegal Harms Codes of Practice, with the wider regulatory landscape applying to the underlying service - particularly in the case of private messaging\\nservices\"\\tAs outlined in our response to Question 16 we welcome Ofcom\\'s clarification that the automated content moderation proposals it brought forward in the consultation are limited to content communicated publicly. We consider the guidance set out in Annex 9 a helpful first step, given the challenges providers are likely to encounter in assessing whether a piece of content is communicated publicly or privately. We appreciate Ofcom\\'s recognition of this. However, we note the potential for conflict between what is considered by Ofcom to be content \"communicated publicly\" on the basis of the criteria laid down in Annex 9 - and the technology measures thereby proposed in the Illegal Harms Codes of Practice, with the wider regulatory landscape applying to the underlying service - particularly in the case of private messaging services\\nAutomated content moderation (User to User) \\t\\tend-to-end encryption should also be considered\\tIn determining whether content is communicated ‘publicly’ or ‘privately’ for the purposes of the OSA [footnote], we agree that the labelling of a communication as private is not, on its own, determinative. However, end-to-end encryption should also be considered (alongside other factors). Automated content moderation (User to User) \\t\\tOn accuracy of perceptual hash matching\\t\"We agree with the recommendations made by Ofcom about having appropriate risk assessment processes and metrics to configure hash matching algorithms to avoid inaccurate enforcement due to incorrect matching. There are a range of measures that, in our experience, can contribute to the accuracy of perceptual hash matching - for example, having a process in place to ensure that content is reviewed by trained human experts before being added to the relevant hash databases, and periodically having such experts review a sample of detections and take-down decisions to measure the effectiveness of particular hashing algorithms, can help to reduce the frequency of false positives. Going further, Ofcom’s observation regarding use of hashing technology where technically feasible is well aligned with our approach of looking at this problematic space. We recognise the following limitations:\\n● Hash matching algorithms can only match the media against known CSAM content hashes stored in our hash database to identify near duplicates. As a result, any uploads of\\npreviously unseen new CSAM content would fail to match any existing stored hashes in\\nthe hash database. ● Hash matching algorithms are designed to identify near duplicates of an image / content, not versions that vary from the original to a greater extent. As a result, it is very much possible that hash matching algorithms will fail to detect CSAM material that is similar to a known CSAM content hash but that doesn’t share the same visual or textual features. Bearing in mind the above limitations, in tandem with the hash matching algorithms it is\\nrecommended that services may make use of advanced AI and ML techniques from the fields of computer vision, optical character recognition (OCR ) etc.\"\\nAutomated content moderation (User to User) \\t\\tExplanation of their hash matching measures\\t\"We support Ofcom’s observation regarding how service providers may decide to implement hash matching database technology by either building it in house or by working with a third party CSAM hash database provider. As pointed out in Annex 15 para.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.991139829158783,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'A15.16 this decision depends upon a number of factors, so it should be left to individual service providers to decide which approach they want to adopt (as mentioned in Annex 15 para. A15.19). We also agree with the conditions set out by Ofcom for a hash database solution to be deemed “appropriate” for CSAM detection. Our in-house hash matching system has already incorporated respective factors into its design. These include: a governance process for content sourcing; partnerships with various industry organisations focusing on child safety such as NCMEC, StopNCII, and relevant NGOs; a governance process to include new content in CSAM hash databases; and strict security & privacy mechanism. We also developed the free photo detection software PDQ, used in, inter alia, the TakeItDown platform and the content moderation platform known as HMA - (Hasher Matcher Actioner), which is used by a range of companies to combat the spread of terrorist content on their platforms.\"\\nAutomated content moderation (User to User) \\t\\tFuzzy matching for CSAM URL detection\\tAs mentioned in the answers above, the technologies identified by Ofcom have both benefits and pitfalls, and can evolve very quickly. As a matter of principle, we would recommend avoiding the imposition of a specific technology and instead recommend allowing providers to choose from a variety of solutions and to continue to support the sharing of best practices, so that all providers are able to make use of the most up to date tools and technologies.': [{'label': 'POSITIVE',\n",
       "               'score': 0.991675615310669,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'In terms of accessibility for smaller services, URL detection can be done in stages, from simple text matching, all the way to crawling the content of the link and hashing media. In general URL blocking is a common integrity capability for services of any size due to the overlap with malware and spam. Providers can refer to industry initiatives such as Lantern to access a shared corpus of CSAM URLs to feed their own URL detection technology\\nAutomated content moderation (User to User) \\t\\tsuggest that the use of keyword detection for other types of harms (such as fraud) remains optional for providers. \"We note that standard keyword detection, while effective for some harms, like hate speech, isn\\'t as effective for other types of harms. In particular, keyword detection is less effective for more adversarial harms, like fraud, because fraudsters can simply change the words they use or avoid using common terms that are likely to be caught by keyword detection. Furthermore, it would be ineffective and inefficient to create a centralised system due to the diversity of usage and formats on the different platforms. We note that shared systems can also fail to meet scaling requirements, and have seen a number of examples of this in practice. Hence, mandating standard keywords to be applied to fraud would have little to no beneficial effect and could have a potential negative effect of diverting resources away from more impactful measures. For these reasons, we suggest that the use of keyword detection for other types of harms (such as fraud) remains optional for providers. As an alternative to static keywords lists, which are easily defeated, we see benefits to the use of machine learning models, and to encouraging the sharing of industry insights and trends to help tackle adversarial behaviours, as it is harder for fraudsters to overcome. E.g. at Meta, we currently share Tactics, Techniques and Procedures (TTP) as part of our Adversarial threat reports.\"\\nAutomated content moderation (User to User) \\t\\tExplain what they use at Meta\\t\"At Meta, we use media matching banks to detect and take action, if necessary, on images and footage that relate to the glorification, support or representation of designated entities, like terrorist or hate organisations, or designated events, like terrorist attacks or hate crimes. These banks help us identify potentially violating imagery at scale and action it rapidly and disrupt potential virality.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9912494421005249,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'When this imagery is shared without additional context to the user, it is deleted per our policy through automation. When there is context associated with the content, in the form of captions or text overlay for example, the content is sent to an automated classifier for an assessment scoring. If the classifier could not assess with confidence whether the content is violating and as such block the content, it becomes eligible for a human review for additional analysis to determine if the content was shared in a violating or non-violating context and if it should be taken down or not. For example, certain imagery related to DOI (dangerous organisations and individuals) is allowed in condemning or news reporting contexts but the same imagery would not be allowed if it is shared to glorify or support the designated entities’ violence. In certain more severe instances (e.g., extreme gore), imagery related to a DOI is policy violating regardless of sharing intent and in those cases, DOI banks are set up to remove content shared with or without context. Due to the adversarial and cross-platform nature of terrorism-related content and behaviours, an effective use of hash-matching technology to prevent the dissemination of such content also requires cross industry collaboration based on sharing technology and best practices, as well as a set of shared standards and protocols. This is illustrated by the Global Internet Forum to Counter Terrorism\\'s (“GIFCT”) rules governing the use of hash-matching technology between its members, specifically, the process GIFCT members have to follow to add a piece of content to the collective hash sharing database (which is available to all the members of the organisation). This process is a key part of ensuring the effectiveness of the database and mitigating the risks to freedom of expression that arise when false positives are included in the data set. Moreover, GIFCT’s Crisis Events Protocol enables GIFCT members to leverage hash-matching technology in order to respond to events in real time in a coordinated way across member platforms.\"\\nDefault settings and user support (U2U)\\t\\tExplain what WhatsApp use\\tWhatsApp offers a robust set of controls and defaults for all users on the platform today. We make controls available to users proactively, including by providing “just in time” notice when data is being collected or processed.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9936856627464294,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This empowers users to create the right WhatsApp experience for themselves. Where the functionalities and default settings for all users meet the threshold for the proposals set out by Ofcom, it should be made clear that this will be in compliance and no separate user experience for under 18s will be expected. Default settings and user support (U2U)\\t\\tExplain what WhatsApp use\\tPlease see our response to Question 31 above for WhatsApp and additionally, WhatsApp has also made dedicated resources easily available for teens, and their parents and guardians, to ensure that they are informed of the applicable standards and available options to them in the Teen Information Center [link included]\\nDefault settings and user support (U2U)\\t\\tAsking for flexibility\\t\"As mentioned by Ofcom, there are different ways to minimise the distribution of problematic content on platforms. We would recommend that providers are given flexibility as to the approach as part of the safe harbour.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929807186126709,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Taking into account the different set ups of services, what is helpful can vary per service. We are not sure whether Question 36 refers to other design parameters specifically geared towards illegal content or towards holistically looking at safety measures in the field of recommender systems. For the latter case, based on our own experience and tailored to our integrity systems, we highlight for some services a three-pronged approach as a set of measures to consider:\\n● Reducing problematic content distribution, e.g. Feed ranking to reduce the distribution of posts that may contain content that people find objectionable, but don\\'t necessarily meet the bar of removal under our policies - Some content will receive a lower relevance score and so be lower in Feed. This should be accompanied by reasonable transparency on underlying guidelines (too extensive transparency may encourage bad actors). ● Setting guidelines for recommendation of content because recommended content doesn\\'t come from accounts or entities that people choose to follow - e.g conducting work to avoid making recommendations that may be low-quality, objectionable, sensitive or inappropriate for younger viewers. ● Applying filters to prevent recommendations of bad actor accounts (e.g., accounts that\\nhave a history of posting policy-violating content but that have not met the criteria for\\naccount disabling) - While this measure focuses on accounts rather than content, it\\nindirectly contributes to limiting the distribution of policy-violating and potentially illegal\\ncontent by reducing the reach of accounts that share such content\"\\nEnhanced user control (U2U) \\t\\tQuestion proportionality\\t\"We agree that user controls and user verification schemes should rely on appropriate safeguards. Providers should be able to develop proportionate solutions for blocking and muting users’ accounts, disabling comments and when they create user verification schemes. However, we highlight that what is proportionate in practice may vary per service.\"\\nEnhanced user control (U2U) \\t\\tQuestion proportionality\\tWe do not think the first two proposed measures should include prescriptive requirements as to how these should be made known to users. What is effective (e.g. on timing, format and type of notification) will vary per service. Enhanced user control (U2U) \\t\\tExplain there are also downsides to voluntary verification schemes\\tBased on our current experience through the deployment of Meta Verified, we see benefits to voluntary verification schemes, as well as some specific challenges. On the positive side, such schemes can help the development of business models, e.g. enabling small & mid-sized businesses and creators to grow their reach and giving businesses and creators the ability to protect themselves from impersonation. It can enable consumers to make more informed decisions on who they interact with. However, verified accounts may face more threats of compromise by bad actors, due to their perceived legitimacy with users; malicious actors may also game requirements of verification to acquire a verification badge. Compromised or inauthentic verified accounts carry a higher risk of misleading and deceiving their audience, therefore driving potential downstream risk (e.g. scams).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924710988998413,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'To mitigate this risk, services may lean on automated and reactive detection-enforcement systems. User access to services (U2U) \\t\\tDisagree on some of the aspects of the proposed measures\\t\"We disagree with the approach in para. A.10.5 of Annex 7 to disable accounts based on the volume of violating content it has posted. In our view, this proposes a very convoluted system.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9700279831886292,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Articulating a specific formula like the one proposed in para. A.10.5 is likely counter-productive, as it fails to take into consideration the specificities of services across the industry and the varied nature of abusive behaviour. It carries a risk of over enforcement which may undermine rights such as freedom of expression.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9963222742080688,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Instead, the formula should be reframed to a more principled level. We recommend that Ofcom articulate the importance of considering violating content as part of the service\\'s effort to determine if an account is operated by a proscribed organisation, such as for example \"\"the nature of violating content posted should be taken into consideration to infer whether an account is operated by a proscribed organisation\"\". On a principle level, we are opposed to the wording to follow a government made list of proscribed orgs/individuals, as such list can end up being weaponized by governments around the world to crack down on political opponents, minorities, journalists and human rights activists\"\\nUser access to services (U2U) \\t\\tNote that for E2EE services, approaches that focus on volume of content would not be actionable or appropriate. \"WhatsApp agrees with the concerns raised above in Meta’s response. Blocking accounts does carry risks of over enforcement as well as the potential for unintended consequences (e.g. when phone numbers are recycled). We therefore welcome Ofcom’s caution in this area. Nevertheless, we strongly agree with the focus here being on sharers of CSAM and proscribed organisations.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9915408492088318,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As Ofcom develops the detail of these measures, we would welcome the opportunity for confidential discussions around our approaches and learnings in these areas. We would also note that for E2EE services, approaches that focus on volume of content would not be actionable or appropriate.\"\\nUser access to services (U2U) \\t\\tSuggest mechanisms such as machine learning and devide blocking\\t\"Various mechanisms come to mind to prevent such user from returning to the service, including:\\n● machine learning that uses a combination of factors to accurately identify a returning user,\\n● device blocking, and\\n● enforcement propagation to other products that are used by the user. No approach comes without disadvantages, as bad actors use sophisticated, adversarial and evasive tactics to evade enforcement and bypass restrictions.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9940284490585327,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'There is also a risk resulting from situations that e.g. an IP address and/or device can be shared across multiple users, so blocking purely on this information can lead to users who may not have intended to share CSAM being impacted. An additional or supporting option is industry coordination.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.989696741104126,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'E.g. “Lantern”, which we established with our partners at the Tech Coalition, was built for the advantage to enable technology companies to share a variety of signals about accounts and behaviours that violate their child safety policies. Lantern participants can use this information to conduct investigations on their own platforms and take action. As mentioned by Ofcom, this space raises specific challenges. Possible mitigations are e.g, that the Tech Coalition also established a Human Rights Impact Assessment which provides participating companies with recommendations that are intended to supplement and enhance the Tech Coalition’s existing efforts to uphold human rights\"\\nUser access to services (U2U) \\t\\tExplain what is done on their services\\t\"For proportionality, the period should vary depending on the nature of the offence committed. Based on the severity of harm and how many strikes a user has accrued on their account a\\nproportionate penalty should be applied. We exemplify an approach we suggest to use in case Ofcom follows up with a recommendation:\\n● For Facebook and Instagram malicious sharing of CSAM falls under the extremely high\\nseverity categories of harm. Any CSAM violator’s account in the case of malicious\\ndistribution is permanently disabled in the event of a single strike. ● For Facebook and Instagram for some more nuanced situations, there is a progressive\\napproach system, such as for non-malicious CSAM distribution to provide an opportunity\\nfor education. A proportionate time for a strike (which does not meet the category of the\\nabove bullet) to expire is one year. For more details on strike systems see below.\"\\nUser access to services (U2U) \\t\\tExplain what is done on their services\\t\"Strikes can form a reasonable part of a concept geared towards proportionality. Below we\\nexemplify an approach we suggest to use in case Ofcom follows up with a recommendation:\\n● Breaches of the provider’s policies may result in strikes and potential disablement of\\naccount. E.g. when a user posts content that goes against the Facebook Community\\nStandards or Instagram Community Guidelines, the content may be removed and we may\\ntake further action, such as applying a strike to the Facebook or Instagram account or disabling it. The actions to take will vary depending on the specific violation and\\nassociated severity. ● Malicious CSAM distribution is considered severe enough to disable the account, Page or group on Facebook or the account on Instagram permanently, after one occurrence. ● As mentioned above, in specific other circumstances, we will apply a progressive penalty system to ensure a proportionate approach.\"\\nGovernance and accountability\\t\\tTracking and reporting should be broadened to include user-reported events\\tTracking and reporting of content should ideally be broadened to encapsulate reporting detected and self-reported harms and safety events on these services (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9914653897285461,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'user usage of safety/moderation tools). Governance and accountability\\t\\tCurrent recommendations do not treat social VR platforms adequately\\tVR platforms operate as black boxes and transparency is needed to estimate the extent of harms posed here and what behaviours are occuring. Calls for these platforms to share monitoring reports on emerging abuses and harms. Second, there is no digital record of what occured in the space unless the user chooses to record the session. Record of events that can be taken forward for leagal action needed. Governance and accountability\\t\\tGovernment accountable for fuelling hate crimes online\\tGovernment needs to take accountability for the harmful rhetoric they enable, which is fuelling the amount of online hate crimes such as doxing and unlawful harassment. See, for example, the current political weaponisation of the transgender community, up to and including the Prime Minister’s multiple derogatory comments on public record.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9971522092819214,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approach to the Codes\\t\\tContests the definition of \\'onerous\\' not quantified. Cost benefit analyses restricted to certain measures, hence \\'onerousness\\' is based on unquantified assumtions of costs of regulatory compliance. Argues that Ofcom should set out for each measure disapplied to low/medium risk services: the anticipated cost and benefit of the measure, and the anticipated risk of disapplying the measure. The risks of not applying the measure to all services has not been clearly analysed. How these risks will be reviewed and monitored should be set out to decide point at which they are applied to more services. Approach to the Codes\\t\\tRisk assessment, monitoring and reporting to be extended to all service providers as is the case in financial services. This is a deviation from best practice and expectations from the financial services industry, where, for example, assessment of the inherent risk of business activities and the financial crime risks to which firms may be exposed has been the norm for many years. It is also a legal obligation under the Money Laundering Regulations 2017. In addition, firms must also have in place internal systems and controls to monitor and assess risks on a continuous basis. As successive studies have noted, online platforms are a key vector for fraud, the proceeds of which are integrated into the banking sector. It therefore appears perverse that service providers are not subject to equal/similar standards of behaviour. Approach to the Codes\\t\\tDefintion of large services should take an eco-system view and not be based on average user base\\tDistinction between larger and small services based on 7million user base/month creates adverse incentives for small/medium services to segregate offerings so as to avoid becoming large and come under additional obligations of the Code. These small/medium services may become attractive for illegal content. Approach to the Codes\\t\\tDefinition of multi-risk services flawed\\tThere appears to be an ontological flaw in the split between low, specific and multi. A service medium or high on one risk that did not fall into one of the nominated specific categories would find that none of the categories would apply to them. Content moderation (User to User) \\t\\tSafeguarding of staff moderating content\\tNo discussion of safeguarding of staff moderating content. Content moderation (User to User) \\t\\tHow to categorise \\'harmful\\' and \\'unharmful\\' content needed\\tTo make the proposal more useful, it can be extended with the (qualitatively) details of policies on what are categorised as “harmful contents” or “unharmful contents”, where are the boundaries set out as the guidance of content moderation. Automated content moderation (User to User)\\t\\tFocussed on text/imagery as content and does not take a holistic approach, esp in social VR\\tDoes not take into account user behaviour, interactions and intentional, directed verbal/non-verbal communication as the \"content\" such as in social VR (e.g., VRChat, RecRoom) and immersive online platforms. Provides examples of non-verbal social signals (proxemics, speech analysis) to infer stress/distress, longitudinal records of behaviour/interactions (particularly for platforms with some authentication). Provides evidence in the formal of journal article on the use of above for content moderation\\nAutomated content moderation (User to User)\\t\\tMissing quality of moderation work, having user feedback to improve accuracy\\tOfcom should set put how the dataset of \\'harmful contents\\' will be created as a result of ACM. This should incorportae user feedback to improve the accuracy and fairness of ACM algorithms. Argues for design of user interface with user input and processing, associated user training to report errors in ACM. Automated content moderation (User to User)\\t\\tACM may misclassify certain terms as hateful\\tSome communities will use terms to describe themselves that could be flagged as hateful, despite being used for reclamation and solidarity, and also to avoid monetization of their content without consent. Default settings and user support (U2U)\\t\\tUser training, esp for children and parents in safety measures on social VR platforms\\tArgues that children need training and education on how to use safety measures, consideration for paretal oversight. Parents can\\'t oversee child\\'s usage in social VR platforms due to the experience being through headmounted device. Parents also lack awareness of controls and risks that social VR platforms may have. Provides example of how parents can gain oversight on these platforms, through automated journalling. Default settings and user support (U2U)\\t\\tArgues for visibility, transperancy and explainability around actions taken for children in particular. Particularly where automated moderation is concerned, there needs to be a degree of visibility, transparency, and explainability around actions taken for children in particular. Without visibility and transparency, automated moderation won’t set suitable boundaries in terms of the expected behaviours on the platform, and without explainability children won’t develop trust in how this moderation is applied. Enhanced user control (U2U) \\t\\tProposes new measures with regards to social VR platforms\\tRecommends additional social VR platform-appropriate user controls, such as personal space, restrictions to speech usage, aged-group matching, private settings that can limit child to interact with their friends\\' list\\nEnhanced user control (U2U) \\t\\tEducational resources to illustrate how users can protect themselves from online harms and illegal content\\tTo ensure users play a more active role in enhancing their online experience, providing insight into how automation algorithms determine the content they see and offering controls to adjust these settings is essential. Provide educational resources such as recognising common threats, using platform safety features, digital literacy etc.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9900409579277039,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approach to the Codes\\tSegmentation \\tAgree that services with high risk of multiple harms should be subject to more onerous measures. Don\\'t agree with our definition of multi-risk services, as too simple, meaning services with a medium risk from two kinds of harm are automatically subject to the same measures as services with high risk of many harms. \"We agree with the general principle that services with high risk from multiple kinds of illegal harms should be subject to more onerous measures. However, as outlined in more detail \\nthroughout our responses (especially questions 15 and 16), we do not agree with the proposed definition of multi-risk services. We have concerns that the definition of multi-risk services as outlined in the consultation will lead to an oversimplified approach, where services with medium risk from two kinds of harm are automatically subject to the same measures as services with high risk from many kinds of harm.\"\\nApproach to the Codes\\tSegmentation \\tDropbox disagree with our treatment of filesharing / file storage services. Those with 70K users shouldn\\'t be automatically considered high risk. Whilst we recognise that these services can, in certain use cases, pose some risk for specific harmful content categories, we do not agree with Ofcom’s proposed approach whereby all file-storage and file�sharing services of 70,000 users or more should be automatically considered to be high risk and therefore subject to more onerous content moderation measures. We would suggest that content moderation - which takes into account privacy and freedom of speech together with user safety - should be proportionate to the risk of harm on the specific service or platform, in line with Ofcom’s stated aim to foreground proportionality. Approach to the Codes\\tSegmentation\\tSurprised to see Dropbox / file sharing/storage services associated with risk of IIA. The article was six years old and involved one reported instance (and explains what Dropbox did to address the issue). Wouldn\\'t want us to assume harm is there, without the benefit of the detailed risk assessments to come (implication being, not much IIA on Dropbox). \"We were surprised to see Dropbox mentioned on p.209, footnote 1093 in reference to the risk of file-storage and file-sharing services being used to facilitate intimate image abuse. The cited article was from 6 years ago and involved a single reported instance - and indeed the article \\ndescribes how Dropbox took down the reported folder swiftly and took steps to prevent it from being further shared. Although any service is potentially at risk of being abused, we would caution against assuming levels of risk associated with harm types on broad categories of services without the benefit of the detailed risk assessments to come, including analyses on the likelihood of abuse occurring (which may leverage insights from volumes of user reports over time) as well as mitigation efforts and responses.\"\\nGovernance and accountability\\t\\tDisagrees with the potential future measure. Services are best placed to understand the structure of own content safety proposals. Costs would be significant, especially smaller services (having an impact on competition). And, unless independent, could provide another opportunity for large services to entrench their market power. We believe that ultimately, services are best positioned to understand how to structure their own content safety programmes, processes and tools in a way that most effectively addresses harms. We would therefore be concerned that, if this measure were to be introduced, third-party auditors simply would not have the necessary context, product, and business knowledge to produce adequate assessments. We believe that with the support, guidance, and oversight of Ofcom, services themselves are best placed to determine whether their processes are effective. We are also concerned about the cost implications of this potential measure, which would likely present significant challenges, especially to smaller services, because third parties will be able to charge high fees for performing the audit. These cost challenges would also negatively impact on competition, as large companies with more resources and bigger compliance/legal teams would more easily be able to manage audit processes and fees.Finally, this potential measure could have other unintended consequences for competition if care is not taken to ensure that the auditor is properly independent. It is possible that the largest, most well-resourced technology companies will wish to take on the role of auditor, which would allow them to become even more entrenched whilst smaller companies struggle to cope with the costs and resourcing demands of complying with the audit measure. Governance and accountability\\t\\tDisagree with the potential future measure. Such measures could undermine users privacy / foe. We would recommend very cautious consideration of any future measures that seek to link remuneration with online safety outcomes. Such prescriptive measures risk services taking overly intrusive measures which could undermine users privacy and freedom of expression. We welcome Ofcom’s own assessment of such measures and its intention to recommend only well-evidenced risk-based measures that effectively support online safety outcomes. Approach to the Codes\\t\\tSupport a risk based / proportionate approach, noting that there\\'s not a \"one size fits all\" solution. The safety of our users is paramount, and we support Ofcom’s commitment to implementing a risk-based and proportionate regime. There is no “one size fits all” solution to addressing illegal content, and a proportional approach will be key to ensuring the Act’s effectiveness across the wide range of services that it covers - as Ofcom rightly recognises. Approach to the Codes\\t\\tAgree that there should be a proportionate approach that doesn\\'t place undue burden on small /medium sized services with limited resources and those services that are low risk. \"We agree with a proportional approach that doesn’t place an undue burden on small/medium sized companies with limited resources and services that are low risk. For those risks and harms that we do experience, we have put in place a range of mitigating measures, in the form of best-practice policies, processes and tools designed to prevent and minimise these harms, including the use of industry standard hash-matching technologies, external reporting, and human review for the detection of known CSAM and terror content.\"\\nApproach to the Codes\\t\\tDropbox\\'s functionality means it presents a very low risk of most online harms (e.g. lack of public indexing of content, browsing, finding other users, messaging or livestreaming).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9915187954902649,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Dropbox’s functionality means that it presents a very low risk of most online harms. Content on Dropbox is not publicly indexed or publicly searchable, and our service does not enable users to browse our platform or discover the content of other users on it. Users do not have ‘profile’ pages that are visible to other users or that enable strangers to connect.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.991263747215271,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We also don’t have messaging or live-streaming functionality.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9909251928329468,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This approach greatly reduces the prevalence, visibility and impact of illegal or harmful content. Approach to the Codes\\tDefintion of large services\\tWelcome our efforts to align defn with other regulatory regimes inc DSA\\t\"We welcome the effort to ensure consistency in definitional thresholds across the various international regulatory regimes, such as the Digital Services Act. \"\\nApproach to the Codes\\tDefintion of large services\\tDropbox would like more clarity on the defintion of what a \"user\" is for the purpose of establishing size (e.g. what about where there\\'s a mix of reg/un-reg services, large number of dormant users). \"However, to help companies navigate and comply with obligations under the Act, we seek further clarification on the definition of a “user” for the purpose of establishing a service’s size. The Act and consultation as currently written offer a broad definition that does not take into account entities that may have a mix of regulated and unregulated services, and services which may have a large number of dormant/inactive users. Where an entity is made up of various services, some of which may be regulated under the Act and some of which may not be, it is currently unclear whether UK users should be counted from only the regulated service(s), or from the entity as a whole. In addition, where services have a large number of registered yet inactive users, it is unclear whether these should be counted as users for the purposes of establishing the service’s size to check whether it meets the large service threshold. We would encourage Ofcom to provide further detail on these definitions. \"\\nApproach to the Codes\\tDefinition of large services\\tThink our CSAM HM segmentation for filesharing / file storage services is not proportionate. The lower threshold could have implications for competition, with smaller services struggling to cope with implementation costs, whilst genuinely large and risky services entrenching their power in the market. We also have concerns with some of Ofcom’s other conclusions relating to size thresholds and risk. The assertion that file-storage and file-sharing services are inherently high-risk, and that a threshold of 70,000+ monthly UK users should automatically require content moderation measures (specifically, the proposed CSAM hash matching requirement) for such services is surprising. We consider the lower threshold figure set for file-storage and file-sharing companies to be in contrast to the risk-based, proportionate approach that Ofcom is seeking to embrace -particularly given that other kinds of high-risk services have much higher thresholds. Beyond proportionality, the lower threshold could also have unintended consequences for competition and user rights as well as for online safety, with smaller services struggling to cope with implementation costs and resourcing demands, whilst genuinely large and risky services become more entrenched in the market. Approach to the Codes\\tDefinition of large services\\tThink the CSAM HM measure will result in undue compliance burdens on services, which actually present relatively low risk of harm to their user bases. Suggest we instead consider each service on a case by case basis, taking all risk factors into account. The vast majority of file-storage and file-sharing services fall well below the 7 million monthly user figure, and the proposed, much lower threshold is likely to impose an undue compliance burden on many services which, in actuality, present relatively low risk of harm due to their user bases, functionalities, and existing mitigation measures in place. We suggest that, rather than making a broad assessment for an entire swath of file-storage and file-sharing services, Ofcom should consider each service on a case-by-case basis taking all risk factors into account. This approach would help to ensure proportionality for all services - there is, after all, no “one size fits all” solution to illegal content, and this should be the case for file-storage and file sharing services as well as others. Approach to the Codes\\tSegmentation\\tAgree that services with high risk of multiple harms should be subject to more onerous measures. Don\\'t agree with our definition of multi-risk services, as too simple, meaning services with a medium risk from two kinds of harm are automatically subject to the same measures as services with high risk of many harms. \"We agree with the general principle that services with high risk from multiple kinds of illegal harms should be subject to more onerous measures. However, as outlined in more detail \\nthroughout our responses (especially questions 15 and 16), we do not agree with the proposed definition of multi-risk services. We have concerns that the definition of multi-risk services as outlined in the consultation will lead to an oversimplified approach, where services with medium risk from two kinds of harm are automatically subject to the same measures as services with high risk from many kinds of harm.\"\\nApproach to the Codes\\tSegmentation\\tDropbox disagree with our treatment of filesharing / file storage services. Those with 70K users shouldn\\'t be automatically considered high risk. Whilst we recognise that these services can, in certain use cases, pose some risk for specific harmful content categories, we do not agree with Ofcom’s proposed approach whereby all file-storage and file�sharing services of 70,000 users or more should be automatically considered to be high risk and therefore subject to more onerous content moderation measures. We would suggest that content moderation - which takes into account privacy and freedom of speech together with user safety - should be proportionate to the risk of harm on the specific service or platform, in line with Ofcom’s stated aim to foreground proportionality. Approach to the Codes\\tSegmentation\\tSurprised to see Dropbox / file sharing/storage services associated with risk of IIA. The article was six years old and involved one reported instance (and explains what Dropbox did to address the issue). Wouldn\\'t want us to assume harm is there, without the benefit of the detailed risk assessments to come (implication being, not much IIA on Dropbox). \"We were surprised to see Dropbox mentioned on p.209, footnote 1093 in reference to the risk of file-storage and file-sharing services being used to facilitate intimate image abuse. The cited article was from 6 years ago and involved a single reported instance - and indeed the article \\ndescribes how Dropbox took down the reported folder swiftly and took steps to prevent it from being further shared. Although any service is potentially at risk of being abused, we would caution against assuming levels of risk associated with harm types on broad categories of services without the benefit of the detailed risk assessments to come, including analyses on the likelihood of abuse occurring (which may leverage insights from volumes of user reports over time) as well as mitigation efforts and responses.\"\\nApproach to the Codes\\tDefinition of multi-risk\\tAgree that services with risks of multiple kinds of illegal harm should be required to mitigate them. We should however put more weight on severity of risk (rather than single/multi), as a small number of high risks may present more danger to users than a moderator number of medium risks. We agree with the general principle that services with risk from multiple kinds of illegal harms should be required to mitigate those risks. However, we believe that Ofcom should foreground overall severity of risks, rather than whether a service is single-risk or multi-risk, as a small number of high risks may often present more danger to users than a moderate number of medium risks. Approach to the Codes\\tDefinition of multi-risk\\tConcerned the defintion of multi-risk is oversimplified/not proportionate. There is a significant difference between services with medium/high risk of two harms and medium/high risk of ten. Firstly, the difference between services with medium/high risk from two kinds of illegal harms, and services with medium/high risk from ten kinds of illegal harms, is significant. We have concerns that the definition of multi-risk services as outlined in the consultation will lead to an oversimplified approach, where (holding risk level constant) services with risk across two harm categories are subject to the same measures as services with risk across many categories. This approach leaves little room for nuance or proportionality and is not in line with Ofcom’s own proposals for a risk-based, proportionate approach. Approach to the Codes\\tDefinition of multi-risk\\tThere will be few companies with one risk, hence negates usefulness of the \"single risk\" classification. Secondly, we suspect that many companies will have more than one medium level risk, which means they will be covered by the term “multi-risk”, and very few will be “single risk”. This negates the usefulness of the classification. Approach to the Codes\\tDefinition of multi-risk\\tInstead of our approach, we should focus on severity, and take a case by case approach. This would help ensure the platforms that present the greatest dangers to users, face the most onerous measures (and reduce compliance burden for less risky services). Considering all of the above, we believe that a more appropriate approach for Ofcom would be to foreground severity of risk, and consider the number of harms each service is at risk for on a case by-case basis, rather than applying a blanket classification. This would help to ensure that platforms that present the greatest danger to users will be subject to the most onerous measures, and avoid an undue compliance burden for less risky services. Approach to the Codes\\t\\tDropbox is supportive of our general approach, but would welcome more clarity/guidance on how their mitigation measures should be factored into their RAs \\tGenerally we are supportive of the draft Codes of Practice. With that in mind, we have set out our approach to safety below and would appreciate further clarity from Ofcom about how such mitigation measures should be factored into services’ risk assessments. Approach to the Codes\\t\\tDropbox think we should take a \"case by case\" assessment of each services functionalities and user base to establish the level of risk. It will help improve our proposals proportionality (e.g. better capturing small high risk services, and not overly burdening large low risk services). We believe that a more effective approach would be for Ofcom to consider each service on a case-by-case basis, taking into account the service’s specific functionalities and user base to establish an overall level of risk. This approach would ensure that all services are placed on equal footing, and avoid small-yet-high-risk services falling through the cracks (equally, no larger-yet low-risk services would be unduly burdened). It would also be more in line with Ofcom’s commitment to proportionality\\nApproach to the Codes\\t\\tDropbox\\'s functionality and business model, mean it presents a relatively low risk of abuse. (see comment above, as similar if less detailed). Dropbox’s functionality, features and business model means that it presents a relatively low risk of abuse. Indeed, most of the harms set out in the Act simply don’t present themselves or aren’t prevalent on Dropbox because we don’t have messaging or discoverability functionality. Dropbox is not a social media company or a publishing platform. The content hosted by Dropbox is not publicly indexed or searchable, and it’s not possible to discover other users or their content within Dropbox. Dropbox users don’t have ‘profiles’ that are visible to others or that enable people who aren’t already connected to discover each other. While Dropbox users can share content through the service, widespread dissemination relies on a distribution channel such as messaging service or social media platform. As a file hosting and collaboration platform, our service is primarily designed as a private space for managing files and workflows within professional teams. Our services are not targeted towards or marketed at young people nor do they present risks of virality or dissemination. Content moderation (User to User) \\t\\tDropbox broadly agree with our ConMod measures, and welcome the flexibility offered to help different services (size, business model, product) comply with their duties. We broadly agree with the proposed measures outlined. Specifically, we welcome the flexibility provided to services to meet the measures in a way that is cost effective and proportionate to their needs and assessment of risk. The application and success of content moderation functions is different for every company and based on a range of factors such as size, business model and product\\nContent moderation (User to User) \\t\\tDisagree with Ofcom\\'s characterisation that the wellbeing of content moderators would only be relevant if it impacted user safety. Their wellbeing is paramount to them doing an effective job on moderating content. We disagree with Ofcom’s assessment that the wellbeing of content moderators would only be relevant if it impacted user safety. Ensuring the mental health and wellbeing of moderation teams should be considered as a core component for services that rely on content moderators to support their content moderation function. The mental health and emotional welfare of moderation teams is as important to the effective content moderation as providing access to training and materials. Neglecting this aspect not only undermines the overall health of moderation teams but also puts at risk the quality of content moderation and, consequently, user safety. We would advocate for a more holistic approach that recognises the link between moderator well-being and the success of content moderation functions. Automated content moderation (User to User)\\t\\tThink our CSAM HM measure is appropriate (for the services identified)\\t\"We recognise that these services can pose some risk as a result of bad actors seeking to misuse the service functionality to store or share CSAM content and, without appropriate \\nmitigation measures in place, they can be a high risk for such content. We therefore believe that Ofcom’s proposal that such services should use hash matching to detect CSAM is appropriate.\"\\nAutomated content moderation (User to User)\\t\\tDropbox take a number of measures to detect and remove CSAM. In terms of our approach to CSAM, we use a variety of tools and processes including hash matching, URL sharing agreements, trusted flagger programme, user reports, and human review, as well as working with industry initiatives, to swiftly find potentially violating content and action it as appropriate \\nAutomated content moderation (User to User)\\tPublic-Private Guidance\\tDropbox support our application of proactive tech to publicaly communicated content. Ofcom’s decision to apply these proposals only in relation to content communicated publicly is a welcome one. It will help to ensure the technical feasibility of the Act, as well as proportionality\\nAutomated content moderation (User to User)\\tPublic-Private Guidance\\tFor various reasons, Dropbox considers content in its service to be \"privately communicated\" \\tDropbox’s service is primarily designed as a private space for managing files and workflows individually or within professional teams. On Dropbox, content is not publicly indexed or searchable. Users can only conduct a search for their own content or for content that has been shared with them. It is not possible to discover other users’ content on our platform, and Dropbox users do not have ‘profiles’ that are visible to other users or that enable people who aren’t already connected to discover each other on Dropbox. Therefore, content hosted on Dropbox falls within Ofcom’s definitions for privately communicated content: it is not easily accessible to a large number of UK individuals, access is restricted via password and invite/permissions requirements, and we do not have many of the functionalities associated with viral sharable content such as a content recommendation algorithm or the ability to repost or tag others. It is possible to create links to stored content that can then be shared publicly via other means, and those file can be viewed by anyone with the link URL. Automated content moderation (User to User)\\t\\tHM is a key part of a content moderation programme, but does have some shortcomings, including accuracy, which is why human review is vital to it being effective and ensuring rights are protected. Hash matching is a key part of our content moderation programme. However, whilst the technology is effective, it has notable shortcomings. Threshold sensitivity is a particular challenge. Hash-matching technology relies on sensitivity thresholds to detect “perceptual” matches - aka “fuzzy matches” - which aim to find images close to the original image but which in practice can - and do with sufficient regularity - result in false positives. This is why human review is a vital component of our content moderation programme.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923064112663269,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Human review is critical to ensuring the accuracy of automated tools, protecting the privacy rights of users, and vetting external or user reports of potentially violative content. Automated content moderation (User to User)\\t\\tThe effectiveness of automated tools (incl HM) vary by service (incl size, business model, product). The costs of such tools can be extensive and on-going. \"Whilst automated tools can prove highly useful to help content safety teams identify illegal \\ncontent and prioritise action, they do not provide a complete solution, nor will they be equally \\neffective for every company. The application and success of such tools is different for every \\ncompany and based on a range of factors such as size, business model and product. Furthermore, the costs associated with a detection programme (both automated and human) can be extensive  and ongoing, covering everything from technical costs (both third-party and in-house), to \\nacquisition and quality control of ingested hash sets.\"\\nAutomated content moderation (User to User)\\tHM/URL detection for Terror\\tAssessing context re: terror is particularly challenging. There may be legitimate reasons why an academic / journalist may want to store what might be otherwise deemed illegal content.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9564703106880188,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As Ofcom is already aware, the challenge of assessing context with regards to potentially illegal content is formidable. The additional layer of context needed to assess the legality of a particular piece of content is almost never available to us. Our ability to review is limited to the content itself, so it is very difficult, if not impossible, to identify the reason why a user would have a given piece of content in their account. For instance, an academic or journalist might want to store, for perfectly legitimate reasons, material that might otherwise be regarded as illegal. Given the lack of context we have, we note the risks that over-removal could pose to users’ privacy and freedom of expression. Automated content moderation (User to User)\\tHM/URL detection for Terror\\tNo perfect solution, but HM, effective reporting and complaints, and human review can help services strike the right balance. \"There are no perfect solutions to this problem, however we believe that a combination of hash matching, human review, and effective reporting and complaints systems (including for \\ncomplaints from users who feel their content has been unfairly removed) can help services to strike the delicate balance between protecting user rights and detecting illegal content. \"\\nAutomated content moderation (User to User)\\tHM/URL detection for Terror\\tA particular challenge with terror is the lack of a global defn, which can mean HM data sets can be limited or limiting. We note however that the lack of global definition around what terror content is can mean the available terrorism hash sets can be either quite limited - or limiting - depending on a company’s policy definition of terror content. Automated content moderation (User to User)\\tHM/URL detection for Terror\\tCosts of terror hash matching can be extensive. \"As with CSAM detection, the costs associated with a detection programme (both automated and \\nhuman) can be extensive, as they must cover both technical and team-related costs.\"\\nTerms of service and Publicly Available Statements\\t\\tDropbox considers our ToS proposals to be sensible, and is doing a number of things to make its ToS accessible to all. Ofcom’s proposals here are sensible.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924873113632202,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Our Terms of Service, Privacy Policy and Acceptable Use Policy are easily available to the general public on our website. All of our communications, including our Terms of Service, Acceptable Use Policy, and materials on the our Help Center, are written to suit the lowest possible reading age to ensure clarity and accessibility for all. We partner with a third-party accessibility testing service to ensure we deliver on our commitment to inclusion at all times, measuring against a set of standards set by the Web Content Accessibility Guidelines (WCAG)\\nUser access to services (U2U)\\t\\tDropbox agrees with the general principles of the proposal. It does worry that the context needed to determine whether an account is a terrorist org isn't available to services like theirs.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9897185564041138,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We agree with the general principles of these proposals. However, with regards to the proposed measure to address accounts operated by or on behalf of a terrorist group or proscribed organisations in the UK, we would like to reiterate our concerns around the difficulty of assessing context with regards to potentially illegal content. As outlined in question 26, such context is often not readily available to services like ours, where content is located within a private space without the additional contextual information provided by features such as personal profiles, comments, or reposts. This lack of context makes it extremely difficult to assess the risk without jeopardising users’ privacy and freedom of expression, so it is important that services seek to strike the right balance between effective content moderation and protecting user rights via robust detection and complaints processes. User access to services (U2U) \\tCSAM blocking accounts\\tIts approach is set out clearly in its ToS. In the case of finding CSAM on an account, they freeze the users account and disable all shared links the user has created. Our Terms of Service clearly state that we can review the conduct and content of our users for compliance with our Acceptable Use Policy, which prohibits a range of activity including the storing, publishing, or sharing of illegal material such as CSAM and terrorist content. Those documents also explain to our users that, in response to violations of our policy, we will take appropriate action, including as removing or disabling access to content, suspending a user’s access to our services or terminating an account. In the case of apparent CSAM, for example, we take immediate action by freezing the user’s account and disabling all shared links that user has created. User access to services (U2U) \\tCSAM blocking accounts\\tThey have systems in place to address potential false positives, specifically appeals. We believe that these actions are appropriate in response to violation of our Acceptable Use Policy, however we also have systems in place to account for potential false positives. Users who feel their content has been unfairly removed can appeal to explain why the content does not violate our terms and conditions, or why they have a legitimate reason to possess it. Depending on the nature of the content, the potential restrictions placed on their account, and the information available to us, the content may be re-reviewed and reinstated. User access to services (U2U) \\tCSAM blocking accounts\\tOnce aware Dropbox disables the account (no time limit). Child sexual exploitation and abuse has no place on our service, and when we become aware of it, we swiftly take action to disable the account and prevent the content from being shared\\nUser access to services (U2U) \\tCSAM blocking accounts\\tDropbox notes more detail on the actions it takes to users accounts who distribute CSAM. As set out in our Terms of Service, violations of our Acceptable Use Policy constitute material breaches of our Terms and we reserve the right to terminate or suspend access to our services without notice to the user when such a material breach occurs. When a user’s access is suspended or terminated, the user is not able to export their files. Child sexual exploitation and abuse has no place on Dropbox, and when we become aware of it, we swiftly take action to disable the account and prevent the content from being shared. Automated content moderation (User to User) \\t\\tAutomated tools are important but must be complemented by human review and other elements (e.g. robust complaints and reporting) as context is a challenge\\t\"Whilst automated tools can prove highly useful to help content safety teams identify illegal content and prioritise action, they do not provide a complete solution, nor will they be equally effective for every company. This is why human review is a critical component of content moderation programmes, to ensure accuracy of automated tools, protect the privacy rights of \\nusers, and vet external or users reports of potentially violative content. However, even with human review it can be incredibly difficult to establish context when reviewing potentially illegal content which can pose a threat to users’ privacy and freedom of expression. This is why we also have systems in place whereby users who feel their content has been unfairly or incorrectly removed can appeal to explain why the content does not violate our terms and conditions, or why they have a legitimate reason to possess it.\"\\nApproach to the Codes\\t\\tFuture measures and a focus on verification \\tFocuses on adult service websites (ASW) and makes recommendations on different measures that can help tackle modern slavery. Approach to the Codes\\t\\tSegmentation \\tShe agrees with our definition of large service, however she flags the need to consider Adult service websites as a caveat. Cumulative Assessment  \\t\\tProportionality of burden on SMEs\\tShe agrees that the overall burden is proportionate for SMEs that find they have a significant risk of illega content. Cumulative Assessment  \\t\\tProportionality of burden on Large Services \\tShe agrees that large services get the most users and have the most recource to address issues. Governance and accountability\\t\\tMicrosoft agrees that certain governance measures should apply to large / multi-risk services. Microsoft agrees that the proposed governance and accountability measures (i.e., annual reviews of risk management activities and internal monitoring and assurance functions) should be applied to large and multi-risk services. Governance and accountability\\t\\tRobust governance measures and regular reviews are integral to effective risk management. Would like Ofcom to provide additional flexibility to allow large orgs to manage these processes through \"centralised risk management functions\" (rather than by service). \"We agree that robust governance processes and regular review are essential to any effective risk management program, particularly for larger regulated services that may pose a \\ngreater risk of online harm due to the amount of its monthly active users. We encourage Ofcom to provide larger regulated services additional flexibility to meet the proposed measures outlined in Section 8 of Volume 3 through centralized risk management functions. In practice, within larger organizations some of the referenced governance functions are operated through centralized practices and teams, rather than at the individual service level. This approach to regulatory governance promotes operational efficiencies, deeper subject matter expertise and economies of scale. For example, many of Microsoft’s services are effectively supported through centralized assurance functions.\"\\nGovernance and accountability\\t\\tIf we require audit, it should be aligned with DSA. Though, notes that audit is very expensive and hence would detract from other actions to keep users safe, and hence might not be aligned with the overall safety goals of the OSA. \"Should a future independent third-party audit measure be proposed, we strongly encourage Ofcom to harmonize the requirements with those found in Article 37 of the Digital \\nServices Act (“DSA”) and, like the DSA, apply it solely to large services that present the highest risks to end users. We agree with Ofcom’s view that alignment between existing online safety regulatory regimes promotes compliance while minimizing the burden on business. Harmonization with the DSA’s \\naudit requirements can achieve both goals while also enabling further development of a global and standardized approach, supporting effective and consistent safety information-gathering \\nacross jurisdictions. Taking this approach would provide clarity for regulated services and allow them to leverage relevant, pre-existing work. We note that engaging third-party auditors to conduct a top-down review of a compliance program is an expensive endeavour. There are currently few audit firms engaged in this work, as standard methodologies have not been established. Independent audit is also a time and labour�intensive endeavour for companies. The internal resources required to support audit \\nengagements are those knowledgeable of and responsible for core safety compliance within a service. Months of audit engagement take these resources away from their core support for \\nsafety improvements in a service. As such, independent audit obligations, particularly where they may be duplicative of work already completed in support of obligations from other jurisdictions, may detract from the overall safety goals of the UK Online Safety Act (“OSA”). \"\\nGovernance and accountability\\t\\tMicrosoft don\\'t support links between renumeration and specific safety outcomes (esp rate based outcome), as (i) stats may be out of a managers control, (ii) measure of success could vary by service, (iii) may have unintended consequences, (iv) lead to artificial inflation of safety outcomes. Microsoft recommends against the use of specific safety outcomes as a significant determination of renumeration, particularly if they are rate-based outcomes (e.g., the presence of illegal content on the service, the number of content items actioned, the number of times users are exposed to illegal content, etc.). Rate-based safety outcomes could be a result of several factors that are not in a senior manager’s control, such as under-reporting of illegal content by end users. The measure of “success” could also vary from service to service, making it difficult for services to accurately determine whether they are in “compliance.” Rate-based reward systems may also have the unintended consequence of discouraging senior managers from seeking or documenting online harm risks or negative outcomes for fear that their renumeration will be impacted; it could even result in an artificial inflation of safety outcomes. Enabling senior managers to dispute a safety-based impact to remuneration introduces even more practical complexity. Should this measure be included in future guidance as an indicator of a company or service’s safety governance practice, we encourage a behaviour-based approach that considers whether services take into consideration positive online safety behaviours as one factor in a holistic review to determine the renumeration for senior managers. Approach to the Codes\\tAccessibility\\tWe may want to break up our consultations into shorter pieces, to optimise accessibliity, particularly for smaller services.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9918467998504639,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Ofcom may wish to consider dividing future consultations into shorter, more manageable pieces to yield the highest quality evidence and optimize accessibility to the material, particularly for smaller services. Approach to the Codes\\tAccessibility\\tThe breadth of content may hinder submitters ability to adquately address all 50+ questions. The extraordinary breadth of content in this ~1700-page Consultation, published all at once, may significantly hinder submitters’ abilities to adequately address all 50+ questions with constructive, data-driven feedback and with submission of specific evidence as requested by Ofcom\\nApproach to the Codes\\tAccessibility\\tDo acknowledge efforts we made including chapter summaries and webinars, but purely focussing on these materials could mean missing other important points. We acknowledge the many resources Ofcom provided, such as webinars and chapter summaries, to explain the contents of the Consultation. However, relying on such materials may cause a service to miss important issues that merit its input. Approach to the Codes\\t\\tAgree that the most proportionate approach is to apply the most onerous measures on regulated services that pose relatively greater risks to end users. Think we should adjust defn of large/multi-risk. We agree that the most risk-proportional approach is to apply the most onerous measures to regulated services that pose relatively greater risks to end-users, dependent on the nature of the service and the risks in question. However, we encourage Ofcom to adjust how it has chosen to define “large services” and “multi-risk services” to more accurately reflect the proportional risk regulated services pose in practice (discussed further at Questions 14 and 15 below). Approach to the Codes\\tDefinition of large\\tMicrosoft disagrees. It thinks our definition is overly inclusive and should be adjusted. No, Ofcom’s current definition of “large service” may be over-inclusive in practice and should be adjusted so that the definition reflects both size and safety risks\\nApproach to the Codes\\tDefinition of large\\tMain reason for using 7M is that it's consistent with DSA. However in DSA Search and U2U services are treated differently. The proposed seven million user threshold has been calculated largely using one factor: the Digital Services Act (“DSA”) methodology for determining Very Large Online Platforms (“VLOPs”) and Very Large Online Search Engines (“VLOSEs”). But the EU’s approach to VLOPs/VLOSEs makes an implicit distinction between services, applying to Online Platforms (as defined in the DSA) and search engines only. Ofcom’s definition of “large services,” on the other hand, captures all types of U2U/search services. As a result, Ofcom’s proposed threshold would apply to many different types of services, equating the risk of any in-scope service without further differentiation. Approach to the Codes\\tDefinition of large\\tWe need to provide more clarity on defn of large. Should we just include active users or all users?\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910056591033936,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Under what circumstances would a user qualify as an active user? Search services would also benefit from the guidance. We also encourage Ofcom to engage with industry to clarify the methodology regulated services should use when determining the number of monthly active users. The “User numbers” section in Annex 7 for U2U services provides incomplete guidance on how services should count users. (Annex 7 - A11.7 - A11.11). For example, does Ofcom want services to include all users or active users? If “active,” under what circumstances would a user qualify as an “active user?” Annex 8 for search services would similarly benefit from more guidance. Approach to the Codes\\tDefinition of large\\tWhatever approach we use, we shouldn't align with DSA, as has had a number of informal / formal challenges to its methodology.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9932426810264587,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Would benefit from working discussions with diverse services. Whatever methodology is chosen, we would advocate against aligning to the DSA’s methodology for monthly active users (“MAU”) for determining designation and for purposes of calculating supervisory fees. We are aware that the European Commission has received a number of informal and formal challenges to its methodology. The variable nature and functions of in-scope services will also impact many variables such as the relevance of daily users versus monthly users, or page views versus engagement. This question may benefit from additional, working discussions with diverse industry participants. Approach to the Codes\\tDefinition of mult-risk\\tDisagree with our proposed defn of multi-risk. Concerned the current defn is over inclusive and could disproportionately apply most onerous measures to lower risk services. No, the definition should be considered further. Additional nuance could be incorporated to more accurately reflect the relative risk of each regulated service. In the definition’s current form, we are concerned it is over-inclusive and could disproportionally apply the most onerous measures to lower risk services\\nApproach to the Codes\\tDefinition of mult-risk\\tDefn of two or more risks is not supported by data / arbitrary. The definition of “multi-risk” service captures regulated services with medium or high risks that at least any two of the 15 illegal harms may occur on the service. But there does not appear to be any data to support the two-harm threshold and we understand Ofcom has acknowledged in other forums that this is to some extent arbitrary, which is reflected in the document’s descriptions\\nApproach to the Codes\\tDefinition of mult-risk\\tBar is too low for medium risk. Almost any risk could potentially be medium. The Annex 5 Table 6 risk-levelling guidelines are also written such that almost any risk could potentially be at least medium. For example: \\uf0b7 Likelihood: a medium risk could exist if there is “some evidence of harm” occurring on the service; \\uf0b7 Impact: a medium risk could exist if there is “evidence of harm impacting a material number of users.” It is reasonable a service may determine a “material number” is reached once the impact exceeds the low-risk threshold of a “very few” number of users\\nApproach to the Codes\\tDefinition of mult-risk\\tThe test is therefore too broad with a service with two medium harms facing the same mitigations as one with high risk for multiple harms. The test for the appearance of harm is therefore broad, increasing the likelihood that many, if not most, services may be at risk of multiple harms in some way. This generalized approach could lead to the disproportionate treatment of two services with significantly different risk profiles. For example, a U2U service with a medium risk for two illegal harms with a relatively less severe impact would still be expected to implement the same measures as a U2U service with a high risk for multiple illegal harms\\nApproach to the Codes\\tDefinition of mult-risk\\tThere are a number of ways to address this issue (multi-risk bar too low), e.g. (i) not treating each illegal harm the same (adding in a severity scale), (ii) increasing the threshold from two to say seven, (iii) integrate other risk vectors into the assessment, e.g. is the service Category 1 or 2a. \"We believe this result can be avoided if additional levels of differentiation are built into the definition, enabling a more tailored analysis. This could be done in several ways, such as: \\uf0b7 Utilizing the information from Ofcom’s risk registers to create a spectrum of approximate impact for the 15 illegal harms. This will avoid treating each of the illegal harms the same \\nand result in a more accurate service risk profile. This could draw on Ofcom’s harms research and collaboration with multistakeholder experts. \\uf0b7 Increasing the threshold for medium/high risk illegal harms. In Volume 4 of the consultative materials, Ofcom balances its decision between “at least one kind of illegal harm” and “many kinds of illegal harm,” opting for two. Two illegal harms are likely found in most U2U services. We believe a more proportional approach could exist if the threshold were set closer to a middle ground such as seven, for example, or if more objective criteria were added to demonstrate the severity of the harm. \\uf0b7 Integrate additional risk vectors into the definition of “multi-risk” such as service functionality, service-type, or taking into consideration whether a service is a Category 1 or 2a service in the future.\"\\nContent moderation (User to User) \\t\\tMicrosoft supports our proposed conmod measures, and would welcome additional guidance on compliance. We agree with the proposals and we encourage Ofcom to provide clarity for when a regulated service would meet their obligations under the measure to set and record performance targets\\nContent moderation (User to User) \\t\\tAgree with offering flex (on performance targets). When would Ofcom be able to determine if a service failed to meet a satisfactory target? What factors / data / docs would Ofcom expect to see in support? We agree that services should have a sufficient amount of flexibility to tailor performance targets so that they are proportionate, given the specific operation of the service. But when would Ofcom determine that a service had failed to set a satisfactory target? Are there operational factors, data or documents Ofcom would expect a service to cite or memorialize when setting its targets? Automated content moderation (User to User)\\t\\tSupportive that ACM measures just applicable to publicly communicated content on U2U services\\tWe acknowledge the thoughtful approach Ofcom has taken to technical proposals and that these have been limited to content that has been communicated publicly on U2U services. Automated content moderation (User to User)\\t\\tConcerned about risks of over moderation with CSAM URL detection and Fraud Keyword, which would have impacts on human rights (e.g. FoE), and expensive human moderation, increasing the time to investigate CSAM cases generally. However, we are concerned that the current state of technology for CSAM URL matching and fraud keyword detection is insufficient to enable this without risking over-moderation, with potential impacts for fundamental human rights, including the freedom of expression and access to information. It may also result in a significant number of moderation decisions that are overturned on appeal – and likely would require a large increase in the need for human moderation to investigate the open internet for further context. In addition to increasing the wellness risks to these moderators, the length of time required to investigate each case may result in slower resolution of CSAM cases generally\\nAutomated content moderation (User to User)\\t\\tConcern re: lack of effective third party databases for fraud keyword. Effectively enabling both types of detection would also require in-scope services to have ongoing and up-to-date access to authoritative sources of information for such URLs and for fraud keywords. While the Internet Watch Foundation currently can provide members with URL lists to support de-listing of CSAM URLs by search engines, we are not aware of any entity providing similar services with respect to fraud. Automated content moderation (User to User) \\t\\tMicrosoft supports the use of HM to detect duplicate CSAM at scale\\tMicrosoft has long supported hash-matching as an effective means to detect duplicate CSAM at scale. Leveraging tools like PhotoDNA enables providers to protect the safety of their users and the integrity of their services, while contributing to addressing this harm across the wider online ecosystem. Automated content moderation (User to User) \\t\\tOwn experience of HM demonstrates cost and limitations of the tech, i.e. (i) it\\'s only as good as the hashes it relies on. So needs human review / needs extensive investment in tech and tools / training to ensure accurate. And operations can cost millions / annum to conduct. Our own experience with hash-matching and supporting others to implement the technology demonstrates some of the costs and limitations involved. First, hash-matching technology is ultimately only as good as the hashes on which it relies. The technology detects duplicate images with precision but relies on access to a database of hashed content. When Microsoft identifies or receives a report of potential CSAM through one of its own services, it conducts human review to determine whether the content is CSAM, and creates its own hashes. Microsoft also receives hashes from trusted third parties such as the Internet Watch Foundation, but here, too, when a match is found on a Microsoft service, Microsoft conducts human review. Third-party hash sets often contain content that is not itself CSAM but may be related to other images that are CSAM; the non-CSAM content is not actionable as it is not illegal content and Microsoft has no way of verifying its relation to other content. All of these processes require extensive investment in technology and tooling, personnel, ongoing training, and wellness, among other things. Even with a smaller specialized team of moderators, these operations can cost millions of pounds annually to conduct. Automated content moderation (User to User) \\t\\tThere are costs to using a 3rd party hashing system (even if from a trusted source). Though human review is not required, reliance on third-party hashes (even from a trusted source) has costly consequences elsewhere in the ecosystem. Without verification, costs are shifted onto law enforcement which receives a greater volume of CyberTips to review where the content is ultimately not actionable. There is a cost to users whose accounts may be inappropriately suspended, and who must appeal for a return. There is also cost to the platforms as it may result in a greater percentage of appeals that must be reviewed and overturned. And, some third-party hash databases may come at a cost to companies through membership or other fees. Automated content moderation (User to User) \\t\\tThough Microsoft supports expansion of CSAM HM, it notes that its expansion is based on the assumption that a \"reliable and low-cost technology\" is commercially available to services. They do not believe this assumption is correct.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9915266633033752,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Costs may also be incurred by smaller providers to access hash-matching services. For example, current licensees of Microsoft’s PhotoDNA Cloud Service are charged for any requests above 10 million per month to recoup processing costs.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9362800717353821,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'These processing fees do not cover Microsoft’s actual costs to make the service available, nor the cost of processing requests under the 10 million monthly threshold. Therefore, while Microsoft supports the expansion of hash-matching for CSAM, we note that the current proposals assume reliable and low-cost technology is commercially available to companies --- an assumption that we do not believe is accurate\\nAutomated content moderation (User to User) \\t\\tAnother reason why commercial viability is a challenge is misuse by adversarial actors. Microsoft has strict criteria on potential licensees. One reason commercial viability of hash matching technologies is limited is because of the potential for misuse by adversarial actors. These adversarial actors have attempted to leverage our hash-matching technology to reverse engineer the content of hash datasets in an effort to circumvent detection of known CSAM and other hashed content areas. For this reason, Microsoft has imposed strict criteria on potential licensees, limiting the overall availability of PhotoDNA so as to remove these potential risks. Automated content moderation (User to User) \\tTerror HM\\tTerror content even more challenging, as unlike CSAM, may not be illegal on its face. Context/intent is key. As a member of the Global Internet Forum to Counter Terrorism (“GIFCT”), Microsoft also leverages hash-matching to detect potential terrorist content across its consumer hosted services. However, unlike CSAM, terrorist content may not be illegal on its face and will often require an assessment of context and/or intent. For instance, even content branded by a United Nations listed organization may be held or shared by researchers or journalists undertaking work to better understand the phenomenon. Research undertaken following terrorist attacks has also shown that content may be shared by news organizations or indeed by ordinary citizens looking to express outrage or sympathy for victims. Examples of the latter have arisen in the context of the recent conflict between Israel and Gaza. Automated content moderation (User to User) \\tTerror HM and URL detection\\tMicrosoft urges caution on HM/URL detection for Terror. GIFCT / Tech against terror are the two sole sources. The GIFCT database is not a source of \"verified\" content, but provides hashed content that other members have determined fell within the taxonomy (of terrorist content). It therefore relies on trust. We therefore urge Ofcom to continue to consider carefully any requirements to mandate hash or URL matching for terrorist content, including to ensure it is applied appropriately. We also note, as the current Chair of GIFCT’s Operating Board, that GIFCT and Tech Against Terrorism are the two sole sources of hashed terrorist content that companies can leverage. In the case of GIFCT, the hash database is not a source of “verified” content but provides access to hashed content that another industry member has determined falls within GIFCT’s taxonomy. The hash database system relies on trust and communication among GIFCT member companies, not independent verification\\nAutomated content moderation (User to User) \\t\\tNote that CSAM AI/ML development has been hindered b/c there is no way to leverage training data without illegally possessing such content. Finally, we note that Ofcom is not currently proposing to recommend the use of AI to detect first generation harmful content. As Ofcom will be aware, the development of CSAM classifiers has previously been hindered because there is no way to leverage such content as training data without illegally possessing such content. User access to services (U2U)\\t\\tWhen Microsoft finds a CSAM sharing account they block it. Upon verification from a specialist, the account is permanently baned. Experience (over many years) has demonstrated that those that share are likely to be serial offenders. When Microsoft identifies CSAM shared through one of its hosted consumer services, we disable the user’s Microsoft account. Upon verification that the content at issue is CSAM by a trained, human review specialist, the user’s account is suspended indefinitely. Our long experience in addressing this harm has demonstrated that users who share CSAM are more than likely serial offenders. Given this and the gravity of the harm, we have taken an approach that relies on indefinite suspension, rather than blocking users for any other period of time. User access to services (U2U)\\t\\tIt is difficult to prevent suspended users from finding paths to gain access to the service again. There are a number of ways they can do this.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928284287452698,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'And to prevent it properly would involve extensive human analysis and may compromise the privacy of legitimate users. We should think carefully about what measures we implement here. However, we have found that preventing suspended account users from re-accessing services presents difficulties. These users may attempt to re-gain access in various ways, such using a different email and IP address to re-register. We have implemented processes to combat instances of recidivism, aiming to identify similarities with accounts previously closed for CSAM violation, enabling us to limit re-registration by the same account holders. However, this is not always possible. Repeat offenders can circumvent our screening processes in various ways and use fake personal/ account information to regain access. Tactics to address this problem require extensive human analysis and may compromise the privacy of legitimate users. We would encourage Ofcom to consider the practical difficulties and potential risks associated with requirements to prevent users from returning to service when formulating any future measures for blocking users that share CSAM. User access to services (U2U)\\t\\tMicrosoft notes the steps it takes when an account appears to be hosting CSAM content. It includes an appeals process. Microsoft also employs differentiated enforcement actions depending on the nature of the violative content at issue. For CSAM content, the user account is temporarily suspended while the content is reviewed by a trained, human reviewer specialist for verification. If the content is confirmed to be CSAM, the account is suspended indefinitely and the content is reported to the National Center for Missing and Exploited Children (“NCMEC”), which serves as a global clearinghouse for CyberTips, and delivers actionable reports to the local law enforcement best placed to address the crime, including in the United Kingdom. Account holders are also afforded the opportunity to appeal.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9920070171356201,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"If the user is successful, access to the account is returned to them. User access to services (U2U)\\t\\tNotes that its automated tools are accurate. But it has checks and balances to ensure non-violative content isn't actioned in error. All identified CSAM is reviewed by trained specialists, which might not be affordable for smaller companies. In our experience, the risk of erroneous identification by our automated system processes is low. We employ checks and balances to ensure non-violative content isn’t actioned in error. As indicated above, all identifications of shared CSAM (whether by automated technologies or reported by users) are immediately subjected to human review by trained specialists, who are supported by a range of wellness tools. We note that the significant investment Microsoft makes to support human review activities may not be supportable by smaller companies.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916967153549194,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Statutory Tests\\t\\tMicrosoft asks us to consider our segmentation of our measures again. Reference is made to our previous responses to Question 14 (definition of large service) and Question 15 (definition of multi-risk service) with regard to assessing the appropriateness of the Codes against the size and type of regulated service (Paragraph 1 of Schedule 4). We would encourage Ofcom to consider our suggestions in each of these responses to ensure the Codes are appropriate to the size and type of regulated service. Terms of service and Publicly Available Statements\\t\\tMicrosoft is supportive and agrees that services should be able to apply it own ToS instead. We are supportive and agree with Ofcom’s position that a regulated service can alternatively meet its duty to take action against potentially illegal content by applying its own terms of service to that content. Approach to the Codes\\t\\tThere is a significant disconnect between the causes and impacts of harms identified in the RoR and the limited set of proposals that Ofcom has put forward to address those risks such that the Codes may fall short of the stated aim of the OSA at section 1(1). Ofcom\\'s approach has produced \"a very limited set of proposals\" and risks falling \"a long way short of addressing causes and impacts of harms identified in RoR. The gaps are so large and numerous that the Codes may fall short of delivering stated aim of the OSA as set out at Section 1(1). Main concerns (1) First iteration does not have to be so unambitious. Ofcom has flagged that this set of Codes is a first iteration and it will return to them but that does not justify setting the bar so low. Ofcom\\'s approach has prioritised the interests of regulatred companies over the interests of users. the response argues that user safety should have precedence and Ofcom should have erred on side of recommending the measures necessary to tackle illegal harms and then could relax measures at a later date in time. It argues that an absence of evidence should not be treated as evidence against a measure: Ofcom should look to fill in gaps in evidence but in the meantime recommend measures of \"the balance of probabilities based on the evidence it does have\". (2) An incomplete package of measures is inconsistent with requirements of Section 41(1) of OSA. The \"safe harbour\" provision makes it critical that the first iteration of the Codes should be a comprehensive set of measures to address the full range of risks and harms identified in the RoR. It is not the intention of s41(1) that services with a high prevalence of illegal harm should enjoy a safe harbour because they follow an insufficiently stringent set of measures. (3) Following Ofcom\\'s measures should not be a tick-box exercise for platforms. There are places where Ofcom could and should have recommended additional specific measures to address a specific harm or risk factor. A signiifcant omission is not recommending platforms give users to option to verify their identity in order to address the various harms Ofcom has identified with anonymous and fake profiles. The response includes a detailed propsal to address this. The response argues that s10(4) and 236(1) allow Ofcom to make recommendations which put the onus on platforms to take their share of responsiblity for identifying and implementing steps to reduce specific risks generated by their services. Enhanced user control (U2U) \\t\\tAgree with proposals to give users the option to block & mute other users and disable comments - as far as they go (see response to Q12)\\tOfcom\\'s proposals to give users the option to bloack and mute other users, and to disable comments  would be more effective if combined with giving users the option to verify their identity, combined with an option to mute or block non-verified users as well. This would make it harder for a bloacked user to circumvent a block by creating a new account and mean that targeted users would have option to block all non-verified accounts (rather than having to play \"whach-a-mole\") \\nEnhanced user control (U2U) \\t\\tYes, there should be a requirement for how the controls are made known to users. It is crictical that users need to be aware not only of the existence of a functionality but also how it could help the user and  how to use the functionality. There is a risk platforms do not see the take-up of these measures as desirable because they could lower engagement / ad revenue. Platforms should therefore be required to take reasonable steps to educate users about these functionalities and to conduct regular assessments of user awarenes and take-up. Enhanced user control (U2U) \\t\\tMaking users aware of the distinction between \"notable\" and \"monetised\" verification schemes is a positive but limited proposal and Ofcom should go further. Making users aware of the distinction between \"notable\" and \"monetised\" verification schemes is a limited but positive proposal. Its main value would come if there was a discrepancy between the level of rigour applied to \"notable\" -v- \"monetised\" scheme and the confusion between the two would make it easier for impersonation scams.Ofcom should set minimum standards for any scheme describing itself as verification to avoid scenarios like the \"Twitter Blue\" scenario in which bad actors could obtain the crediblity of a blue tick by paying for it. Ofcom should go further to address the risks associated with anonymous and fake accounts set out in the RoR. Agrees with proposal not to recommed mandatory user verification, argues that Ofcom should have recommended a voluntary verification schemes. Cumulative Assessment\\t\\tSerious concerns that Ofcom has not advanced an overall methodology for assesing proportionality and there is too much focus on costs to businesses\\tSerious concerns about how Ofcom is choosing to define proportionality. Ofcom has not advanced an overall methodology for assesing proportionality and there is too much focus on costs to businesses and not enough focus on costs to society or hamrs to individual users. Cumulative Assessment\\t\\tReservations about Ofcom\\'s approach to proportionality and that, taken together, Ofcom\\'s proposals might not do much to improve online safety. Unsure whether overall burden is proportionate for small and microbusinesses with significant risks. Response has reservations about Ofcom\\'s approach to proportionality and that, taken together, Ofcom\\'s proposals might not do much to improve online safety. Response argues that it is entirely appropriate for businesses with more risks to have to take more measures to address those risks. Do not see any suggestion in OSA that platforms which pose a serious risk should escapte the Act merely by being small. Cumulative Assessment\\tProportionality\\tReservations that Ofcom\\'s approach to proportionality focuses on minimising costs to companies and \"proportionate\" is a synonym for \"light touch\". Unsure about proportionality about overall burden on large services. There is too much emphasis on minimusing costs to companies and, as a result, Ofcom\\'s proposals are too light touch. Response argues that proportionate should be interpreted in terms of measures being sufficient to accomplish the safety duties set out in the OSA and tackle harms and risks identified in the RoR. Response argues that measures set out for large and high risk services will not be effective so would not describe them as proportionate. Statutory Tests\\t\\tDo not agree that Ofcom\\'s proposed recommendations for the Codes are appropriate in light of the matters to which Ofcom must have regard because they will not deliver on OSA\\'s overall purpose of improving online safety. Response does not see how Ofcom\\'s proposed recommendations - taken together - will deliver overall purpose of improving online safety. Ofcom has placed too much emphasis on rights of certain users whose speech or reach might be impacted by a measure under consideration and have not done enoughto balance this against rights of other users (e.g. victims of serious crimes). Give example of anonymity: it can be a feature which is valued by vulnerable and minority grouos (as Ofcom has noted) but at the same time can also be used against those groups becasue of the harms associated with anonymous and fake accounts. Governance and accountability\\t\\tAgree with proposals, and longer term ADR could have a role\\tSupport governance as one of Ofcom\\'s strategic priorities. \"Longer term, ... expert Alternative Dispute Resolution (ADR) could be the missing piece of the puzzle ... to resolve individual disputes and also provide industry-level insights.\" Trust Alliance Group \"is developing our thinking and evidence base with regard to the provision of ADR in digital markets, and we will share this with Ofcom, if and when needed, to support a move to mandating ADR for UK digital markets.\"\\nGovernance and accountability\\t\\tBroadly agree, with one caveat\\t\"We deem the list of types of services proposed to be sufficiently comprehensive. Our only doubt relates to video chat services (e.g. Omegle) which could technically fall out of scope of any types listed, due to the phrasing used.\" [This is the full response]\\nGovernance and accountability\\t\\tWelcome third party auditing being an option, with offer of assistence if we want to consider more\\t\"We welcome that independent third-party auditing is an option for providers in ensuring that measures taken to mitigate illegal harms are effective. If a future change was being considered by Ofcom to make this a requirement rather than an option, TAG would be happy to share our experience of the Internet Commission’s work as an independent third-party auditor and provide more information to show the efficacy, costs and risks of third-party auditing.\" [This is the full response]\\nApproach to the Codes\\t\\tDefinition of large services reasonable, but devision into \\'large\\' and \\'small\\' too simplisitic and may miss some risky services\\t\" ... This binary representation of the digital landscape fails to capture and reflect the complexities of the market ... \"  \"the threshold for ‘large users’ is so high that it will not be met by companies like Roblox, which are among the largest platforms in the UK in use by children - one of the most vulnerable groups of Internet users.\"  \"... it will be important for Ofcom to ensure it has sufficient resource to adequately evaluate the risk assessments submitted by online services.\"  ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9906396865844727,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"It is possible that among the mass of smaller services, at least some may feel inclined not to identify risks ... It is not clear how this will be addressed ...\" \\nApproach to the Codes\\tSegmentation\\tMulti-risk set too low, should be at least 5 medium or high risk areas\\t\"Many ODDA members will be classified as having a risk of more than 2 listed priority harms. However, they are also small services, and they have noted the significant increase in compliance for small, multi-risk services. We would propose the threshold for being classified as a small, multi-risk service increased from at least 2, to at least 5 medium or high risk priority illegal harms. ... One ODDA member is a single person business\"\\nGovernance and accountability\\t\\tGovernance measures for small, multi-risk services are excessive\\tFor a small business, some of the requirements for small multi risk businesses are \"onerous and take significant time away from running and building a service. If the staff is 1-3 people, the same person will be responsible for all of these areas, alongside every other part of the business. There is some concern from ODDA members that high levels of governance requirements will have an impact on innovation in the dating and social discovery space, as many new services have extremely small staff and limited capacity.\"\\nContent moderation (User to User) \\t\\tContent moderation measures reasonable, and need flexibility to interpret\\t\"ODDA Members felt the Content Moderation compliance expectations were fair. However, concerns were still expressed about the ability of very small businesses to comply, as well as what the compliance standard would look like. For instance, setting performance targets for content moderation for a 1-3 person team would look very different than for a large service with a trust and safety department.\"\\nUser reporting and complaints (U2U and search) \\tReporting and complaints\\tSome concerns about reporting and complaints measures, especially regarding successful appeals of bans, where bans hard to relate to specific illegal harms\\t\"... For instance, giving indicative timeframes for considering complaints will be entirely related to the specificity of each complaint and the capacity of the business.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9599685072898865,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Particular concern ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7318350076675415,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'in relation to successful appeals. ODDA members are seeking clarification on their ability to ban accounts. Often, accounts are banned on dating services to keep the community safe, but it may be difficult for the trust and safety team to pinpoint an illegal harm that has occurred; rather the behaviour of the profile is related to known behaviour by bad actors. ODDA Members are concerned about these bans withstanding an appeal.\" Response gives an example and asks for clarity. Approach to the Codes\\tCosts\\tConcern about the cost of compliance for small businesses\\t\"There is also a concern about the cost of compliance with the regime for small businesses. Again, GDPR demonstrated the cost of compliance as having an impact\"\\nApproach to the Codes\\t\\tWant collaborative approach between Ofcom and industry, and iterative process\\t\"To develop realistic and effective best practices and guidelines, collaboration between Ofcom and industry is paramount. ODDA strongly recommends an iterative process allowing for continuous improvement, considering evolving technologies and online risks, is crucial for the development of a solid and workable regulatory framework. We look forward to ongoing collaboration to address these challenges and build a safer online environment.\"\\nGovernance and accountability\\t\\tConsumers should have a consistent experience across different types of service, not just online but also film, television etc\\tConsumers will benefit from a consistent experience across different types of service. \"We think people should have similar experiences and expectations of content protections and warnings regardless of where that content is found or the form it is in\". Applies not just social media and search, but also film, television etc\\nApproach to the Codes\\t\\tEndorse Ofcom approach, with suggestion for helping services judge content\\tIn New Zealand, have found benefits from making fast quasi-judicial decisions about what is and isn\\'t illegal. Although UK law does not provide this function, regulators can support services to quickly identify content and apply legal tests, which is especially important in \\'edge\\' cases. This can also help uphold freedom of expression by avoiding risk-adverse services taking down too much. Hash-matching can also be beneficial for content that has been judged to be legal. Approach to the Codes\\t\\tCare needed with criteria for content removal and associated metrics\\t\"Performance targets for content moderation should not stray too far from the types of factors contained in the proposals. Targets should not include, for example, amounts of content removed, or amounts of referrals or requests actioned by the service\". Also important that targets are grounded i evidence with reference to fair, evaluative criteria of risk\\nAutomated content moderation (User to User)\\t\\tInformation on what is public and private should be provided in an accessible form to the public\\tSimilar to response to Q50 of ICJG\\nAutomated content moderation (User to User)\\t\\tGIFCT hash-sharing database is effective\\tGIFCT\\'s hash-sharing database is a good example of how hash matching can be effective. \"It is best used when judicial or quasi-judicial decisions like ours are the basis for hashing and take-downs, since the appropriate legal and human rights test have already been worked through.\"\\nUser reporting and complaints (U2U and search) \\t\\tSuggest a simple, centralised method for users to navigate complaint and reporting systems\\tEven high levels of compliance from online services will mean users face many different ways of reporting content, appealing decisions and requesting information. A \\'single front door\\' would avoid systemic fragmentation. \"Such a solution should also allow services to innovate and implement complaints and reporting systems that are appropriate for them\"\\nUser reporting and complaints (U2U and search) \\t\\tThere should be some sharing of information for cross-platform abuses\\tWhen abuse happens on multiple services, victims face having to complaint to each service. This does not always provide the full picture of the scale of abuse and is onerous for complainant. Services could be required to make info supplied to them by complainants available to Ofcom and/or other services, with appropriate safeguards\\nDefault settings and user support (U2U)\\t\\tRisks with informing young people of illegal content\\t\"Service will need to strike a balance between informing young users of the risk of possessing or sharing illegal content, against the risk that by informing them of this, they then decide to seek or share this content.\" [This is the full response on this question]\\nGovernance and accountability\\t\\t\"Match Group believes that robust, transparent and accountable governance structures are vital to ensure that services put the interests and safety of users first, and we agree with the governance and \\naccountability measures proposed in the illegal content Codes of Practice\"\\t\"Match Group believes that robust, transparent and accountable governance structures are vital to ensure that services put the interests and safety of users first, and we agree with the governance and  accountability measures proposed in the illegal content Codes of Practice. We agree that all services should:\\n• Name a person accountable to the most senior governance body for compliance with illegal \\ncontent duties and reporting and complaints duties. We agree that all multi-risk services and all large services should:\\n• Have written statements of responsibilities for senior members of staff who make decisions related to the management of online safety risks. • Track evidence of new kinds of illegal content on their services, and unusual increases in particular kinds of illegal content, and report this evidence through the relevant governance channels. U2U services should also track and report equivalent changes in the use of the service for the commission or facilitation of priority offences. • Have a Code of Conduct that sets standards and expectations for employees around protecting users from risks of illegal harm.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9174340963363647,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '• Provide sufficient training in the service’s approach to compliance to staff involved in the design and operational management of the service. We agree that, for large services:\\n• The most senior body in relation to the service should carry out and record an annual review of risk management activities in relation to online safety, and how developing governance risks are being monitored and managed.\"\\nGovernance and accountability\\t\\tOfcom strikes the right balance by focusing these measures on services with the highest likelihood and potential impact of illegal harms\\tMatch Group agrees with Ofcom’s approach of applying the proposed governance and accountability primarily to large services (defined as services with greater than 7 million monthly average users in the UK) and/or multi-risk services (defined as a service with medium to risk for at least two kinds of illegal harms). While these measures are important tools to improve the safety performance of services, they often come with considerable financial costs to implement and maintain. We believe Ofcom strikes the right balance by focusing these measures on services with the highest likelihood and potential impact of illegal harms\\nGovernance and accountability\\t\\t\"We would advocate for caution when requiring companies to have their illegal content risks audited by \\nindependent third parties\"\\tWe would advocate for caution when requiring companies to have their illegal content risks audited by independent third parties. While this would theoretically hold companies accountable and increase the transparency and reliability of reports, this approach could come with significant drawbacks\\nGovernance and accountability\\t\\tCompanies themselves are most aware of the risks which are present on their platforms. In our experience, it is companies themselves which are most aware of the risks which are present on their platforms, how these risks are evolving, as well as the means of mitigating such risks which are most effective and feasible. This institutional knowledge is hard to replicate amongst third parties. Governance and accountability\\t\\t\"For companies with business models such as ours which are incentivised to invest heavily in online safety \\nfeatures or risk losing subscribers, we are directly incentivised to be at the forefront of promoting online \\nsafety. \"\\tFor companies with business models such as ours which are incentivised to invest heavily in online safety  features or risk losing subscribers, we are directly incentivised to be at the forefront of promoting online safety. Match Group is constantly evaluating safety trends, retraining algorithms, and adding new tools to promote safe online experiences, such as part of our work identifying “pig butchering” as a new type of scam and then getting educational resources out specifically targeting that type of bad actor. Governance and accountability\\t\\tThe constantly evolving nature of online harms means that auditing could become outdated rapidly.': [{'label': 'POSITIVE',\n",
       "               'score': 0.996894359588623,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Further, the constantly evolving nature of online harms, combined with the fast pace of the industry and the heterogeneity of the services within scope, means that such independent auditing could become outdated rapidly. By the time a new harm or behaviour is shared with, and understood by, a third-party, the information could already become outdated, slowing down a service’s ability to respond. This is particularly the case for easily scalable harms such as fraud or scams. Governance and accountability\\t\\tCould be significant costs associated. We also believe there could be significant costs associated with such measures, especially for smaller services if they are required to finance independent auditing on a limited budget. Governance and accountability\\t\\tMore effective for services to resource their own internal trust and safety teams. In our view, it would be more effective for Ofcom to require services to appropriately resource their own internal trust and safety teams, rather than mandating independent auditing processes. This would allow companies to identify new trends in online harms and move quickly to implement solutions. Governance and accountability\\t\\tMatch Group is not aware of additional evidence that demonstrates tying remuneration to online safety outcomes would result in improved efficacy. Match Group is not aware of additional evidence that demonstrates tying remuneration to online safety outcomes would result in improved efficacy. Governance and accountability\\t\\tThere are already overwhelming incentives for businesses to comply with their duties under the online safety regime. From our perspective, there are already overwhelming incentives for businesses to comply with their duties under the online safety regime - including fines, penalty notices and the potential suspension of the service within the United Kingdom. When businesses fail to implement online safety measures and suffer commercial consequences for this failure, business decisions will adapt to avoid such outcomes. Online safety is already at the core of Match Group’s business model, and we have organisational-wide incentives to deliver a safe and enjoyable experience for users, making the proposed measures unnecessary. Governance and accountability\\t\\tMarket forces should be the primary driver to force industry to implement measures. We therefore believe that where possible, market forces should be the primary driver to force industry actors to implement these measures. Match Group supported the adoption of the new online safety regime, and as such believes that expansion of the scope is unwarranted at this time and that expansion only be considered if adherence with the law, rules, and regulations is determined to be inadequate. Governance and accountability\\t\\tTying remuneration to safety outcomes could present a perverse incentive to businesses who choose to report and act upon less instances of potential harm than others. Most importantly, we would note that tying remuneration to safety outcomes could present a perverse incentive to businesses who choose to report and act upon less instances of potential harm than others. Internal transparency is critical to improve long-term safety outcomes, and short-term self-interest may undermine this. For example, if a service’s performance was crudely tied to a low number of reported online harms, senior executives would be directly incentivised to underestimate the incidence of harm on their services. This would send the opposite signal which Ofcom intends. Governance and accountability\\t\\t\"Setting specific targets may also fail to consider the nuances of content moderation decisions, particularly in \\ncases regarding specific uses of language.\"\\tSetting specific targets may also fail to consider the nuances of content moderation decisions, particularly in cases regarding specific uses of language. While the usage of countless words and phrases are clear and without any subtext, language is dynamic with slang and new uses of words or phrases constantly occurring.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9971575736999512,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is especially true for younger demographics. For example, a term which might normally be construed as an insult may become fashionable to be used in a different context. Governance and accountability\\t\\tUser bans or the taking content down may not be the most effective or appropriate approach. In these cases, user bans or the taking content down may not be the most effective or appropriate approach. Arbitrary performance measurements are not able to account for nuances and the dynamic use of language and could result in overbanning and inappropriate banning of individuals. Remuneration would therefore be an inartful tool to encourage greater user safety and the free flow of dialogue. We believe there is greater merit in relying on the measures already in place within the online safety regime and the internal and external factors which steer regulated services towards better outcomes. Approach to the Codes\\t\\t\"We believe that Ofcom has significantly underestimated the overall cost to build and implement online \\nsafety features.\"\\tWe believe that Ofcom has significantly underestimated the overall cost to build and implement online safety features such as CSAM detectors, as well as running maintenance costs. We would note that projected engineering costs are lower than the costs that we have experienced when developing our own systems, and that trust and safety features are materially more expensive that Ofcom’s estimates. Content moderation (User to User) \\t\\t\"Match Group agrees that as a baseline, all services should have systems or processes designed to swiftly \\ntake down illegal content of which it is aware.\"\\t\"Match Group agrees that as a baseline, all services should have systems or processes designed to swiftly take down illegal content of which it is aware. We also largely agree that all multi-risk services and all large services should:\\n• Set and record internal content policies. • Set and record performance targets for its content moderation functions and measure and monitor its performance against these targets. Ofcom proposes these should include targets for both how quickly illegal content is removed and for the accuracy of content moderation decisions. When setting performance targets, services should balance the need to take illegal content down swiftly against the need to make accurate moderation decisions. • Prepare and apply a policy about the prioritisation of content for review. This policy should have regard to at least the following factors: virality of content, potential severity of content, and the likelihood that content is illegal, including whether it has been flagged by a trusted flagger. • Resource its content moderation function so as to give effect to its internal content policies and performance targets. • Ensure people working in content moderation receive training and materials that enable them to moderate content effectively.\"\\nContent moderation (User to User) \\t\\tMatch Group takes the safety of its users very seriously and commits to maintaining the highest standards. Match Group takes the safety of its users very seriously and commits to maintaining the highest standards. The proposed measures represent sensible, best-practice content moderation systems that, if delivered effectively, will have a meaningful impact on online safety. Our content moderation systems use a combination of human and machine-assisted tools to detect key words, phrases, or comments, that are evaluated by humans before being removed or approved to remain on the service. We prioritise content which would fall into Ofcom’s bracket of illegal harms, though we generally see very little of such content due to our existing trust and safety features. Importantly, these proposed measures also largely align with and complement the terms of the Online Fraud Charter. Content moderation (User to User) \\t\\t\"While we agree with the importance of setting and measuring delivery against performance targets for \\ncontent moderation functions, it is important Ofcom avoids setting overly prescriptive requirements about \\ntargets.\"\\tWhile we agree with the importance of setting and measuring delivery against performance targets for content moderation functions, it is important Ofcom avoids setting overly prescriptive requirements about targets, because, as discussed above, this could lead to perverse incentives and companies actually reducing their safety measures (e.g., to reduce the number of reports, a company could make the reporting feature harder to find). Content moderation (User to User) \\t\\t\"Content moderation decisions are generally made in low-information environments, and each service will \\nweigh decision accuracy vs safety differently.\"\\tContent moderation decisions are generally made in low-information environments, and each service will weigh decision accuracy vs safety differently. As Ofcom’s Consultation Annex 10 notes, content may be removed for breaching a service’s Terms of Service well before a formal judgement has been made on whether it meets the criteria of illegal content. Performance targets specifically relating to illegal content may not accurately reflect the performance of services that have a much lower threshold for taking action against content that does not meet the “illegal” threshold but is nonetheless deemed inappropriate to the platform by the service provider. Given this, Ofcom must also be conscious of creating unintended incentives that encourage services to only take action against content or behaviour that meets that higher bar of “illegal content” relative to other harmful behaviour. Automated content moderation (User to User)\\t\\t\"Match Group agrees with Ofcom’s proposal that certain types of service should use an automated \\ntechnique known as hash matching, or other techniques which are equal or greater in recall, to analyse \\nrelevant content to assess whether it is CSAM and should take appropriate measures to swiftly take down \\nCSAM detected.\"\\t\"Match Group agrees with Ofcom’s proposal that certain types of service should use an automated technique known as hash matching, or other techniques which are equal or greater in recall, to analyse relevant content to assess whether it is CSAM and should take appropriate measures to swiftly take down CSAM detected. This measure would apply to:\\n• large services which are at medium or high risk of image-based CSAM;\\n• other services which are at high risk of image-based CSAM and have more than 700,000 monthly UK users;\\n• services which are at high risk of image-based CSAM AND which are file-storage and filesharing \\nservices that have more than 70,000 monthly UK users.\"\\nAutomated content moderation (User to User)\\t\\t\"We also agree certain types of service should use an effective technique of URL detection to analyse \\nrelevant content to assess whether it consists of or includes a CSAM URL and should take appropriate \\nmeasures to swiftly take down those URLs detected.\"\\t\"We also agree certain types of service should use an effective technique of URL detection to analyse relevant content to assess whether it consists of or includes a CSAM URL and should take appropriate measures to swiftly take down those URLs detected. This measure would apply to:\\n• large services which are at medium or high risk of CSAM URLs;\\n• other services which are at high risk of CSAM URLs and have more than 700,000 monthly UK \\nusers.\"\\nAutomated content moderation (User to User)\\t\\tLarge services who are at medium/high risk of fraud use standard keyword detection technology to identify content. We also agree that large services which are at medium or high risk of fraud use standard keyword detection technology to identify content that is likely to amount to a priority offence concerning articles for use in frauds (such as content which offers to supply individuals’ stolen personal or financial credentials) and consider detected content in accordance with their internal content moderation policy. Automated content moderation (User to User)\\t\\t\"Match Group believes that running CSAM hashes to detect known CSAM images is a baseline and relatively \\neasy measure to implement for any service that allows photos.\"\\t\"Match Group believes that running CSAM hashes to detect known CSAM images is a baseline and relatively easy measure to implement for any service that allows photos. The National Centre for Missing & Exploited Children (NCMEC) and Thorn, both partners on the Match Group Advisory Council, have inexpensive technology that is highly effective at finding known CSAM, which our brands have implemented. They also have tools that may be effective, depending on the company and service, at finding grooming or other illegal behaviours (we have found them not to be effective on our services). Recognizing that what is effective for one company may not work for another, or that publicly available tools may not be suitable for a particular service is a key aspect of a workable regulatory scheme, as OFCOM has recognized. Thus, while not using certain publicly available tools, Match has deployed internal tools such as photo review tools or keyword detection at scale, with robust lists of key terms for detection that expand as our awareness of new and emerging online harms grows. We would also note that we generally see very little CSAM material on our services due to the existing trust \\nand safety features on our services, which disincentivise bad actors from using our platforms to find or share CSAM\"\\nAutomated content moderation (User to User)\\t\\tMatch Group agrees with Ofcom\\'s approach. \"Match Group agrees with Ofcom’s approach to assessing whether content is communicated ‘publicly’ or  ‘privately’ based on the three factors included in the Online Safety Act:\\n• the number of individuals in the United Kingdom who are able to access the content by means of the service;\\n• any restrictions on who may access the content by means of the service (for example, a \\nrequirement for approval or permission from a user, or the provider, of the service);\\n• the ease with which the content may be forwarded to or shared with: (i) users of the service other than those who originally encounter it; or (ii) users of another internet service.\"\\nAutomated content moderation (User to User)\\t\\t\"We have implemented perceptual hash matching and we do not believe that we have seen any \\nfalse positives. However, we also do not see many true positives, given the nature of our service \\ndoes not lend itself to the sharing of CSAM.\"\\t\"We have implemented perceptual hash matching and we do not believe that we have seen any \\nfalse positives. However, we also do not see many true positives, given the nature of our service  does not lend itself to the sharing of CSAM.\"\\nUser reporting and complaints (U2U and search)\\t\\tMatch Group agrees with some of the proposal. \"Match Group agrees with the proposal that all services must:\\n• Have complaints processes which enable UK users, affected persons and (for search services where relevant) interested persons, to make, for example, each of the below type of complaint:\\n• Complaints about the presence of illegal content; appeals where content may have been \\nincorrectly identified as illegal; complaints about reporting function; complaints about a \\nservice not complying with its duties; and complaints about the use of proactive technology \\nin a way that is inconsistent with published terms of service;\\n• Have an easy to find, easy to access and easy to use complaints system including:\\n• Easily findable and accessible content reporting tools and ways to make other kinds of \\ncomplaint; as few steps as reasonably practicable to make a complaint; ability for UK users to \\nprovide context/supporting material; and information and processes to be accessible and \\ncomprehensible, including having regard to users with particular accessibility needs such as \\nchildren (if children use the service) and those with disabilities. • Acknowledge receipt of each relevant complaint with indicative timeframes for deciding the \\ncomplaint\\nWe also agree that all large services with a medium or high risk of fraud should establish and maintain a dedicated reporting channel for fraud for trusted flaggers across Government and law enforcement.\"\\nUser reporting and complaints (U2U and search)\\t\\tMatch Group believes that a simple and effective reporting function is a bare minimum. Match Group believes that a simple and effective reporting function is a bare minimum needed to provide an online platform, and we are proud to have an easily accessible and easy-to-understand reporting system that was informed by collaboration between third party experts and internal teams. Collaboration between public and private sectors is vital to improve online safety, and we are proud to already have a proactive working relationship with both the Financial Conduct Authority (through the Digital Regulation Cooperation Forum) and with the City of London Police in our work on preventing financial fraud. We welcome measures that foster greater cooperation and action across the wider digital ecosystem. Importantly, the reporting and government cooperation measures proposed by Ofcom align with the terms of the Online Fraud Charter, of which we were a signatory and contributed our views on combating online fraud. Terms of service and Publicly Available Statements\\t\\tMatch Group agrees with the proposal for all services to ensure that the provisions in their Terms of Service are easy to find. \"Match Group agrees with the proposal for all services to ensure that the provisions included in their Terms of Service (ToS)/Publicly Available Statements (PAS) are easy to find, in that they are:\\n• clearly signposted for the general public,\\n• locatable within the ToS/PAS,\\n• laid out and formatted in a way that helps users read and understand them,\\n• written to a reading age, comprehensible for the youngest person permitted to agree to them; \\nand\\n• designed so people dependent on assistive technologies can access them.\"\\nTerms of service and Publicly Available Statements\\t\\tWe agree that Terms of Service for platforms should provide easily accessible information about services’  policies and duties regarding illegal content, and that users are able to make informed decisions about  whether to use a platform based on the level of comfort and safety they feel\\t\"We agree that Terms of Service for platforms should provide easily accessible information about services’ policies and duties regarding illegal content, and that users are able to make informed decisions about whether to use a platform based on the level of comfort and safety they feel. However, we would advocate for caution when sharing specific information about our trust and safety tools and processes with the public, as providing too much information will improve the ability of bad actors to circumvent these features. In addition, as services like Match Group’s are constantly evolving their technology and processes, requiring sharing this detailed information would put a significant burden on companies to constantly update their Terms of Service. While Match Group strives to provide accessible and simple informational resources, these are ultimately left unread by a vast majority of users, indicating these proposed measures may have less urgency or safety impact than other measures included in the draft Codes of Practice.\"\\nTerms of service and Publicly Available Statements\\t\\tPrompts serve as a useful alternative measure to build understanding and shape user behaviour. Prompts serve as a useful alternative measure to build understanding and shape user behaviour.Match Group is proud of its ‘Are You Sure?’ feature (AYS?), first introduced to Tinder in 2021. This automated system detects potentially offensive language and asks users to reconsider sending the message. If the message is sent as first flagged, the system asks the recipient whether the message is appropriate.The feature was shown to be effective in reducing potential harm to users, and subsequently rolled out across our other platforms. This is a useful example of where best practices on one service were then mirrored on other Match Group platforms, and how we continuously innovate and build upon our leading trust and safety practices. Tinder recently released its new ‘warnings’ feature, which are classified into three categories: authenticity, respectfulness, and inclusiveness. They cover in-app behaviour when engaging with other users, such as harassment, and protect users against advertising and impersonation. The introduction of warnings builds on Tinder’s trust and safety commitment and will go live globally in the coming weeks.AYS? has since expanded to Hinge, Plenty of Fish and other services and more than 500 million messages have either been edited or deleted by users after the system flagged that their message might include offensive, harassing or potentially problematic language. We noted that on Plenty of Fish, there has been an 84% decrease in the feature being triggered, which indicates that the tool is effectively guiding users to learn from their behaviour. The tool is continually evolving to flag new words, phrases and behaviours in an effort to future-proof the system, and it is now available in 18 languages worldwide. Default settings and user support (U2U)\\t\\t\"Match Group agrees that services should implement default settings for child users and relevant \\nfunctionality.\"\\t\"Match Group agrees that services should implement default settings for child users and relevant \\nfunctionality:\\n• Children using a service are not presented with prompts to expand their network of friends, or \\nincluded in network expansion prompts presented to other users. • Children using a service are not included in publicly visible lists of who users are connected to, and  lists setting out who child users are connected to are not displayed to other users.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9961540102958679,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '• Where services have functionality which allows users to formally connect with one another (e.g. become ‘friends’) they should ensure that people cannot send direct messages to children using the service without first establishing such a connection. • For services with no user connection functionality, child users are provided with a means of \\nactively confirming whether to receive a direct message from a user before it is visible to them, \\nunless direct messaging is a necessary and time critical element of another functionality, in which case child users should be presented with a means of actively confirming before any interaction associated with that functionality begins. • ‘Automated location information displays’, which automatically create and display the location \\ninformation for child users, are switched off.\"\\nDefault settings and user support (U2U)\\t\\tWe also agree with the proposal for services to provide supportive information to children using a platform in a timely and accessible manner. \"We also agree with the proposal for services to provide the following supportive information to children using a platform in a timely and accessible manner:\\n• Seeking to disable one of the default settings recommended. • Responding to a request from another user to establish a formal connection.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9907627701759338,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '• Receiving a direct message from another user for the first time. • Taking action against another user, including blocking and reporting.\"\\nDefault settings and user support (U2U)\\t\\t\"Match Group has a longstanding and highly public commitment to ensure that child safety and protection \\nare our utmost priorities, and our group platforms do not allow underage individuals to create and maintain \\nprofiles.\"\\tMatch Group has a longstanding and highly public commitment to ensure that child safety and protection are our utmost priorities, and our group platforms do not allow underage individuals to create and maintain profiles. Our focus is on preventing children from accessing our platforms to begin with, working with policy makers to further mandate wider ecosystem partners to invest in this. We use a combination of technology and human moderators to monitor our platforms to prevent underage users creating accounts, detect suspicious profiles and take swift action to remove them. We also rely on other users to report suspicious or concerning profiles. In this context, none of our functionalities are designed with child users in mind or to help child users make informed choices about risk. We do not, therefore, believe we can provide insights on creating default settings for child users as we do not intend to have child users on any of our services. Default settings and user support (U2U)\\t\\t\"Match Group believes the most efficient means of preventing minors from accessing adult-only apps such as ours is to check users’ age within the distribution layer of the digital ecosystem (such as an app store) and \\nprevent access at this stage.\"\\tMatch Group believes the most efficient means of preventing minors from accessing adult-only apps such as ours is to check users’ age within the distribution layer of the digital ecosystem (such as an app store) and prevent access at this stage. This would provide two layers of age-gating to protect children: one at the distribution layer, maintained by the distributor of the service, and one on the service itself, maintained by the service developer. The Online Safety Act requires Ofcom to conduct a review into app stores’ role in children accessing harmful content online, and we believe this is critical. We look forward to Ofcom beginning its report into app stores as soon as possible. We do not seek to absolve ourselves of responsibility by forcing app distributors to bear this burden – instead, we propose a greater level of cooperation between service providers and service distributors. We have always acknowledged our responsibility to our users, and we want all service providers to do the same – especially where they have the resources and information needed to take decisive action for the benefit of millions of underage users. Recommender system testing (U2U)\\t\\tMatch Group agrees. Match Group agrees with the proposal for services which already carry out on-platform testing of  recommender systems that upon identifying as medium or high risk for at least two specified harms. Services should, when they undertake on-platform tests, collect safety metrics that will allow them  to assess whether the changes are likely to increase user exposure to illegal content\\nRecommender system testing (U2U)\\t\\tThis provision wouldn\\'t apply to Match Group services. As this measure would only apply to recommender systems that are used for the curation of user-generated content feeds, such as newsfeeds and reels on certain services, this provision would not apply to Match Group’s services. We are therefore not able to provide insight into the expected efficacy of this proposal. Enhanced user control (U2U)\\t\\tMatch Group generally agrees. Match Group generally agrees with the proposal for all large services with user accounts that identify as medium to high risk for any of the specified harms listed should offer every registered user options to block or mute other user accounts they are connected with on the service\\nEnhanced user control (U2U)\\t\\tBlanket rule requiring services to provide options to block some or all unconnected users doesn\\'t recognise the differences in what services provide. However, while we agree that providing an option to block all non-connected users may be appropriate for some services or user demographics, a blanket rule requiring services to provide options to block some or all un-connected users does not recognise the fundamental differences in what services provide. In particular, finding non-connected users is the only function of dating services, so a requirement to block that functionality would effectively render the service useless. Enhanced user control (U2U)\\t\\tMatch Group generally agrees that all large services with medium/high risk should allow registered users to disable comments on their posts. \"We also generally agree that all large services that identify as medium or high risk for any of the specified harms listed and which enable users to comment on content should:\\n• Offer every registered user the option of disabling comments on their own post.\"\\nEnhanced user control (U2U)\\t\\tBlock, report, and disable comment mechanisms are commonly expected features for users on many major online services. Block, report, and disable comment mechanisms are commonly expected features for users on many major online services. Match Group implements a block and report function in all of our services and encourages our users to block those who make them feel afraid or uncomfortable in any way and to provide us with information through our easy-to-access reporting tools. This allows users to tailor their interactions to provide the safest and most enjoyable experience possible. However, our systems could not work if we were required to present an option to block all non-connected users--by its very nature, users join our dating services to find people that they are not already connected with, and this feature would thereby defeat the purpose of joining the service. In practice, users are blocking un-connected users every time they decide not to match. We would therefore caution against Ofcom requiring this feature for all services, and we advocate for Ofcom considering the best means for services to fulfil their obligations while retaining their core functionalities. Similarly, it will be important to consider whether the requirement to provide options to disable comments on posts can appropriately be applied to a broad range of services. Enhanced user control (U2U)\\t\\tIf requirements are established, they should be flexible to cover a range of user system designs and functionalities and also respond to potential evolutions. As mentioned above, these two measures are already widely expected features by users on social networks and other online platforms. We are not aware of any evidence suggesting it is necessary to establish specific requirements for how these controls are made known to users. In the event these requirements are established, they would need to be flexible enough to cover the wide range of user system designs and functionalities of a broad range of services, while also responding to potential evolutions in technology and user behaviour. We therefore believe it would be valuable for Ofcom to take input from services themselves as to how these measures are made known to users, such as through transparent and easily-accessible Terms of Service, in-app pop-up messages, or other means\\nEnhanced user control (U2U)\\t\\tVoluntary verification schemes can result in meaningful improvements however it is critical that users understand what is being verified and what it means. We agree that voluntary verification schemes can result in meaningful improvements to safety and the perceived safety of a platform. We do believe, however, that it is critical to ensure users understand exactly what is being verified, and what the verification means. For example, we have been very careful to inform users that our “selfie verification” just verifies that the person in the pictures looks like the person who controls the account. It doesn’t verify their name, age, or any information contained in their bio. We believe it is important for users to understand what selfie verification does and does not mean in an effort to encourage our users to stay vigilant in regards to their own safety. Enhanced user control (U2U)\\t\\tSelf-verification should be introduced in a nuanced manner as difficult processes could mean less users make use of it. We would also note that self-verification should be introduced to services in a nuanced manner. If verification is a difficult or proscriptive process, less users will use it and this defeats the purpose of the feature. On our services, selfie verification helps to confirm that the account owner is the person in their profile picture. Users who verify their profile get a blue check mark and are more likely to match with others. Tinder also allows users to apply a filter so that they only see profiles of other verified users. Additionally, photo-verified members can ask their match to do the same before chatting with them, allowing users a greater level of control over how they interact with others.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9884769916534424,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We believe this empowerment of our users is greatly beneficial. User access to services (U2U)\\t\\t\"Match Group agrees with the proposal requiring all services to remove a user account from the service if \\nthey have reasonable grounds to infer it is operated by or on behalf of a terrorist group or organisation \\nproscribed by the UK Government.\"\\tMatch Group believes that sharing terrorist or CSAM material is always unacceptable and we operate a zero tolerance policy blocking users that share this content. While Ofcom has only referenced CSAM and terrorist material as content that would result in a service blocking users and preventing them from returning to a service in relation to CSAM and terrorist material, Match Group pursues this action for a much broader range of harms. Users engaged in fraudulent activity, for example, are subject to bans, and we note that the Online Fraud Charter commits signatories to “maintain effective processes to block users from creating accounts when they have previously been removed for fraud”\\nUser access to services (U2U)\\t\\t\"The identifiers used to block and prevent user return to a service will vary depending on the information \\nnecessary to create accounts on services. Match Group services primarily rely on emails and phone numbers \\nas identifiers.\"\\tWe would note that phone numbers are often recycled, and this must be taken into account regarding establishing longer term trust and safety actions. Additionally, mobile phone providers now commonly recycle and repurpose / recondition mobile phones and resell them back into the marketplace. If a block is made on the device level, the block stays active on the device and therefore when it is resold the block is still active. Since phones are generally resold to consumers who either chose to buy a reconditioned phone or buy one due to economic conditions, permanent blocks may result in users not being able to use their phone in the same manner as users that have the means to exclusively purchase new phones.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9899472594261169,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As a result, companies need to put processes in play to roll off bans after a certain amount of time. However, with serious bans, such as for terrorism or CSAM, we believe a relatively long period (10-15 years) is appropriate for data retention, and during that time a service could handle any issues on a case by case basis\\nUser access to services (U2U)\\t\\tThere are new, more effective measures, such as biometric identifiers. The limitations of relying on frequently used identifiers that are easy to circumvent, such as phone numbers and emails. underline the importance of new, more effective measures that are emerging, such as biometric identifiers. User access to services (U2U)\\t\\tSee response to Question 41.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9473265409469604,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See response to Question 41. User access to services (U2U)\\t\\tWe acknowledge the potential risk that a user could be erroneously banned for life by an automated system misdiagnosing CSAM content. We do acknowledge the potential risk that a user could be erroneously banned for life from a service by an automated system misdiagnosing content as CSAM, which is why we rely on a combination of both human and automated moderation to ensure we strike the right balance between banning unacceptable content and ensuring we catch the right content. However, given the severity of an issue like CSAM, along with our stance that neither nudity nor children are allowed on our services, we feel that we can be very aggressive in banning the limited number of users that upload items flagged as CSAM by our automated systems\\nStatutory Tests\\t\\t\"We broadly agree that the codes reflect Ofcom’s duties under the Online Safety Act and the \\nCommunications Act.\"\\tWe agree with the appropriateness of provisions of the Codes and their alignment with the obligations for different kinds and sizes of services within the broader online safety regime. We do not believe it is appropriate for us to comment on the duties relating to search services. Overall, we are supportive of the Online Safety Act and Ofcom’s proposed regime. While we have provided comments here on certain aspects which we believe could be simplified, removed or changed slightly, we are supportive of this legislation and the subsequent regulation. We hope to continue providing our input where possible as the online safety regime continues to evolve. Governance and accountability\\t\\tLinkedIn agreed with Ofcom\\'s proposals. \"LinkedIn agrees with Ofcom’s proposals. The proposed governance and accountability measures \\nset forth in the Codes of Practice (i.e., annual reviews of risk management activities and internal \\nmonitoring and assurance functions) serve as an effective means by which regulated services can \\nhelp manage risk over time.\"\\nGovernance and accountability\\t\\tGovernancy and accountability measures proposed are appropriate for large and multi-risk services. \"The governance and accountability measures currently proposed are appropriate for both large \\nand multi-risk services. If Ofcom proposes any additional measures going forward, it should \\ncontinue to carefully assess if applying such measures to large services as a whole will be proportionate, targeted and appropriately risk-based, as Ofcom has done in the current  consultation materials. \"\\nGovernance and accountability\\t\\tLinkedIn appreciates the diligent and measured approach Ofcom is taking in gathering evidence of the efficacy, costs and risks associated with imposing such a third part audit requirements on regulated services before imposing any such requirements. \"LinkedIn appreciates the diligent and measured approach Ofcom is taking in gathering evidence of the efficacy, costs and risks associated with imposing such a third part audit requirements on \\nregulated services before imposing any such requirements.\"\\nGovernance and accountability\\t\\tBaseline standards and success measures for independent third-party audits of online platforms\\' risk assessment and mitigation efforts have yet to be fully developed and tested. Baseline standards and success measures for independent third-party audits of online platforms\\'  risk assessment and mitigation efforts have yet to be fully developed and tested. As Very Large Online Platforms and Search Engines undergo their first external audit under Article 37 of the Digital Services Act (“DSA”), we encourage Ofcom to monitor and learn from the successes and shortcoming of that process. Specifically, we recommend Ofcom consider whether application of traditional financial accounting audit processes and frameworks are the appropriate fit. It is, of course, imperative that any third party audit requirement potentially implemented be harmonized with the audit requirement in the Digital Services Act. But it may prove true that the application of more traditional “audit” processes designed to measure enterprise risk (i.e., risk to a business and its owners) is a more appropriate fit in both settings to evaluate services’ measures to mitigate online harms associated with illegal content. Governance and accountability\\t\\tIf Ofcom proposes additional measures on point in future guidance, we strongly recommend that such measures focus on company-wide responsibility.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9749412536621094,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Doing so will better ensure the right level of corporate incentives and help avoid unintended behaviours or outcomes. If Ofcom proposes additional measures on point in future guidance, we strongly recommend that such measures focus on company-wide responsibility. Doing so will better ensure the right level of corporate incentives and help avoid unintended behaviours or outcomes. Approach to the Codes\\t\\tOverall, the proposed Codes of Practice are very well done. \"Overall, the proposed Codes of Practice are very well done. LinkedIn appreciates the thought \\nOfcom has put into framing them as one streamlined set of Codes to avoid repetition and \\nconfusion. We are also encouraged by Ofcom’s continued recognition that the Codes should not \\ntake a “one-size-fits-all\"\" approach. \"\\nApproach to the Codes\\t\\tLinkedIn generally agrees. \"LinkedIn generally agrees, but as noted above, we encourage Ofcom to remain open to the \\npossibility that large services can present lower risks of harm in certain areas such that some \\nrequirements may be disproportionate or unreasonable for them to apply.\"\\nApproach to the Codes\\t\\t\"Although LinkedIn does not dispute whether it should be classified as a large service for purposes \\nof the proposed Codes, given Ofcom aims to impose reasonable and proportionate obligations on \\nregulated services, we note that the definition’s current userbase threshold seems low and likely \\nto result in overinclusion.\"\\tAlthough LinkedIn does not dispute whether it should be classified as a large service for purposes of the proposed Codes, given Ofcom aims to impose reasonable and proportionate obligations on regulated services, we note that the definition’s current userbase threshold seems low and likely to result in overinclusion. Approach to the Codes\\t\\t\"We recommend that Ofcom (1) further refine the definition of “multi-risk” services to prevent it \\nfrom being overly inclusive or disproportionately burdensome, and (2) continue to carefully assess \\nwhether measures should be applied based on the existence of specific risks or a service’s status \\nas multi-risk.\"\\tWe recommend that Ofcom (1) further refine the definition of “multi-risk” services to prevent it from being overly inclusive or disproportionately burdensome, and (2) continue to carefully assess whether measures should be applied based on the existence of specific risks or a service’s status as multi-risk. As currently framed, a service with any minimal evidence of only two types of harms occurring on its service could be treated the same as a service that has ample evidence of the extensive occurrence of all 15 of the illegal harms. Additionally, the current risk classification model overlooks the fact that the 15 illegal harms are not equal in severity. To apply a more risk-based, proportional approach, we recommend that Ofcom revise the definition of multi-risk to only include services with evidence of the material presence of at least four or five illegal harms. Ofcom should also consider incorporating into the “multi-risk” classification process a weighing of additional criteria like service characteristics and the relative severity of the harms at issue. Approach to the Codes\\t\\t\"Although the proposed Codes are generally reasonable, clear and digestible, as noted below, \\nthere are several aspects of the Codes that require further attention.\"\\t\"Although the proposed Codes are generally reasonable, clear and digestible, as noted below, \\nthere are several aspects of the Codes that require further attention.\"\\nContent moderation (User to User) \\t\\tLinkedIn agrees with Ofcom\\'s proposals. \"LinkedIn agrees with Ofcom’s proposals. Measures like those proposed by Ofcom have been \\neffective in helping LinkedIn mitigate risk on our platform.\"\\nUser reporting and complaints (U2U and search)\\t\\tLinkedIn generally agrees. \"The following response contains CONFIDENTIAL INFORMATION that bad actors may try to \\nleverage to abuse our systems and content moderation processes. Please do not disclose or \\npublish. Although we generally agree with Ofcom’s proposals, we note that Ofcom expects logged-in and logged out users to have the same reporting experiences. However, for anti-abuse reasons, such flows may need to differ. In our experience, logged-out reporting flows are common abuse vectors used by bad actors to flood LinkedIn’s customer support and moderation teams. Accordingly, services like LinkedIn may need to structure such logged-out reporting flows to include additional anti-abuse features. Ofcom also has specified that reporters should be able to submit additional supporting materials to contextualize their complaints. Before requiring services to accept free form text and files as part of in-product content reporting flows, we strongly encourage Ofcom to gather additional evidence about what the actual benefit of imposing such a requirement will be as compared against the operational inefficiencies and potential abuse vectors do so might cause. \"\\nTerms of service and Publicly Available Statements\\t\\tLinkedIn generally agrees. \"Generally, we agree with Ofcom’s proposals. However, requiring regulated services to publicly \\ndescribe any proactive technology used to comply with the illegal content safety duties (including the kind of technology, when it is used, and how it works) do not allow sufficient flexibility for regulated services to balance the level of detail with the need to prevent abuse. At a certain level of detail, bad actors are able to learn how to circumvent defences because they know too much about how those defences work\"\\nRecommender system testing (U2U)\\t\\tLinkedIn agreeds but any related obligations should be risk proportional. Although LinkedIn agrees that services should consider and test the safety implications of changes to their recommender systems, any related obligations to do so under the Codes should be risk proportional. Given services continually make minor changes to their recommender systems, including small A/B tests that are not ultimately deployed, we recommend annual, semi-annual or quarterly assessment of safety metrics of what has actually been deployed. Recommender system testing (U2U)\\t\\t\"There are various features and parameters that services can use or offer in recommender systems \\nto potentially improve user safety, in connection with illegal or otherwise harmful content.\"\\t\"The following response contains CONFIDENTIAL INFORMATION that bad actors may try to \\nleverage to abuse our systems and content moderation processes. Please do not disclose or \\npublish. There are various features and parameters that services can use or offer in recommender systems to potentially improve user safety, in connection with illegal or otherwise harmful content. For example, if a piece of content mocks or ridicules others or is otherwise derisive in tone, platforms can elect not to broadly distribute the content to others, in an effort to both protect viewing members and to disincentivize future similar content. Also, platforms can utilize objective, explicit negative member feedback (e.g., reports, “I’m not interested” signals, etc.) as proxies for content that may be unsafe for or unwanted by other members. Such features and parameters should be used in a manner that accounts for potential manipulation of such feedback by bad actors (e.g., “heckler’s veto”). \"\\nEnhanced user control (U2U)\\t\\tLinkedIn agrees with Ofcom\\'s proposals. \"LinkedIn agrees with Ofcom’s proposals. In-product controls like those proposed by Ofcom are \\nimportant user empowerment tools that help a service’s users further shape their experience on  the service. Paired with effective content moderation systems, they can help provide users a \\nsafer experience on platform.\"\\nEnhanced user control (U2U)\\t\\tYes. Yes. If framed properly, voluntary verification systems can be strong user empowerment tools. Specifically, they can provide a service’s user base highly valuable authenticity signals to help such users make more informed decisions about what content and individuals they engage with online. For example, LinkedIn is a real-identity online service for professionals to connect and interact with other professionals, learn, hire, and find jobs. Our members look to engage with real people and not with fake accounts, bots, or other inauthentic actors. Accordingly, to complement LinkedIn’s robust proactive fake account detection and removal measures, LinkedIn has been rolling out a range of free verification features during the past year, These features allow our members to verify certain information about themselves, like their association with a particular company or educational institution or their identity (using a valid government-issued ID). Once a member has successfully verified information about themselves, a verification badge will appear on the member’s profile. The badge will be visible on the platform and other members can click on the badge to find out additional basic details about what information the member has verified and when they did so.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9914141297340393,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Member can manage or delete their verifications at any time by going to their settings. User access to services (U2U)\\t\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM.\"\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM, including for example a layered use of various \\ndetection technologies and human review. Additional examples of these measures and \\nsafeguards were recently listed in the European Commission’s December 2023 report on the \\nimplementation of Regulation (EU) 2021/12324.\"\\nUser access to services (U2U)\\t\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM.\"\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM, including for example a layered use of various \\ndetection technologies and human review. Additional examples of these measures and \\nsafeguards were recently listed in the European Commission’s December 2023 report on the \\nimplementation of Regulation (EU) 2021/12324.\"\\nUser access to services (U2U)\\t\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM.\"\\t\"Regulated services can use various checks and balances to mitigate the risk that non-violative \\ncontent will be erroneously actioned as CSAM, including for example a layered use of various \\ndetection technologies and human review. Additional examples of these measures and \\nsafeguards were recently listed in the European Commission’s December 2023 report on the \\nimplementation of Regulation (EU) 2021/12324.\"\\nCumulative Assessment\\t\\t\"The proposed Codes, as currently drafted, appear to impose a proportionate burden on large \\nservices. \"\\tThe proposed Codes, as currently drafted, appear to impose a proportionate burden on large services. That said, Ofcom has called out that “all else being equal,” the benefits of a measure will be greater when applied to services with larger user bases. While LinkedIn does not dispute that in principle, we have found that is not always the case and that in practice, all else is seldom truly equal even among services with seemingly comparable functionalities. Cumulative Assessment\\t\\tA service’s risk profile can be shaped significantly by characteristics other than size. A service’s risk profile can be shaped significantly by characteristics other than size — like its purpose, target audience, and whether the service requires users to log in and operate under their real identity. Given these characteristics could render a service with a large user base relatively lower risk5, as and when Ofcom imposes additional obligations on services via the Codes going forward, we encourage Ofcom to use a more flexible and proportionate risk-based approach. Statutory tests\\t\\tGenerally, we agree. \"Generally, we agree.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916600584983826,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As noted in our responses to 47 above, a proportionate, risk-based and \\ntargeted approach helps ensure that regulated services’ differing risk profiles are taken into \\naccount, including the recognition that size alone does not determine risk level. \"\\nApproach to the Codes\\t\\tOfcom continues to pretend that E2EE can exist alongside ‘appropriate measures’ whilst holding a specific person in a company accountable for “illegal content duties” which are clearly not executable when using E2EE. In effect this is removal of E2EE from service provisions due to holding a specific person accountable to an as yet undefined set of standards. Investors pulling out due to perceived risks of OSA. CRITICAL that OFCOM brings forward similar guidance on E2EE in patient-clinician confidentiality (like Australia has done). Approach to the Codes\\t\\tThere is insufficient clarity on when/how smaller organizations will need governance, when this will occur or to what standards. A perspective on the peer to peer nature of E2EE and what this means when one of the peers is a Trusted person in the context of technology use is needed. OFCOM could make statements in this regard that would open this pathway up for sustained innovation, investment and E2EE adoption with a clear limitation of why and how that exclusion can/should exist. Approach to the Codes\\t\\tOfcom should consider the difference between OT (Operational technologies) as opposed to IT (Information technology)\\tOne area that may help to understand the challenge is to understand the mapping between Trustworthy people and Trust points in cyber security implementations. That mapping can be done relatively straightforwardly by understanding access rights and controls in the context of the accessible data types, to resolve to a clear accountability function and responsibility associated with such access rights as a Trusted person mapped to a secure digital trust point. Approach to the Codes\\t\\tVery unclear is what is a risk in multi-factor risk. Not clear if E2EE used where one peer in the communication is Trusted, whether this reduces risk to a level where it no longer falls into the multi-risk category driven by E2EE use in the context of the Online safety’s Bill approach to the tech as a risk creation factor. Approach to the Codes\\t\\tAgrees measures should be applied to Large services, but is unclear on what constitutes High Risk services\\tAgrees measures should be applied to Large services, but is unclear on what constitutes High Risk services\\nApproach to the Codes\\t\\tDoes not agree on definition of Multi-Risk services \\tThe definition of what is a Risk in the context of certain tech functionality is unclear. Content moderation (User to User) \\t\\tAgrees with Ofcom\\'s proposals to an extent. In Safe Space One\\'s generalisable use case, users expect confidentiality in their information sharing. So no pre-content assessment can or should be done otherwise it breaks confidentiality in healthcare. So the guidance is little unclear\\nAutomated content moderation (User to User) \\t\\tDoes not agree with proposals. Exemptions are needed when user confidentiality trumps risk of content sharing such as in healthcare, and where, in effect, a human moderator exists on every communication because one of the parties is a Trusted person (clinician). Automated content moderation (User to User) \\t\\tContent should remain confidential in a clinician-patient consultation and or information exchange. Ethical clinical guidance already determines when/if such content should be shared with appropriate services. User reporting and complaints (U2U and search) \\t\\tAgrees with proposals \\tNotes that in an E2EE healthcare consultation system the service provider is NOT the organisation that can be held accountable for responding to such issues as they do not have access to the content. Enhanced user control (U2U) \\t\\tIn general agrees with proposals \\tNotes that here is a bias towards guidance towards an assumption of social media like services. Guidance fails to be wide enough or clear enough for other services to have a confident interpretation for their own business and product implementation. Enhanced user control (U2U) \\t\\tIt is very unclear to patients what type of qualification is appropriate to assess that a mental health practitioner is appropriately of sufficiently qualified to be a TRUSTED remote service provider. The NHS has an accreditation control system, the private sector lacks any central repository to validate that a claim of identity relating to qualifications is true and trustworthy. OFCOM needs to address this factor as the next big issue is going to be abuse via ‘trusted’ clinicians no matter what underlying delivery technique is used. Cumulative Assessment  \\t\\tThe issue is not whether the measures themselves are cumulatively proportionate, but whether the impact of them are appropriate.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9394487142562866,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Chilling effect on secure systems innovation and early stage investor confidence of the cumulative complexity of these enforcements, lack of wide enough exemptions for specific sector use cases, and ultimately risk to businesses generally to be based and develop in the UK. Statutory Tests \\t\\tSomewhat agrees but approach blinkered by focus on social media\\tSomewhat agrees but approach blinkered by focus on social media\\nAutomated content moderation (User to User) \\t\\tNiantic values ACM measures to detect and remove CSAM and terrorism content. Niantic explain how they use a layered approach of tools, technologies and processes to moderate illegal harms. In particular, this approach prioritises tackling the most egregious harms and illegal offences, namely child sexual abuse material (CSAM) and terrorist content. They report CSAM material to NCMEC. They rely on a combination of moderation classifiers built by Microsoft, Google, and Cleanspeak to further detect policy-violating text, codenames, and images. Also collaborate with IWF and GIFCT. This evidence substantiates our position in ACM that ACM techniques are valued by services to moderate illegal harms. Content moderation (User to User) \\t\\tWhile ACM techniques are important human oversight and moderation also play a role. Automated detection helps minimise our players’ and moderators’ exposure to illegal and harmful content; however, we also deploy human oversight and moderation to bolster our safety efforts. Keywords Studios is our primary moderation partner at Niantic, their work is integral to our safety efforts. User reporting and complaints (U2U and search) \\t\\tNiantic values User Reporting mechanisms. Niantic have built reporting tools for users of their games and services, enabling them to flag issues to Niantic so that their teams can investigate and address concerns. Reporting is a key piece of their safety efforts, and they continue to iterate to design better, more effective, and simpler reporting options for their players. Governance and accountability\\t\\tSupport the principle that compliance burden should be proportionate to service\\'s size, resource levels and risks the service poses. Support the principle that compliance burden should be proportionate to service\\'s size, resource levels and risks the service poses. In relation to the establishment of an internal monitoring/assurance function (measure 3D), we believe this risks posing a disproportionate burden for providers that are not amongst the largest services, have nimble resourcing, and/or already have in place effective solutions. A dedicated assurance team of the size Ofcom is contemplating is not always necessary, when there is other effective solutions which are less burdensome ie leveraging an existing T&S team, achieving the same oversight from more senior roles within a compliance team, or involving multiple teams in the OS process to provide checks/balances against each other, rather than a separate audit-like function. Believe measure 3D is better suited to \"truly\" large services. Approach to the Codes\\t\\tFlexibility for services to apply the Codes of Practice in a manner suited to the characteristics of their service. In a number of areas, Ofcom\\'s guidance, due to breadth and generality, is less helpful to atypical services like Roblox. It would be valuable for Ofcom to expressly acknowledge that the measures proposed will not always be appropriate for all services, and services should apply Codes flexibly and/or comply with their obligations using alternative measures which take into account their own specific characteristics. This reflects the approach contemplated in Section 49 of the OSA, which is less apparent in Ofcom\\'s guidance. Approach to the Codes\\t\\tInteraction between obligations in the Codes of Practice and other laws applicable to online intermediaries. There are various legal regimes around the world that apply to online intermediaries, and many services within scope of the OSA are likely to also fall within these other regimes. These regimes recognise intermediaries cannot be held legally responsible for user activity and cannot be excpected to generally monitor user activity on services. For online intermediaries to be able to benefit from the legal protections afforded them, services need to take an approach that does not unwittingly provide them with knowledge of illegal activity. This has to be balanced against the needs of a service to protect its users and, by extension, its brand, and reputation. Needs to be a balancing act to ensure services will not expose themselves to legal risks whilst also being mindful of their legal obligations under the OSA. Any measures proposed by Ofcom need to take account of this. The Codes of Practice should not require services to undertake obligations that amount to active monitoring of user activity or confer knowledge of content that would mean providers could no longer take the benefit of intermediary liability exemptions otherwise applicable. Approach to the Codes\\t\\tSupport the principle that compliance burden should be proportionate to service\\'s size, resource levels and risks the service poses. Support the principle that compliance burden should be proportionate to service\\'s size, resource levels and risks the service poses. See Q14-15 for more detail regarding definitions of large and multi-risk services. Approach to the Codes\\t\\tConcerns about Ofcom\\'s proposed definition of large services and its corresponding implications. Our concerns are that 7 million monthly users is not an appropriate threshold, and if Ofcom maintains this threshold, it should at least take into account variations betweens services that meet the threshold. Approach to the Codes\\t\\tOfcom setting the \"large service\" threshold in similar way to DSA is disproportionate and will catch services not equivalent to VLOPs. Whilst the DSA has set the VLOP threshold at 45 million average monthly active recipients, all VLOPs have significantly higher numbers of average monthly active recipients. Ofcom setting the \"large service\" threshold in a similar way to DSA is disproportionate and will catch services not equivalent to VLOPs. Roblox provides information from the European Commission in relation to average monthly active users for VLOPs ie Meta 259 million, Pinterest 124 million, Snap 102 million, TikTok 135.9 million and YouTube 416.6 million. Approach to the Codes\\t\\tServices with over 45 million monthly users are different, and cannot be equated with, providers that offer services over 7 million monthly users. Ofcom\\'s threshold proxy analogy does not acknowledge that providers who offer services which have over 45 million monthly users are different from, and cannot be equated with, providers that offer services which have over 7 million monthly users. The other aspects of the DSA analogy must be considered ie the analogy assumes that the 7 million monthly user threshold means those providers have similar resources to VLOPs - this is not the case. Approach to the Codes\\t\\tOfcom should be conscious not to equate very large and smaller providers in a way that means compliance requirements inadvertently become barriers. VLOPs have had a very large user base for many years, so they have had time to scale up operations, and their ability to resource compliance with onerous legal requirements, in a way smaller providers with a steadily growing userbase have not. A number of examples in the Ofcom condoc where Ofcom has relied on measures VLOPs already have in place to support recommendations, or an assumption that all \"large\" services can absorb significant compliance costs. Examples of this include measures 3D, 3E, 4C and 8A - these measures will be difficult to develop/implement for providers who do not have a head start on this. Approach to the Codes\\t\\tUser base size is not necessarily determinative of whether it is justified to impose more onerous measures for some services. User base size is not necessarily determinative of, or the most appropriate proxy to, whether it is justified to impose more onerous measures for some services. Applying Ofcom\\'s \"large\" service defiition to mean \\'service deserving of more measures\\' can lead to disproportionate results and unfair outcomes. Do not agree that 7 million UK average user base is an appropriate threshold to define \"large\" service. Approach to the Codes\\t\\tOne-size-fits all approach does not take into account characteristics of services like Roblox. If Ofcom maintains 7 million threshold, it should be acknowledged that variations in services that meet this threshold need to be taken into account. Roblox does not fall typically into Ofcom\\'s risk factor characterisations and does not offer a lot of functionalities Ofcom seems to assume large services offer. For example, content virality is much lesser a consideration for Roblox compared to large social media services, which significantly reduces the reach and impact of illegal content. Approach to the Codes\\t\\tProviders who offer safer services by default should not be penalised for attracting a large user base. Providers who offer safer services by default should not be penalised for attracting a large user base, especially when those providers have managed to obtain good results using their judgement on appropriate safety measures/processes. Roblox links to a 1 February 2024 article examining Roblox\\'s digital civility strategy as evidence of its good practice. The measures we already take demonstrate that positive online safety outcomes can be achieved using more modest resources when they are deployed effectively.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9653351306915283,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Approach to the Codes\\t\\tThreshold for multi-risk services should not be set artifically low. Services need to be able to employ proportionate means to conclude that the risk of certain harms is low. If the bar for concluding that medium-high risks exist is set artifically low, a lot of services could find themselves in the multi-risk category, despite not actually presenting the level of medium-high risk that would warrant the more burdensome obligations associated with multi-risk designation. Approach to the Codes\\t\\tTwo or more harms is not the right threshold, there should be a more stage approach. The current definition of multi-risk services treats services that identify as medium risk of 2 different types of illegal harm in the same way as those that identify as high risk of all or almost all types of illegal harm. Fairer and more proportionate to introduce a middle ground between services that only pose a risk of one type of illegal harm and those that pose risks of nearly all. Approach to the Codes\\t\\tCertain types of illegal content clearly pose a greater potential severity of harm to individuals than others. Ofcom's current approach treats all illegal harms as being equal, whereas in criminal law different offences are treated very differently based on severity and harm they cause. Services will have to make judgements on how to deploy resources effectively to achieve highest safety for users, we consider it appropriate, and in line with Sections 9 and 10 of OSA, to prioritise resources addressing harms that have potential for most severe consequences. Content moderation (User to User) \\t\\tInteraction between obligations in the Codes of Practice and other laws applicable to online intermediaries. There are various legal regimes around the world that apply to online intermediaries, and many services within scope of the OSA are likely to also fall within these other regimes. These regimes recognise intermediaries cannot be held legally responsible for user activity and cannot be excpected to generally monitor user activity on services. For online intermediaries to be able to benefit from the legal protections afforded them, services need to take an approach that does not unwittingly provide them with knowledge of illegal activity. This has to be balanced against the needs of a service to protect its users and, by extension, its brand, and reputation. Needs to be a balancing act to ensure services will not expose themselves to legal risks whilst also being mindful of their legal obligations under the OSA. Any measures proposed by Ofcom need to take account of this. The Codes of Practice should not require services to undertake obligations that amount to active monitoring of user activity or confer knowledge of content that would mean providers could no longer take the benefit of intermediary liability exemptions otherwise applicable. Automated content moderation (User to User) \\t\\tOfcom needs to define more granularly what is captured by a medium risk of fraud. We encourage Ofcom to define more granularly what is captured by a medium risk of fraud, and to narrow this requirement to those services that have a medium risk of fraud, linked specifically to offences concerning articles for use in frauds. Automated content moderation (User to User) \\t\\tWhere services do not carry a risk of offences relating to articles used for fraud, implementing measure 4I could be overly burdensome. We understand the proposed measure 4I is aimed at detecting offences relating to articles used for fraud and the presence of other types of fraudulent harms on a service should not render the service in scope of this requirement. Where services do not carry a risk of offences relating to articles used for fraud, implementing measure 4I could be overly burdensome, ineffective in improving online safety outcomes for user, and divert resource away from other measures more relevant to specific fraud risks posed by a service. User reporting and complaints (U2U and search) \\t\\tWelcome Ofcom's overall conclusion to recommend more general rather than detailed or specific requirements. Welcome Ofcom's overall conclusion to recommend more general rather than detailed or specific requirements. It is extremely important for services to retain a relatively high degree of flexibility in how they design their complaints systems/processes. This outcome is preferable to the other option Ofcom considered of recommending specific design features. Terms of service and Publicly Available Statements\\t\\tConcerns regarding Ofcom's proposal that complainants be restored to their original position. We have concerns regarding Ofcom's proposal that complainants need to be restored to their original position. We think the concept is problematic and needs revision. Terms of service and Publicly Available Statements\\t\\tConcerns regarding technical feasibility to implement proposed measure 5F. We have concerns that it may not be technically possible to put content back in the position it would have been if not taken down. For example, where visibility of content depends on engagement, it will not be technically possible to put content back in position it would have been if not judged illegal. It may not even be possible for a provider to ascertain what that original position would be. Terms of service and Publicly Available Statements\\t\\tConcerns regarding content moderation implications. Ofcom proposes that providers may also need to adjust their content moderation guidance and take steps to ensure automated content moderation technology does not remove the same content again. It does not seem practical or proportionate for services to adjust their content moderation guidance and/or technology upon every instance of an incorrect decision. We would suggest Ofcom make a softer recommendation that services, at their discretion, may consider changes in the event of certain types of false positives are revealed to be common. Terms of service and Publicly Available Statements\\t\\tConcerned about wording used by Ofcom in its guidance and the unintended consequences it may have. The wording used by Ofcom in its guidance and concept of putting a user back in the position they would have been for an appealed content moderation decision, could lead users to bring claims against providers, due to interpretation that financial compensation could be due to users. Encourage Ofcom to revise its guidance and avoid language that may lead to unintended consequences. Approach to the Codes\\t\\tShe is unclear whether her site falls in scope of the regulation (e.g. has a comment section, users can upload images to their profiles). As a very small site with low traffic (2-3K visitors per month), she finds the requirements for a small /low risk platform like herself onerous. Its not at all clear what counts as a “user to user” service. I am the sole publisher on my website, which is low traffic (2-3k visitors per month) - I'm the only one who uploads videos to it. But I have a comment section below posts where users can post comments, which is an important part of the community feel of the site, and allows me to interact with my viewers in a way that humanises me and increases the personal connection. Having read the guidance I remain unclear whether this comment function counts as a “user to user” service which is covered by the guidance. Users can upload images to their profiles - does that count? If so then the requirements even for a low-risk, small platform seem overly onerous for a one-person band like me to comply with. User reporting and complaints (U2U and search)\\t\\tComplaints and reporting tools can be misused to attack vulnerable groups such as sex workers (e.g. via trolls, stalkers, anti-sex work/anti-porn people).\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928472638130188,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Could result in emotional harm to her and large amounts of admin to manage. Sex workers face a constant threat from disgruntled fans, trolls, and stalkers, as well as anti-sex-work and anti-porn crusaders. I'm afraid that these proposals would make it far too easy for bad actors to cause trouble by reporting my images as trafficked or abusive to platforms I rely on, such as social media platforms. Even if my account is reinstated this could cause harm to me emotionally and to my ability to run my business. They could also harm me by maliciously making multiple spurious reports on my own website, potentially requiring me to do an enormous amount of admin needlessly responding to the reports. Governance and accountability  \\t\\tSupportive of having a named person accountable for compliance.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9951059818267822,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Previously suggested Ofcom should be able to impose criminal sanctions on senior managers. Approach to the Codes\\t\\tSupportive of general approach, one that is focused on greatest assessment and mitigation of risks of harm to children.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9962936043739319,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'A concern raised that some measures are targeted at some services, rather than recommended for all user-to-user services, though this may be addressed in the forthcoming Children\\'s Code? Content moderation (User to User) \\t\\tSupportive. Supportive of need to for services to take-down illegal content and that larger and multi-risk services should have performance targets for this work. Content moderation (Search)\\t\\tStrongly supportive. Strongly supportive. Automated content moderation (Search) \\t\\tSupporive. Supporive. User reporting and complaints (U2U and search) \\t\\tVery supportive. Very supportive. \" Default settings and user support (U2U)\"\\t\\tGenerally supportive of proposed measures for default settings and user support. \"Preferance that measures for  default settings are recommended for implementatiopn across all services, though awareness this might be supplemented by age assurance in the Children\\'s Codes of Protections of Children. Concern that recommender settings are not picked up in measures for default settings. Concern that information about risk should be accompanied by blocking functionality(?) or support post-risk / post-harm,\"\\nRecommender system testing (U2U) \\t\\tSupportive. Supportive.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992904007434845,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Governance and accountability\\t\\tExpressing concerns how these proposals will impact free expression and human rights orgs  to protest and dissent on line. The suggestion is that the proposal risks forcing social media platforms (SMPs) to take action against legitimate, good-faith actors who are dissenting to government policies or views. With c concern for the potential for abuse and over-reach.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9972262978553772,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As such further scoping and more targeted approaches are recommended. Governance and accountability\\tGovernance and Accountability\\tThere is agreement in the types of service the proposals apply to. Response: To prevent stifling innovation and entrepreneurship the proposals must be adequately scoped\\xa0\\nApproach to the Codes\\t\\tTwo aspects of the Act cause concern: Protecting human rights advocates, universities, research organisations and researchers could be efected by the proposal. Further scoping is required to protect these groups whilst doing their work. Ofcom must engage closely and regularly with civil society and academia through an established forum to identify and monitor emerging online harms before they arrive on mainstream platforms. Approach to the Codes\\t\\tClassification should be based on the type of platform as well as its size. Research has highlighted the role small, but influential alt-tech platfoms  play in providing an initial home for extremism and disinformation to expand. The code must take these alt-tech platforms into account. Approach to the Codes\\tdefinition of large services\\tAlthough not classified as large services, high concentrations of malicious actors, could cause them to fall out of scope. Research  from GNET, EGRN and OxDEL highlighted the role that small alt-tech companies play in an initial home for xtremism and disinformation to expand. Approach to the Codes\\t\\tThe proposed Codes of Practice must be more regularly updated to reflect changes in technologies and online harms.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9972062706947327,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"The Codes of Practice must be more adaptable and flexible. Content moderation (User to User) \\t\\tConcerns around free expression of civil society organisations (CSO's) academic researchers, and human rights advocates being curtailed by the existing proposals. Requesting more targeted scoping, with clear definitions, transparency and robust appeals process, to avoid over-reach to target legitimate peaceful dissent, civil society and academic researchers. Content moderation (Search)\\t\\tIn cases, search functions recommend extremist content, not designated to have come from a terrorist organisation. The proposals need to take better account of this. Concerns remain about the free expression of civil society organizations (CSOs), academic researchers, and human rights advocates being curtailed by the existing proposals. using more targeted scoping and clear definitions, along with transparency and a robust appeals process in determining how content is classified and moderated, would alay some concerns. Recent research from Dr Aaron Zelin (Brandeis University) has proposed the development of white-listing certain users or organizations, along with broader solutions that include transparency reports and a robust appeals process\\nAutomated content moderation (User to User)\\t\\tFree expression concerns were once again reaised. Automated content moderation using AI tools or algorthymic detection, removes essential human intervention. Existing examples of risk include individual users, organizations, and researchers who have been caught up for discussing or analysing content which falls outside the narrow bounds of the guidelines. Moreover, when such violations have occurred the appeals process remains unclear, slow, and difficult to navigate. Automated content moderation (User to User) \\t\\tAssociations such as GIFCT and Tech Against Terrorism are critical in ensuring 'hashes' are included. Expressed concerns around AI processes and how this could be problematic in the future. Hash-sharing is effective but only to the extent the database is continually maintained and updated with new research on extremism and disinformation. Publications by The Global Network on Extremism and Technology (GNET) and the Extremism and Gaming Research Network (EGRN) have published significant guidance on this subject. Automated content moderation (Search) \\t\\tAutomated content moderation has the potential to improve results and filter out harmful content, however, concerns aorund CSO', academic researchers and human rights advocates could be curtailed by the existing proposals. Moderation of extremist groups and malicious actors operating on private platforms is necessary and legal, we worry about the scoping of the existing proposal and its potential over-reach or abuse to target legitimate peaceful dissent, civil society and academic researchers. More targeted scoping and clear definitions, along with transparency and a robust appeals process in determining how content is classified and moderated. If legitimate political dissent or human rights advocacy is removed or downranked, it can impact on public trust, potentially causing surrounding bias and prejudice against particular political movements. Greater reliance on SME's is necessary to maintin  fairness and equal application to the decision making. User reporting and complaints (U2U and search) \\t\\tA stronger and more robust complaints measures must be introduced, with a review by an SME for content and context. Trust in the proposal and platform could be enhanced by ensuring that user reporting and complaints will receive serious consideration. These measures not only reduce perceptions of bias, they can help improve adaptability and accuracy in identifying online harms\\nTerms of service and Publicly Available Statements \\t\\tFully agree with the proposals, with a focus on greater clarity and transparency. Improved clarity, standards, and transparency is essential in both improving public trust and enabling academic researchers to access data for further analysis\\nTerms of service and Publicly Available Statements\\t\\tGreater use of prompts is urged, with the use of  redirect methods and pre-bunking initiatives. These should be used cautiously as they can be ignored or disregarded.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.8912879824638367,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"prompts and others nudge methods must be continually updated and changed. Moreover, they must use empathy, humour, and serious engagement with disinformation and misinformation to be considered credible by users who are most vulnerable to these online harms. Recommender system testing (U2U)\\t\\tWe are strongly supportive of the proposals and encourage greater testing of all products before public release. Testing and red/blue teaming are essential parts of a safety-by-design approach and we encourage the development of these tools. Recommender system testing (U2U) \\t\\tSupport from industry wide associations and larger platforms which can ‘lend’ expertise or mentorship to smaller platforms would support the overall health of the ecosystem. Future developments in this field could replicate the success of such programs related to content moderation and trust & safety which have provided smaller companies which lack the resources to ensure they are complying with all regulations. Cumulative Assessment\\xa0\\xa0\\t\\tWe urge the development of counter measures to address the burdens placed on small businesses. Both of these approaches have been successfully applied to other regulatory issues and could be replicated by Ofcom and the Online Safety Bill. Recommended that a one stop shop office would reduce the challenge for small businesses. Cumulative assessment\\t\\tWe urge the development of counter measures to address the burdens placed on small businesses. Both of these approaches have been successfully applied to other regulatory issues and could be replicated by Ofcom and the Online Safety Bill. Recommended that a one stop shop office would reduce the challenge for small businesses. Automated content moderation (User to User) \\t\\tCPS concerned that human review of CSAM could perpetuate offending, so steer should be to minimise number of people exposed to this material and to ensure those people are A. approriate (i.e. have been DBS checked etc) and B. supported\\tThe CPS would be concerned regarding members of the public or those employed by providers reviewing detected CSAM, as this can perpetuate offending of this nature. As OFCOM have noted, hash-matching is not 100% effective, and therefore human moderators could view CSAM as part of their moderation. The aim should be to minimise the number of individuals who may be viewing images, and to ensure that individuals who are viewing the images have had appropriate checks on their history relating to children. We take note of the record-keeping and the proportionality principle to be applied. Those that are reviewing the content detected as CSAM, would likely benefit from wellbeing support. Viewing CSAM is likely to be incredibly distressing and individuals should have access to support, should they require it.'\\nAutomated content moderation (User to User) \\t\\tOfcom should review proportionality regularly to prevent excessive reviewing of detected CSAM\\tIt would be appropriate for OFCOM themselves to measure proportionality and re-assess this at regular intervals, to prevent excessive reviewing of detected CSAM and prevent the perpetuation of abuse.'\\nTerms of service and Publicly Available Statements\\t\\tService providers should consider making information/policies accessible to wider range of ages\\tIt may be useful to ask providers to consider the age of the child users that are likely to use their service, whether the material is targeted at a certain age group, or not. Comprehension ability will differ, and therefore equates a potential need to tailor communication to different ages.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9738211035728455,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'There may also need to be consideration regarding special educational needs and language/translation to ensure accessibility. We would like to emphasise that providers should be enforcing their terms and conditions regarding age limits on websites to ensure that messages hit the appropriate age limits. However, there is a possibility that children are younger than the nominated age limit for accessing sites, and therefore providers may need to consider widening the targeted age for information to ages other than the prescribed age limits of their site.\\'\\nContent moderation (Search)\\t\\tIllegal content should ideally be deindexed rather than downranked\\tThe CPS understands that downranking makes the search content less easy to locate, however the illegal content will still be available to be found, should an individual seek it out specifically. Any illegal content would ideally be recommended to be removed/deindexed from search providers. We acknowledge that within the COP, CSAM URLs are always recommended to be indexed.\\'\\nService design and user support (Search) \\t\\tShould consider provision of age-appropriate support alongside warnings for child users searching CSAM\\tAs OFCOM has acknowledged, child-on-child sexual offences are occurring, and as highlighted within the VKPP Analysis on CSAE, an increasing number of all child sexual offences are committed by children. It may be appropriate to signpost different, age-appropriate links to adults and children who search for child sexual abuse content. There are specific resources available such as Shore Space, which provides information for teenagers who are concerned about their sexual behaviour.\\'\\nGovernance and Accountability  \\t\\tUrge Ofcom to not mandate use of external auditors for a range of reasons \\tMSPG urge Ofcom to not mandate use of external auditors for various reasons and instead focus on the principles and expectations that underpin audits. The three main reasons MSPG are concerned about mandating external auditors is, the artificial inflation of cost of audit services, resource burden for smaller services and experience from DSA regulation. Governance and Accountability  \\t\\tGovernance and accountability measures are not proportionate \\tCurrently the governance and accountability measures imply a lot of paperwork and prescriptive processes which is very time consuming, especially for smaller platforms. This has the knock on impact of disproportionately affecting challenger and alternative platforms\\nGovernance and Accountability  \\t\\tAgree with the exemption from most of governance and accountability measures for large vertical search services \\tMSPG really welcome, however, the exemption from most of these measures for large vertical search services. They argue anything beyond what is being proposed currently, given the lack of available evidence and the inherently low-risk posed by VSS, would be disproportionate. Governance and Accountability  \\t\\tUrge Ofcom to not mandate use of external auditors for a range of reasons \\tMSPG urge Ofcom to not mandate use of external auditors for various reasons and instead focus on the principles and expectations that underpin audits. The three main reasons MSPG are concerned about mandating external auditors is, the artificial inflation of cost of audit services, resource burden for smaller services and experience from DSA regulation. Approach to the Codes\\t\\tConcerned that Codes are too prescriptive \\tMSPG welcome proportionality but worry that Codes could be too prescriptive and not reflect diversity of platforms. Argue that there is nit enough clarity in how platforms show compliance without fear of being accused of non-compliance. Approach to the Codes\\t\\tRegualtory burden linked to the DSA should be considered by Ofcom \\tMSPG members welcome efforts to reduce regulatory compliance burdens by aligning with the EU’s DSA, but Ofcom should also look to improve on the DSA where it has fallen short \\nApproach to the Codes\\t\\tDisagree with the threshold to be a multi-risk service and ask for a more nuanced approach \\tThe thresholds particularly for ‘multi-risk’ have been set at a low level. This risks capturing a large number of platforms in the scope of more onerous obligations. Services that identify as medium risk of just two kinds of harm, out of a total of 15 priority harms and any other relevant non-priority harms, should not then be deemed as posing a “significant” risk for illegal harms “in general\" because this is incredibly disproportionate. MSPG argue that a more nuanced approach could involve increasing the number of medium-rated risk factors above two, or being more specific as to which harms require specific mitigations\\nApproach to the Codes\\t\\tCodes do not always reflect best practice from industry \\tMSPG concerned about Codes not reflecting best practice particularly keyword detection for fraud. Services have found that this can lead to excessive flagging of content and measures such as behaviour signals have instead been used to identify and tackle fraud. Approach to the Codes\\t\\tCosts estimates attached to Codes are very broad \\tMSPG has concerns around the approach used to estimate the costs of certain measures which has resulted in very broad estimated cost ranges and limited attention to how additional costs may impact competition within the sector \\nContent moderation (Search)\\t\\tSearch content moderation is drafted too narrowly \\tThe content moderation obligation for search services has been drafted too narrowly, failing to take account of the fundamentally different ways in which VSS and general search services operate. VSS do not index sites from across the clear web, for example, so they do not have in place systems or processes that “deindex” illegal content. They do, however, have processes or systems in place to remove illegal content that they are aware of, and so it should be made clear that other forms of removal are acceptable beyond “deindexing.”\\nApproach to the Codes\\t\\tStrongly welcome and agree with Ofcom\\'s proportional Approach to the Codes\\tMSPG welcomes proportionality being central to the Codes of Practice and welcomes Ofcom highlighting its intentions that both size and risk are considered to ensure only the largest and most high risk are burdened with more regulations \\nApproach to the Codes\\t\\tMSPG has concerns about the defintion of large service under the OSA vs the DSA\\tArgument that the obligations tied to the UK’s definition of a large service and the obligations attached to the VLOP/VLOSE designation in the EU’s DSA are not, for the most part, comparable. It would therefore appear there is little to be gained in aligning with the EU’s definition for the sake of it\\nApproach to the Codes\\t\\tWant for more emphasis on \\'registered monthly active users\\' in Codes\\tMSPG reccommends that Ofcom clarifies its guidance to place more emphasis on ‘registered monthly active users’, as opposed to only considering ‘registered users’. Approach to the Codes\\t\\tRegistered monthly active users\\' in Codes should be particularly considered for marketplaces\\tSome marketplace platforms allow no functionality for most user experiences beyond considering the private purchase of goods or services. For such limited services where users are simply making a personal purchase that is not seen by other users nor influenced by other areas of the service, content is not created by all users and exposure to wider content is greatly limited. Therefore, the user number should be weighted more towards the subset of users who are posting a good or service for sale and are therefore users that are engaging with the service in a manner that can be viewed by other users. Approach to the Codes\\t\\tCosts are discussed in a silo when the cost analysis should be cumulative, urge Ofcom to continue investigating costs \\tSome assumptions made throughout the consultation that the costs identified are low. However, the costs of each measure that have been identified are often discussed in silo rather than the cumulative cost of implementing these measures across all issues. This makes business planning very difficult, especially when individual cost estimates have vast ranges. MSPG urges Ofcom to continue investigating costs and continuously monitor the costs of implementation to ensure it can cost its proposals as accurately as possible whilst working to understand whether prices are being artificially inflated by specific measures it recommends\\nApproach to the Codes\\t\\tReccommnedation informed by the DTSP’s Safe Framework on costs and the regulatory framework \\tMSPG stresses the importance of incorporating considerations of company maturity, resources, and capability into the regulatory framework to ensure it is sufficiently tailored to platforms in the sector as suggested by the DTSP’s Safe Framework \\nGovernance and accountability  \\t\\tGovernance and accountability measures are not proportionate \\tCurrently the governance and accountability measures imply a lot of paperwork and prescriptive processes which is very time consuming, especially for smaller platforms. This has the knock on impact of disproportionately affecting challenger and alternative platforms\\nGovernance and accountability  \\t\\tAgree with the exemption from most of governance and accountability measures for large vertical search services \\tMSPG really welcome the exemption from most of these measures for large vertical search services.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9594298601150513,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They argue anything beyond what is being proposed currently, given the lack of available evidence and the inherently low-risk posed by VSS, would be disproportionate. Content moderation (User to User) \\tOverall view of U2U content moderation proposals \\tContent moderation proposals are unintentionally prescriptive, reductive, disproportionate, or do not reflect what is best practice in industry\\tContent moderation proposals are unintentionally prescriptive, reductive, disproportionate, or do not reflect what is best practice in industry. It is the Group’s perspective that these measures risk favouring the largest companies with similarly designed, centric or attention-led moderation models.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8931231498718262,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Smaller services bear a much larger compliance burden. For example, performance targets measuring how quickly and accurately a service arrives at decisions following receipt of complaints have been proposed.': [{'label': 'POSITIVE',\n",
       "               'score': 0.915647566318512,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"This risks being reductive, inaccurate, creating comparison between services that is not helpful and incentivises over moderating on services. Content moderation (User to User) \\tTechnical feasibility\\tWant some assurances about Ofcom’s expectations for what it might deem as ‘technically feasible’ \\tWelcomes Ofcom's flexibility and looking at technical feasibility but want assurances on what technical feasibility might mean or have more information on how Ofcom would like to work with platforms to understand what that looks like would be helpful \\nContent moderation (User to User) \\tAgainst specific content moderation measures \\tKeyword detection for fraud is disagreed with\\tKeyword detection would require some platforms to disrupt core features of the service in a way which would drastically alter user experience without actually benefitting safety\\nContent moderation (User to User) \\tAgainst specific content moderation measures \\tDuty to enable users to block comments on their posts from other service users is problematic\\tThe duty to enable users to block comments on their posts from other service users, even if the respondent has not previously been blocked by the poster does not align with some members’ service offering. It would require some platforms to disrupt core features of the service in a way which would drastically alter user experience without actually benefitting safety\\nContent moderation (Search)\\tConcerns for vertical search services \\tAsk for more clarity in wording of search moderation obligation \\tThe search moderation obligation does not reflect the different way in which VSS operate so welcome further clarity with regards to the wording of this obligation to make clear that other systems or processes that see illegal content removed from search results is also acceptable. MSPG overall welcome that Ofcom are not being too prescriptive \\nAutomated content moderation (User to User) \\tUnintended consequences of automated content moderation \\tAutomated content moderation reccommendations will have unintended consequences and could lead to substantial resource demands \\tAutomated content moderation recs will have unintended consequences and could lead to substantial resource demands (both financial and human)with companies with medium and small particularly impacted. MSPG want to avoid these proposals distorting the market. Keyword detection to tackle fraud risks increasing the demand on human resources for example. Automated content moderation (User to User) \\tUnintended consequences of automated content moderation \\tAutomated content moderation vs human moderation concerns \\tArgument that automated content moderation is meant to augment human moderation not replace it. User reporting and complaints (U2U and search) \\tRisks and compliance burden of user reporting and complaints \\tRisk that the number of complaints procedures suggested in the consultation becomes cumbersome and disproportionately impacts small and medium platforms \\tThere is a risk that the number of complaints procedures suggested here becomes cumbersome and disproportionately impacts small and medium platforms, and the proposals may ignore different models which serve the particular purpose a platform sets out to deliver. Argument that regardless of a service’s size, requiring certain services which are low or negligible risk to have multiple complaints procedures is clearly disproportionate, particularly around illegal content complaints procedures and appeals processes. Urge Ofcom to consider resource intensity and compliance burdens e.g. in relation to the requirement for all services to provide an acknowledgement of all relevant complaints along with an indicative timeline \\nTerms of service and Publicly Available Statements\\tSafety policies\\tRisk of bad actors benefitting from safety policies \\tGroup members are committed to media literacy however, each ensures this is done in a way that does not provide instructions for users on how to evade safety policies. Terms of service and Publicly Available Statements\\tTerms of service and Publicly Available Statements linked to DSA\\tReccommendation to Ofcom to align with DSA to reduce compliance burden \\tDSA in the EU has similar provisions for in-scope firms in this area, we recommend that Ofcom ensures what it is proposing will allow multinational services to provide one statement and terms of service that can be used across the UK and EU, to minimise compliance obligations\\nDefault settings and user support (U2U)\\tDefault settings and user support for child users \\tOfcom should ensure default settings and user support for child users are compatible with a variety of platforms \\tMSPG welcome proportionality and Ofcom should ensure that the measures it is proposing are compatible with a variety of platforms and business models\\nService design and user support (Search) \\tService design and user support for vertical search services \\tMSPG agree with measures being proposed only applying for general search service\\tMSPG welcome that these measures are being proposed only for general search services, given that, in line with the limited functionalities of VSS and the lack of available evidence for such harms existing on such sites, it would clearly be disproportionate to recommend these proposals for VSS. Recommender system testing (U2U) \\tReccomender testing\\tGeneral comment on reccomender testing \\tEach of the Group’s members adopts a safety by design principle whereby design choices have been made to ensure a positive user experience.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9925823211669922,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This means members have a holistic and systemic approach to minimising the distribution of illegal content, rather than simply relying on identifying individual pieces of content. Enhanced user control (U2U) \\tEnhanced user control \\tSome enhanced user control proposals not straightforward to implement and Ofcom needs to think about resource intensity and technical limitations \\tSome enhanced user control proposals not straightforward to implement and Ofcom needs to think about resource intensity and technical limitations. MSPG recommend Ofcom remains open to alternative enhanced user controls, especially if the proposed duties conflict with how some services operate and urge consideration for the practical challenges associated with implementing the proposed duties\\nEnhanced user control (U2U) \\tEnhanced user control \\t3 risks associated with voluntary verification schemes\\t3 risks associated with voluntary verification schemes. 1.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9878664016723633,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Collecting and verifying user information may raise privacy concerns. Users may be hesitant to share personal details, and there is a risk of disproportionate res. 2. Users may assume that a labelled account is entirely trustworthy, leading to a false sense of security.ponsibility placed on platforms to handling or authorise access to sensitive information. 3. Voluntary verification risks anonymity and for some services such a characteristic is of para-mount importance for the service’s offering and one which users greatly value. Approach to the Codes\\t\\tThere is an over-reliance on reactive action for course of conduct harms.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9926438927650452,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Reliance on reactive action to tackle these harms may escalate offline targeting. The onus is placed on the victim and survivor to instigate action. Tech companies should adapt their policies to recognise harms which are perpetrated through patterns of behaviour. Hash-matching\\tHash-matching should be recommended for intimate image abuse. Hash-matching should be recommended for intimate image abuse. Approach to the Codes\\t\\tApproach to the Codes is based on incorrect assumptions.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9360467791557312,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'There is too much emphasis on retrospective measures such as takedown and blocking. Ofcom should consider what design choices may perpetuate harmful behaviours. Ofcom assumes that tech companies will adopt best practice, despite evidence that they do not even adhere to their own terms and conditions. The approach to segmentation risks missing sites which have lower traffic but cause considerable harm. Evidence from the Commissioner and Refuge is provided. Approach to the Codes\\t\\t\"More attention needed to freedom of \\nexpression.\"\\t\"More detail should be provided on how platforms can ensure they are complying with freedom of \\nexpression obligations during risk management.\"\\nApproach to the Codes\\t\\t\"Platforms should be compelled to \\nenforce freedom of speech.\"\\tPlatforms should be compelled to enforce freedom of speech. \"Content Moderation \\n(User to User)\"\\t\\t\"Agrees but suggests greater \\ntransparency is needed.\"\\t\"Approves of the balance between taking down illegal content and making accurate decisions.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9916096329689026,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approves moderator training but suggests this should be extended to users with violations. More information on trusted flaggers and how they will be held accountable would be welcome. Evidence from the author is provided.\"\\n\"Automated content \\nmoderation (User to User) \"\\t\\tAgrees but suggests a robust appeals process is needed. Approves but states that automated processes should be supported by effective appeals overseen by a human moderator to prevent over-enforcement and other mistakes. Evidence from the author is provided. \"Automated content \\nmoderation (Search)\"\\t\\tAgrees but reiterates need for a robust appeals process. Approves but reiterates concerns about over-enforcement and need for appeals system and human moderators to address this. \"User reporting and \\ncomplaints (U2U and search) \"\\t\\tGreater transparency needed around flagging. Acknowledges the importance of trusted flaggers and user reporting. Consideration should have been given to malicious flagging, which is particularly effective at silencing marginalised communities. Platforms should also be more transparent about whehter user flags are taken up. Transparency is needed in the choice of trusted flaggers so as not to reinforce the disproportionate focus of law enforcement on certain marginalised groups. Evidence is provided.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9868651032447815,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"Terms of service and \\nPublicly Available Statements\"\\t\\tAgrees but suggests greater transparency is needed. Agrees that users need accessible ToS. Platforms should inform users of enforcement decisions and provide explanations of violating content. Gives example of the lack of transparency in Instagram removing kink and sex positive accounts. Platforms needed to be clearer about any demotion (\"shadowbanning\") techniques.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8592842817306519,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Evidence is provided. \"Default settings and user support (U2U) and \\nuser support (U2U)\"\\t\\tPadditional protection for children should not be puritan in nature. Agrees children should have additional protection. Criticises the puritan moderation practices of platforms, especially where sex education and LGBT+ communities have been censored. Notes that for some children this kind of content is key to safety, consent and affirmation.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9921029210090637,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Academic evidence is provided. \"Default settings and user support (U2U) and \\nuser support (U2U)\"\\t\\tSuggests content-specific age-gates. Suggests periodic checks with users about what content they do and do not want to see. Allowing creators to age-gate specific posts. Evidence from the author is provided. \"Recommender system \\ntesting (U2U) \"\\t\\tRecommender systems must consider workers\\' rights. Recommender systems can also cause harm, especially to content creators, through demotion. Workers\\' rights therefore need to be built in to any recommender systems. Academic evidence is provided. \"Enhanced user \\ncontrol (U2U) \"\\t\\tMore resource for users with protected characteristics. Blocking and restricting puts the onus on users. Users with protected characteristics should have access to dedicated teams within service providers who can help them address harassment issues. Academic evidence is provided. \"Enhanced user \\ncontrol (U2U) \"\\t\\tPaid-for verification increases certain risks. Allowing users to pay for verification increases risk of fraud and mis/disinformation. Cumulative Assessment\\t\\tAgrees with certain stipulations. Agrees as long as the unintended consequences of harm reduction and freedom of expression are taken into account. Statutory Tests\\t\\tAgrees although more consideration should be given to at-risk users. Agrees although more is needed to account for over-compliance and over-moderation. Direct support is also needed for at-risk and overly-targeted users. Approach to the Codes\\t\\t\"Codes do not go far enough and seem \\nto be based on practices already adopted by industry. \"\\t\"Appreicate that Codes of Practice will be iterative, but feel the Codes must go much further. The current proposals set a low base-level which most providers already comply with so there will not be a noticeable impact for platform users. The codes recommend hash-matching for CSAM for example, but many platforms already use this. Evidence is provided. The only measures that address CCB are giving users the option to block or mute other accounts which most platforms already allow. Unclear how adopting only measures already used by the indutry is going to address the fact that women and girls are not currently sufficiently protected online. There is a disconnect between measures presented in Codes and the risks analysed in the Register, as the Register acknowledges the severity nd prevelance of VAWG but the Codes do very little to address it.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9930804371833801,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'No methodology is provided for the development of Codes but it appears that preferential weighting is given to evidence from industry. Codes must be more aspirational. Further measures should be included that focus on safety by design and the prevention of online VAWG. The focus of the current Codes seems to be on mitigations after harms have occured. A preventative approach is important and what was promised by the OSA. \"\\nApproach to the Codes\\t\\t\"Disagree with the application of most \\nonerous measures to large/multi-risk services and provide evidence of harmful smaller services. \"\\t\"Do not agree with the heavy differentiation between large/multi-risk services and small/single-risk services. Small services includes extremeley harmful sites (e.g. platforms dedicated to \\'collector culture\\' intimate image abuse or which are dedictaed to deepfake intimate image abuse)and yet only very light measures are applied to them. Scale of use should not e the sole indicator of risk. This regulatory loophole may encourage the creation of new, small platforms for harmful purposes to avoid regulation. It is disappointing that even basic safety measures such as training and resorucing of content moderation staff are not recommended for small services. Evidence provided on specific case of Jake Davison who visited small incel forums after being banned from Reddit. While the majority of tech-facilitted domestic abuse is expereinced on Meta-owned platforms Refuge have supported women who have been subjec tto abuse on much smaller platforms. Smaller platforms such as 4chan and Gab have been documented as contaiing high-levels of misogynoir. Evidence provided.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9937064051628113,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Company revenue appears to be prioritised as Vol 4 states that more onerous measures may have implications for the cost, competition and innovation of smaller services. Recommend that Ofcom reconsiders it\\'s averarching approach to ensure smaller platforms do not avoid much-needed safety measures. \"\\nApproach to the Codes\\t\\t\"Disagree with the threshold for large \\nservices.\"\\t\"Disagree with the high threshold used in definition of large services. The 7 million user base \\nthreshold would not include platforms such as Fortnite and Roblox. In some cases, a platform may not be used widely by the general population, but may be highly used by a particular group such as children.\"\\nContent Moderation (User to User)\\t\\tRecommend improvements for the content moderation codes, including additional detail about performance targets, training and priortisation. \"Broad principles on content moderation are welcome. However, the measures need to be more tightly defined. At the moment the requirement for content moderation systems or processes designed to take down illegal content swiftly is vague. Currently many survivors are left waiting weeks, months, or even years for a response to flagged content and many never recieve one. Evidence from Refuge provided.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933856129646301,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This leaves space for a range of interpretations, including the possibility that social media compnaies will de-amplify content rather than removing it. Further detail should be given to platforms on what performance targets should look like. Additional detail should be provided on the training and materials that staff in contnet moderation should recieve. Recommend it is harms-specific, delivered by experts, and covers harmful terminology in different languages. Many platforms use third-party content moderation and these systems and processes should be included in measures on Content Moderation. Domestic abuse and online VAWG should be included as content for priortisation by content moderation. How will severity be assessed? Disagree that \\'virality\\' should be a prioritisation metric as in many cases content only needs to be shared a couple of times to place the survivor at risk.\"\\nContent Moderation (Search)\\t\\tConcerns about AI-driven platforms which search for information on specific users. Raise concerns about the growth in AI-driven platforms which identify individuals.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9937050938606262,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Provided evidence of such platforms. This allows perpetrators to track down victims and survivors of domestic abuse. Automated content moderation (User to User) \\tHash-matching\\tRecommend hash-matching for intimate image abuse. Given hash-matching has been recommended for CSAM, question why it hasn't been recommended for intimate image abuse. Provide evidence of the viability and effectiveness of StopNCII.org.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.991753339767456,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'User reporting and complaints (U2U and search) \\t\\tRecommend dedicated reporting channel for VAWG and measures on cooperation between platforms and police. \"Recommend a dedicated reporting channel for VAWG, similar to what has been recommended for fraud. This should be staffed by specialist employees. Recommend Ofcom consult with specialist VAWG organisations in developing such reporting channels. Trsuted flagger pathways can be valuable but still have long waiting times. Support the recommendation that complaints systems should be easy to find, easy to access and easy to use. However, feel that the measure lacks specificity, and recommendations should be included about reporting in different languages and accessibility. Recommend measures on platforms cooperating with the police to obtain digital evidence, particularly of domestic abuse. Provide evidence on WhatsApp failing to work efficiently with police. \"\\nTerms of service and Publicly Available Statements\\t\\tBroadly agree with Terms of Service measures but are concerned about use of AI in content moderation. \"Agree with the measures on clarity and accessibility of Terms of Service and the information they should contain. Ensuring this information is provided in an accessible way is the minimum platforms should be doing. Recommend providers set out to users timelines around how they will remove illegal content. Transparency around the use of AI in content moderation is key. Ofcom in cases of domestic abuse identifying harmful content requires context, nuance and understanding. AI is not able to provide this. Recommend that all content moderation processes invovle human oversight.\"\\nDefault settings and user support (U2U)\\t\\tAgree with the measures on default settings and user support for children. Agree with the measure to provide children with supportive information. Enhanced user control (U2U) \\t\\tAgree with enhanced user control measures but recommend more on prevention and accessibility. Agree that users should be given more control and be able to block accounts and disable comments. However, in general onus should not be placed on users to take action against online harms, more emphasis should be put on prevention and safety by design. Recommend that user control features such as blocking are made accessible across all app/web versions. Should be easy to find, use and should be accessible for users with disabilities. Approach to the Codes\\t\\tThe right to freedom of expression is not clearly defined within the OSA and therefore services will need to have a good understanding in order to uphold it. The OSA does not clearly explain freedom of expression and therefore services providers will need to have a good understanding of the  law and case law. There is a risk that, due to a lack of understanding, services will not properly account for freedom of expression rights when deciding whether to take action againt illegal or legal but harmful content (and the users who have posted this content). Services will need to have a good understanding of the right to freedom of expression in order to uphold it. Approach to the Codes\\t\\t(same comment as above) The right to freedom of expression is not clearly defined within the OSA and therefore services will need to have a good understanding in order to uphold it. (same comment as above) The OSA does not clearly explain freedom of expression and therefore services providers will need to have a good understanding of the  law and case law. There is a risk that, due to a lack of understanding, services will not properly account for freedom of expression rights when deciding whether to take action againt illegal or legal but harmful content (and the users who have posted this content). Services will need to have a good understanding of the right to freedom of expression in order to uphold it. Governance and accountability  \\tGovernance\\t\"Making external audit requirements is the most effective step towards \\nenforcement and implementation.\"\\t\"Audits and oversight mechanisms, proven successful in various industries including \\ntechnology, is undeniable. In democratic contexts, products entering the market undergo \\nrigorous scrutiny, encompassing external oversight to assess the implementation of risk \\nmitigation measures. Implementing independent third-party audits ensures robust inspection of systems, safeguarding individuals throughout technological processes.\"\\nGovernance and accountability  \\tcosts of external audit\\texternal audits are more effective despite the costs\\t\"External audits offer superior effectiveness, despite the associated risks and costs, \\nmirroring the success seen in other sectors where mandatory regulations, such as seatbelt laws \\nand vaccine clinical trials, have vastly improved safety standards. Similarly, AI and online \\nsystems benefit from rigorous safety testing and auditing to enhance user protection.\"\\nGovernance and accountability  \\t8.117\\t\"Ofcom highlights current practices by large user to user and search\\nservices including Meta, Google, and YouTube, but CCDH is concerned that benchmarking those processes gives the impression\\nthat they are sufficient to meet the aims of the internal assurance and compliance functions required by the OSA. CCDH believes it is a misstep for 8.117 to highlight current measures large services take to assess\\nemerging illegal content risks when it is those same services that launch markedly unsafe\\nnew technology products.\"\\t\"CCDH is concerned with weakness in the sections following 8.108, “Tracking evidence of\\nnew and increasing illegal harm”. Particularly in 8.117, Ofcom highlights current practices by large user to user and search\\nservices including Meta, Google, and YouTube. It is reasonable to reference these\\npractices, but CCDH is concerned that benchmarking those processes gives the impression\\nthat they are sufficient to meet the aims of the internal assurance and compliance\\nfunctions required by the OSA. The processes noted in 8.117 are essentially user risk identification processes used for the\\nidentification of corporate risk and business threats. While these have relevance to illegal\\nharms and were pointed to by these platforms in their response to Ofcom’s 2022 Illegal\\nHarms Call for Evidence, they are not processes designed for the primary purpose of user\\nsafety. Indeed, CCDH has compiled extensive evidence of the practices commended in 8.117 as\\ninsufficient to secure user safety in the face of emerging trends and technologies. In our\\nreport “Horizon Worlds Exposed” (2023), CCDH documented extensive failures by Meta to\\ninstitute basic user safety in its Metaverse products. Even following CCDH reporting on\\nrisks to minors when the app was purportedly 18+, Meta pushed forward with plans to\\nofficially open the VR product to minors. In CCDH’s report “AI and Eating Disorders” (2023), our researchers showed how these\\nemerging technology products, including Google’s Bard AI, generated harmful eating\\ndisorder content in up to 41% of tested results. Ofcom should be cautious about representations from industry, particularly if those are\\nrepresentation of good practice and reassurance that industry is doing all it can. CCDH\\nbelieves it is a misstep for 8.117 to highlight current measures large services take to assess\\nemerging illegal content risks when it is those same services that launch markedly unsafe\\nnew technology products\"\\nGovernance and accountability  \\taudit failures and limitations\\t\"At present, there are a limited number of entities capable of conducting large-scale audits\\nof the largest platforms regulated under the OSA. Some of those theoretically capable are\\nirreparably compromised by existing ties to industry and a litany of auditing failures\\nrelated to profit prioritisation.\"\\t\" The nascency of online safety regulation means there is only limited evidence concerning\\nindependent third-parties auditing digital platforms for illegal content risk management. At present, there are a limited number of entities capable of conducting large-scale audits\\nof the largest platforms regulated under the OSA. Some of those theoretically capable are\\nirreparably compromised by existing ties to industry and a litany of auditing failures\\nrelated to profit prioritisation. CCDH believes that the large-scale failure of major auditing firms must be kept in mind\\nwhen designing future requirements for the audit of regulated services. Ofcom should\\ntherefore foster optimal conditions for a diversity of audit practitioners with high levels of\\nindependence from commercial interests. Fundamentally, CCDH believes this is further evidence of the need to increase data access\\nfor academics and non-academic experts in civil society. Breaking the current status quo\\nof information asymmetry, which has allowed platforms to (1) deny the extent of the\\nillegal content risks on their platforms or (2) hide behind rubber stamp “audits” from\\ncompromised auditors, is critical to fostering an efficacious auditing environment. Fostering optimal conditions will require a diversity of auditors and auditing organisations\\nwith high levels of independent data access and research competency. This cannot be\\nachieved by relying on compromised auditing firms and without widening data access to\\nindependent researchers. CCDH has evidenced the rapid erosion of transparency and data access pathways which\\nhad previously been used to independently assess platforms’ successes and failures in\\nmitigating illegal content risks. To give just one example, CCDH used the TikTok Creative\\nCenter to assess the viewership of different types of content on the platform, including\\nsuicide and eating disorder promotion and disinformation related to the October 7th\\nterrorist attack in Israel. In 2023, viewership data was provided for hashtags when\\nsearched in the TIkTok Creative Center. However, apparently in response to criticism,,\\nTikTok no longer displays how many times videos with a specific hashtag have been\\nviewed. This is just one example of the restrictive efforts many platforms regulated under the\\nOnline Safety Act have taken against independent and effective oversight. For this reason,\\nCCDH urges Ofcom to work to increase transparency and data access pathways for\\nresearchers as a necessary component of efficacious auditing under the illegal harm\\nduties, but also in future stages of the online safety regime dealing with children and user\\nempowerment.\"\\nGovernance and accountability  \\tremuneration of senior managers tied to online safety\\tCCDH supports Ofcom scrutinising these incentives, particularly at board-level and for shareholder accountability\\t\" CCDH is aware of the societal impacts caused by senior managers at technology\\ncompanies who receive enormous compensation without repercussions for the\\nconsequences of their products. Ofcom is right to identify the misalignment of business incentives and online safety. Introducing a layer of accountability to the regulator via the threat of personal financial\\npenalty can counterbalance the destructive privileging of shareholder value over online\\nsafety outcomes. At present, the factors contributing to senior manager remuneration are opaque. This lack\\nof transparency obscures key incentives driving senior manager behaviour and business\\nchoices. CCDH supports Ofcom scrutinising these incentives, particularly at board-level\\nand for shareholder accountability. Realigning these perverse business incentives is a\\npriority on the road to ensuring healthier online safety outcomes. Therefore, CDH argues that this question is inversely framed. The primary consideration is not what risks and costs are associated with tying senior\\nmanager remuneration to positive online safety outcomes, but rather what accountability\\nit may provide to rectify the problems apparent in the present\"\\nRecord keeping and review guidance\\xa0\\tCodes\\t\"CCDH believes the rules-based Codes do not strike the correct\\nbalance between the Act’s onus on the service providers\\nto decide specific steps and Ofcom’s duty to assist platforms in compliance with their obligations.\"\\t\"CCDH believes the overarching approach to developing the illegal content Codes of\\nPractice lacks focus on addressing systemic issues and ensuring safety by design. CCDH believes the Codes contain overarching misalignment:\\na. Recommending measures without describing desired outcomes\\nb. Not fully accounting for the register of risks in the Codes of Practice\\nc. Offering a safe harbour from enforcement, removing incentives for platforms to\\ndo anything beyond the measures prescribed\\nd. Over-reliance on “iterative” nature of the codes, establishing a weak baseline that\\nwill be harder to strengthen later on\\nRecommending measures without describing desired outcomes - the approach Ofcom\\nhas taken to developing the illegal content Codes of Practice has been to “set out\\nmeasures we will recommend for services to comply with their safety duties” (11.2). Ofcom has interpreted the term “measures” as specific instructions, which is reasonable,\\nbut CCDH points out that “measures”, by definition, would not exclude describing desired\\noutcomes. The Act is clear: first and foremost, the onus sits with service providers to properly assess the risks users may encounter and decide which specific steps they need to take to\\naddress them (as referenced in “Ofcom’s approach to implementing the Online Safety\\nAct”). A balance must therefore be found between the Act’s onus on the service providers\\nto decide specific steps and Ofcom’s duty to assist platforms in compliance with their\\nobligations. At present, CCDH believes the rules-based Codes do not strike the correct\\nbalance. “Measures” do not need to be exclusively technical. As defined by the Online Safety Act in\\nsection 236(1), measures can be “any system or process” to comply with a duty of the Act. Safety by design being one of those duties, CCDH suggests incorporating a safety by design\\nmeasure throughout the Codes to make them less prescriptive and more holistic. Not fully accounting for the register of risks described in Volume 2 in the Codes of\\nPractice - CCDH will expand on this in response to question 16 (below), but in general\\nterms the the approach set out in Volumes 2 and 3 (identifying risks and the approach to\\nrisk management) does not follow through to the measures described in the Codes. Offering a safe harbour from enforcement - CCDH is concerned with the potential\\nconsequences of offering a safe harbour from enforcement. As stated in Volume 4, 11.7:\\n“Services that choose to implement the measures we recommended in our Codes of\\nPractice will be treated as complying with the relevant duty. This means that Ofcom will\\nnot take enforcement action against them for breach of that duty if those measures have \\nbeen implemented.”\\n CCDH fears that the combination of a tick-box list of measures in the Codes of Practice\\nwith a safe harbour provision against enforcement creates a perverse incentive for\\nregulated platforms to do the minimum necessary to tick-off the measures but do nothing\\nfurther in the knowledge that they are safe from enforcement action. Unaddressed, this combination could result in a situation where a service has flagged a\\nparticular risk within their risk assessment, such as a risk from a product design choice, but\\nwould not implement mitigating measures for this risk beyond what is described in the\\nCodes – even if those measures are inefficient at mitigating that risk. Ofcom could then be\\nprevented from taking enforcement action by the safe harbour protection in 11.7. To address these risks, CCDH suggests Ofcom regularly revisit the enforcement safe\\nharbour and conduct regular assessments of whether it is being misused by platforms. Over-reliance on “iterative” nature of the codes, establishing a weak baseline that will\\nbe harder to strengthen later on – given the weaknesses CCDH highlights in the points\\nabove, CCDH is concerned that over-reliance on the iterative nature of these first codes\\nwill, in effect, establish a weak baseline for the regulatory regime that will be much harder\\nto strengthen later on. Yes, it is true that the Codes can and will be updated. But it is\\ncritical that the first iterations are robust, setting a high bar to propel the future regulatory\\nregime.\"\\nRecord keeping and review guidance\\xa0\\tproportionality\\tCCDH agree that in general we should apply the most onerous measures in our Codes only to services which are large and/or medium or high risk\\t\"However, as answered in Questions 14 and 15, the definitions of “large” and “medium/high\\nrisk” services are currently too exclusionary. It is right that more onerous measures apply to large and risky services, but this necessitates better thresholds for what constitutes large and risky.\"\\nRecord keeping and review guidance\\xa0\\tdefinition of large services\\tthe threshold for large companies is too high for services operating in the UK. \"CCDH would point out a significant differentiation in Ofcom\\'s approach to the risk\\nassessment duties and the codes between large companies (7 million monthly users and\\nabove) and small companies (6.99 million and below). CCDH believes the definition of “large” companies makes theoretical sense (and replicates\\na similar threshold as the EU DSA very-large platform status), but that this threshold is too\\nhigh when in the context of the services operating in the UK. For example, services such\\nas Fortnite or Roblox would not be caught up in this definition of “large service”. To\\ncontextualise, this would leave Roblox\\'s estimated 1.5 million child users gaming on a\\nplatform with lower regulatory obligations. If large platforms like Fortnite and Roblox are\\nnot captured by the regime’s definition of “large services”, that definition is too\\nexclusionary. CCDH understands that additional \"\"risky\"\" status could catch smaller platforms and apply\\nthe same, more onerous obligations. Nevertheless, CCDH is concerned that the bar too\\nhigh for what constitutes a “large”, with large services having more online safety\\nobligations, will thus exempt too many services from requirements CCDH would like to see\\nbroadly applied. Finally, it would be helpful to understand the legal basis upon which Ofcom has\\ndetermined it is acceptable to use size as a factor in determining safety standards online.\"\\nApproach to the Codes\\t\\tThe bar is set too low\\t\"1. The Codes lack focus on the Online Safety Act’s duty to ensure “services regulated by this\\nAct are safe by design” (Part 1.3.a). 2. The Codes suggest a process-driven approach that lacks outcome orientation. Their\\nprimary purpose is improving online safety by reducing illegal online content, not ensuring\\nplatforms meet their obligations under the Act. Platforms meeting their obligations is\\nfacilitatory towards the wider goal, not the goal in and of itself.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9918980598449707,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '3. CCDH is concerned about the disconnect between the registered risks in Volume 2 and the\\nmitigation measures proposed in the Codes of Practice in Volume 4. 4. The effect of this is to set the bar too low in terms of the measures with which regulated\\nservices must comply via the Codes and risks reinforcing the status quo which the\\nlegislation was intended to improve.\"\\nApproach to the Codes\\tcosts assumptions\\t\"CCDH reiterate that\\nthe cost to these companies is secondary to rectifying the harm they have done.\"\\t\"1. This is not a specific CCDH competency. But we would like to register that weighing public\\nsafety from illegal harms (grooming, terrorism, intimate image abuse etc) against costs to\\nprivate companies (some worth billions of pounds) does not align with Parliamentary or\\npublic expectations of what the regulatory framework should achieve. 2. CCDH does recognise that costing is necessary for operating this regime. But as an\\norganisation committed to rectifying the consequences these companies have had for\\nindividuals and society – all while they raked in massive profits – CCDH will reiterate that\\nthe cost to these companies is secondary to rectifying the harm they have done. 3. Finally, it is wrong to suggest that safety stifles innovation or competition. It is also wrong\\nto suggest that if a small service is unsafe, the regulator should not take measures to\\naddress that lack of safety if those interventions could hinder the small platform’s ability\\nto compete.\"\\nApproach to the Codes\\tIntention of the Act\\tIntention of the act was that what services are currently doing is not enough. By focusing on best practice we are falling short.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9852747917175293,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agrees with OSA. \"We wholeheartedly agree with and support the OSA Network statement on the Illegal Harms Consultation, which can be found here:  https://www.onlinesafetyact.net/analysis/osa-network-statement-on-illegal-harms-consultation/ . They express a set of concerns we fully agree with, and more eloquently that we could.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7480513453483582,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We would like to expand on the point about focus on best practice. The Government stated throughout the process of passing the Online Safety Act that most platforms were not doing enough. This certainly came from the premise that best practice as accepted by industry today falls short. By focusing on achieving best practice on a slightly wider scale this consultation fails to deliver against many of the aspirations that drove the passing of the Act. There is also a dangerous circular logic. There is little or no incentive in this consultation for industry to improve best practice (merely achieve it). As a result, best practice is likely to remain static, and future reviews of this guidance will not “raise the bar” as best practice has not changed.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9947323799133301,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is a missed opportunity to have the Act and Codes of Practice drive improvement. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\tRecommendation doesn’t raise the bar by misisng out small services. \"Dame Melanie Dawes stated in the Public Accounts Committee that the initial objective in implementing the Online Safety Act is “is to take what the industry is already doing, put the evidence behind it, and then get everybody doing it, in order to raise the bar and raise the standard”. This section of the code appears to take the level of intervention or mitigation required from “is to take what the industry is already doing, put the evidence behind it, and then get everybody doing it”. This will undoubtedly lead to some improvements, but as most measures, even baseline capabilities such as hash matching of known CSAM, are only required for large and high risk platforms it fails to deliver on the intent to “get everybody doing it”. Ofcom’s analysis of “The causes and impacts of online harm” recognises evidence that offenders often use smaller, less well moderated, U2U services to host and promote content, often linking it (directly or indirectly) from larger platforms, and recognises this as a risk factor. The proportionality of regulation is important, but in other areas of safety we do not allow companies to avoid baseline requirements simply on the basis of size. If you wish to market an electrical appliance, vehicle, food or medicine to the public there are minimum standards for companies at all sizes. Given the volume of evidence Ofcom has presented across all harms on the role of smaller platforms, we believe the current proposals are far too weak. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\tOfcom position of segmentating due to pressure on databases sets up unclear forward path. Ofcom should be clearer in language about revisiting the segmentation of measure around database capacity and when this will ne expected\\t\"With respect specifically to measures related to hashing, the section 14 on Automated Moderation acknowledges this: \\n \\n\\t“In principle, we provisionally consider that, even where they are very small, it could be justified to recommend that services which are high risk to deploy these technologies. However, we are proposing to set user-number thresholds below which services would not be in scope of the measure. This is because to implement hash matching and URL detection services will need access to third party databases with records of known CSAM images and lists of URLs associated with CSAM. There are only a limited number of providers of these databases, and they only have capacity to serve a finite number of clients. Setting the user-number thresholds we have proposed should ensure that the database providers have capacity to serve all services in scope of the measure. Should the capacity of database providers expand over time, we will look to review whether the proposed threshold remains appropriate.” \\n \\nTo expand the capacity of database providers is likely to require investment. It is unclear whether the statement “...we will look to review…” is adequate to enable that investment. Some of the investment required will be within the database providers, which are typically NGOs already struggling with funding in a difficult economic environment. Other investment may be required in companies in the private sector which typically provide the engines of innovation creating new products and services to enable expansion of capacity – whether within regulated services or in the wider commercial community. Such investment is typically made based on a clear expectation of outcome, whether through social impact, financial returns or regulatory compliance, either by organisations themselves or by their funders. As structured just now, there is a danger we remain trapped in a vicious cycle: \\n• Investment in scaling availability of hashing has not been made as the demand does not justify the required investment. • Ofcom has indicated it cannot regulate without that scaled availability being in place, with no indication of when such a review might take place or what would be required. • Were such a review to take place there is no surety Ofcom will recommend a change, and even if a change were proposed it could potentially be a small change bringing a handful of additional platforms into scope. This does not create a driver for investment to break the cycle. Ofcom should make a clear statement, such as “We will review the capacity of database providers annually and hope to be able to bring Xxx additional platforms in scope for this requirement within 6 months of capacity becoming available”. This would provide a much clearer basis for the investment needed for future expansion. Any clarity Ofcom can provide would help, including \\n• Which services (size, profile) Ofcom would have sought to place this requirement on if capacity were not an issue \\n• Under what circumstances and on what timescale a review might take place \\n• What factors the review might consider \\n• What Ofcom would be seeking to achieve with the review \\nClarity could create the incentive needed for investment in change. \"\\nAutomated content moderation (User to User) \\t\\tPromoting future innovation - current tech reccomendations are old and mght signal industry can realy on these technologies. \"Dame Melanie Dawes also referred commitments to “raise the bar” and “raise the standard”. A huge volume of discussion in and around the parliamentary process for what is now the Online Safety Act centred on how new technology could improve online safety, and empowering a regulator to require the use of such technology where appropriate. We believe that technology is a necessary component of improving online safety because of the need to operate at scale. Technology can also operate in ways the protect privacy by avoiding unnecessary moderator viewing of private content. We know that deploying people as moderators has a human cost as well as an economic one. Moderators frequently report experiencing poor mental health and trauma from the content they are required to review, and there have also been reports of suicide, and of moderators becoming addicted to exactly the sort of toxic content they are paid to remove. Technology that can minimise and support human intervention is crucial. For these reasons the UK government, through DSIT and the Home Office, has continued to express a strong desire for innovation in online safety technology. This is re-affirmed in the recent MoU with the Australian government.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9935213923454285,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'In doing so it has identified areas where it would like to see innovation and promoted Online Safety as an area for investment for innovation at all stages from fundamental research in Universities through to commercial development at later Technology Readiness Levels. The technologies mentioned in the section on Automated Content Moderation of this consultation are not at the cutting edge of innovation: Hash matching dates back to 1979 and has been in use for CSAM since at least the 1990s Perceptual hashing dates back to 1980 and PhotoDNA for CSAM to 2009 Keyword matching is almost as old as computing – it would have been familiar in Bletchley Park in WWII URL matching is almost as old as the internet, dating back to the 1990s There is a real danger that this signals to regulated services that they can rely on old technologies, and that there is no need for them to invest in newer technologies either through internal development or by buying in. This also fails to signal to investors in innovation elsewhere, from research councils and Innovate UK to private sector Angel and Venture investors, that there is any incentive to create new technology.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.809112548828125,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We have heard from Ofcom on a number of occasions that they will seek to continuously review these codes and “raise the bar”. However, there is little clarity here or elsewhere on how this will happen. We believe that a high functioning innovation ecosystem for Online Safety would: Have a clear understanding of where the regulator would like to be able to act, and how and when it would be able to do so. Feel confident to invest in the development of new online safety technologies to the point where there is clear evidence for their efficacy in addressing harm  Have a reasonable expectation that the regulator would encourage or mandate the use of technologies with a sufficient evidence base Have a reasonable expectation that the actions of the regulator would create conditions where the technology would be able to deliver the desired outcomes in online safety and provide return on investment (whether measured in social outcomes or financial ones). Currently none of these conditions is true. We would therefore encourage Ofcom to indicate now: How and when codes will be revised Priority areas where Ofcom would be keen to recommend or require use of automated moderation technology were it available, and what evidence would be required This would play a significant role in enabling investment in technology development aligned with Ofcom’s goals, creating a virtuous cycle of innovation and mitigating the damage done to the online safety technology ecosystem by these initial draft codes. While slightly tangential to this consultation, it is also worth noting that outside of regulated services, the availability of data to determine technical approaches, train, and/or test innovation is often unavailable. It is very hard to build tools to detect harm if there is only anecdotal data about how that harm takes place. We encourage Ofcom to consider whether it can contribute to bringing together innovators and harms insight, training and/or test data to help build a high functioning ecosystem. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\tCritical of our approach to false positive rate. \"14.26 False Positives When considering the performance of detection technologies “false positive rate” is only one relevant component. False Positive Rate tells us how many false positives will be generated for a given volume of content  Nature of False Positives tells us the characteristics of these false positives Consequence allows us to explore what the impact of these false positives is on the users rights. Assessment of false positive rate without consideration for the nature of false positives and what the action following detection (and consequence thereof) is meaningless, and the consultation documents should reflect this. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\tDisagrees with statement \"we are aware of recent research that has indicated perceptual hashing algorithms could be repurposed to add hidden secondary capabilities\"\\t\"We believe this statement to be inconsistent with the terminology in this document and therefore incorrect. The use of the term “perceptual hashing” up to this point in the document appears to describe technologies such as PhotoDNA which match a specific hash against a database, usually using some form of Euclidean distance. These solutions rely on the integrity of the database, and the measures for ensuring the integrity of the database are described elsewhere in this consultation. There are multiple ways of verifying end-to-end integrity of the system as the original database entries can be reviewed by humans, and map one-to-one onto hashes through a deterministic mathematic process. Either exact matching or a well know heuristic (typically Euclidian distance) is used to determine matches. The paper referenced at 197 does not describe perceptual hashing in this sense. Instead it describes a system where a deep learning model is fed with CSAM images non CSAM images to “train” it. The output of this training process is a “model”, often built on top of a base model. This model cannot usually be mapped back onto the training data in any human understandable way, and the authors of the paper demonstrate that a “dual purpose” model can be built which is effectively indistinguishable from a single purpose one. The lack of explainability and transparency is a risk across many AI and Machine Learning technologies. We believe this is, from a technology and impact perspective, very different to “perceptual hashing. We believe the statement should more correctly read:  “Further, we are aware of recent research that has indicated deep learning algorithms could be repurposed to add hidden secondary capabilities.197”  This might be a justification for suggesting caution in the use of such models, but is irrelevant to the performance of perceptual hashing as described elsewhere in this document. We are making the assumption that it is not Ofcoms intention to include deep learning models within the term “perceptual hashing”. If it is Ofcom’s intention that deep learning approaches should be seen as a form of perceptual hashing then many of the other statements about performance require significant revision to reflect characteristics of deep learning models including lack of explainability and traceability, and risk of unintended bias. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\tConsidere the biases in paragraph 14.54 on other harms\\t\"This is an excellent description of the likely biases in hashing or perceptual hashing approaches. We suggest it would be useful to consider the consequences of these biases on different harms.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.99155592918396,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Public discussion of bias often focuses on disadvantage and exclusion which relates to the creation of new harms or amplify existing biases (and prejudices). Typically we are talking about the introduction of a measure that leaves some group in society worse off than they were under the previously existing measures. The introduction of perceptual hashing as Ofcom proposes does not have a primary effect of making any group in society worse off than if detection were not introduced. There could be a second order effect that if offenders understood that CSAM containing some groups were less detectable they would specifically target that group to evade detection increasing inequality. While still undesirable and something that should be used to drive continuous improvement, this is not the same as the type of bias which excludes groups from participation or targets them unfairly. We assume from context that Ofcom has taken this into consideration, but we believe that this subject should be covered explicitly, probably at this point 14.54 so that at the same time as acknowledging potential bias, Ofcom explains its reasons for believing this level of bias is not itself a barrier to deployment. We also suggest that explicitly stating databases must avoid systematic bias within their control would be helpful. For example by adding a statement in A15.23 that databases should determine addition of content solely based on whether or not it is CSAM, and ensure minimisation of bias in processes making that determination. If some database systematically refused to include CSAM relating to a particular gender, sexuality or ethnic group it should be clear that was not acceptable to use that database for the purposes of complying with this regulation. For the avoidance of doubt we have no reason to believe any such bias exists in any database today. \"\\nAutomated content moderation (User to User) \\tCSAM hash matching\\t\"Question on paragraph 14.109: There is an implicit assumption that a new risk assessment on the service deemed low risk would identify the presence of CSAM and thus increased risk of CSAM in future. If there is no pro-active detection of CSAM and no pro-active moderation, how would the low risk service know that it had been used for CSAM? \"\\t\"There is an implicit assumption that a new risk assessment on the service deemed low risk would identify the presence of CSAM and thus increased risk of CSAM in future. If there is no pro-active detection of CSAM and no pro-active moderation, how would the low risk service know that it had been used for CSAM? This information could come into being if a user report had been made, but even in public services there is often the ability to create content in such a way that it is hard to discover even though Appendix 9s guidance would suggest that it has been “communicated publicly” for the purposes of the act. Such content can then be shared by a group of offenders, none of whom is likely to report it. Unless another user stumbles across it, no report will be made. We therefore believe that the assertion that “new risk assessment would identify CSAM” is likely to be incorrect in most practical cases. In practice, small platforms could easily be oblivious that they were being used by offenders. We believe this is a compelling reason to increase the scope of application for hashing in automated moderation to a wider proportion of platforms, including smaller ones. \"\\nAutomated content moderation (User to User) \\tmore measures to prevent SGII\\t\\t\"Volume 2 of the consultation document recognises SGII (6C) recognises the increasing prevalence of SGII and the harm it causes, yet the recommendations for Automated Content Moderation do offer nothing to combat this type of harm except where imagery is already in hash lists (by which time huge harm has doubtless already occurred. Preventing SGII should be a priority, and we believe that for large or high risk platforms there are measures that could be recommended by Ofcom. \"\\nUser access to services (U2U)\\tAge assurance to combat SGII\\tAge assurance combined with ACM measures could combat SGII\\tAge Assurance technology is already recognised by Ofcom as an effective solution in the recommendations relating to pornography. If this same technology were applied on regulated services, it would be possible to reliably identify which users are children. Automated content moderation (User to User) \\tUse nudity filters combined with AA to prevent SGII\\t\\t\"There are very few legitimate use case for children to either post content containing nudity. There are classifiers which can identify nudity to a very high degree of confidence. Combining age assurance (knowing which users are children) with nudity detection (focused on those accounts) could play a hugely important role in preventing children posting content containing nudity including SGII This could be used block upload with an option to appeal blocking with a moderator. In this mode the SGII is not seen by anyone protecting the child’s privacy. An alternative would be conventional referral to moderator for review, with the consequence that at least one person will view the SGII resulting in some intrusion to a childs privacy (albeit with good intentions) We would hope that in either case the response be appropriate and ensure that children were supported and not criminalised for their actions, which are almost always misguided or coerced rather than criminal in intent. Nudity detection is not 100% reliable (no technology is) but the consequences of a false positive are limited if there is moderator review or appeal available. By limiting application of the technology to accounts of children, false positives have no impact on the adult population. \"\\nAutomated content moderation (User to User) \\tUnknown SGII\\tWays to detected SGII\\t\"New imagery including SGII will not be detected by hashing until it has been discovered through other means and added to databases. There are classifiers which are focused on detecting CSAM. The accuracy of these detectors is sufficient that they should play an important supporting role for large or high risk platforms in detecting CSAM including SGII For platforms that do not permit nudity accuracy is very likely to be high, as false positives tend to come from misinterpretation of age rather than activity. For platforms that do allow nudity it may be appropriate to set higher thresholds and/or use approach based on “strikes” (where moderator intervention occurs when a certain number of items are flagged), which could happen at the level of user accounts or groups. Information from detection could also be combined with other risk factors for individual users or groups. Platforms have publicly claimed to use factors including behaviour, behaviour based age estimation, and metadata including which users are communicating with each other to generate risk scores, and CSAM classifiers would clearly add valuable information to such tools. Given the availability and level of demonstrated reliability and efficacy of these technologies both in online applications and in Law Enforcement, we believe these should at the very least be referred to as a route that large or high risk platforms should consider using in mitigating risk relating to SGII and previously unknown CSAM more generally, with a requirement to demonstrate effective alternative capabilities to detect these categories of content if these measures are not adopted. \"\\nUser access to services (U2U)\\t\\tOur measure suggests removing accounts on lower legal grounds then found in UK law. Also doesn\\'t account for impact on FoE. Ofcom suggests that “Accounts should be removed if there are reasonable grounds to infer they are run by or on behalf of a terrorist group or organisation proscribed by the UK Government.” Such a categorical approach, is based on a much lower standard of evidence (“reasonable grounds to infer”) than it would have been required under UK Criminal law. It also does not take into account the serious harms to freedom of expression, including access to information and important evidence of crimes including human rights violations, that such removals can cause. User access to services (U2U)\\t\\tNot all accounts engage in terrorist activity. Designated terrorist organizations are sometimes state-sponsored, part of elected governments, or have the resources to form quasi governments. Not all of their activities and accounts engage with terrorist activities, while some engage with providing public services and announcements. In certain circumstances, some designated terrorist organizations have governmental power obliging the local population to join compulsory military service, for example. Association with such accounts might not even be voluntary. User access to services (U2U)\\t\\tCategorical / low evidence approaches to suspending accounts (and removing content) loften leads to removal of reporting. \"Furthermore, despite Ofcom’s acknowledgement in A2.4 that “it is not an offense to portray terrorism (for example in a video clip from a film or TV show) or to report on terrorism (for\\nexample as news or current affairs),” categorical and low-evidence approaches to suspending accounts and removing content often leads to removal of reporting, and even condemnation, especially when combined with automated content moderation.\"\\nUser access to services (U2U)\\t\\tThinks we shouldn\\'t ban accounts that are on a list. Thinks we should focus on the info they are disseminating (and thinks our approach potentially inconsistent with what we say in ICJG). Instead of the proposed approach, the decision to remove accounts, and the access to all the information provided by such accounts, should be based on the type of content information such accounts are disseminating, rather than the fact accounts are on a list. Here is where CCAN very much agrees with Ofcom that: “Services should consider the purpose and meaning of content when making illegal content judgements, having regard to the whole context in which it appears. Ofcom would take into account a user’s right to freedom of expression in enforcing the safety duty.” (A.2.4, p.19)\\nUser access to services (U2U)\\t\\tThinks our approach is broader than ICJG and will lead to \"collateral censorship\". \"However, A.2.18, allows for a broader interpretation: “Content which does none of the above, but which relates somehow to a proscribed organisation, may still be illegal content.”. Considering accounts held by proscribed terrorist organizations as of higher risk sounds legitimate. However, in practice since the companies are usually very risk averse, there are limited attempts to contextualize illegal content and prefer not to allow for any kind of proscribed organization to have an account (a practice called ‘collateral censorship’).\"\\nUser access to services (U2U)\\t\\tOfcom\\'s giving service provides a \"blank cheque\" to breach basic legal standards to preserve human rights (with consequences for decisions in UK law). In other words, Ofcom departs from positive obligations under UK law to determine criminal conduct, and gives a blank check to service providers to apply standards that breach basic legal standards to preserve human rights. This delegation of powers to the private sector under lowered standards could ultimately lead the courts to declare takedown decisions illegal under UK law. It is worth noting that the lack of transparency around removal notices made under ToS by the UK police have already been critiqued from a human rights perspective, including by the Oversight Board for Meta in a case 1 where it considered a request to remove a “drill rap” video under the company’s terms of service rather than through a legal order (https://www.oversightboard.com/decision/IG-PT5WRTLW) \\nUser access to services (U2U)\\tContent Moderation (U2U) / Search (?)\\tRemoval of content can lead to the destruction of important evidence. We should work to create an evidence preservation mechanism, to ensure such evidence isn\\'t lost. Removal and takedowns of certain types of content can result in harm including destruction of evidence, such evidence can be critical for law enforcement and/or international bodies like the International Criminal Court or investigations being carried out by the United Nations. This is further highlighted by CCAN members in this report for the Global Internet Forum to Counter Terrorism (GIFCT) and this whitepaper. Such removals would hinder the UK’s ability to uphold the Call commitment to: “Ensure appropriate cooperation with and among law enforcement agencies for the purposes of investigating and prosecuting illegal online activity in regard to detected and/or removed terrorist and violent extremist content, in a manner consistent with rule of law and human rights protections.” CCAN suggests Ofcom works to create an evidence preservation mechanism when such content does need to be removed for legal reasons. CCAN maintains that Ofcom should use a more holistic approach (if allowed by law) and consider the context and content by undertaking a human rights impact assessment to ascertain that informational content does not become inaccessible. Approach to the Codes\\t\\tSupports us working to identify proactive approaches to emerging threats, rather than focussing on reactive measures.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9930081367492676,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They can help us do this. We recommend a stronger, proactive approach to addressing emerging threats as opposed to putting in place reactive measures. This must be done together with the tech sector and intersecting subject-matter experts and organizations. CCAN members would be willing to consult with the UK Government on such initiatives\\nApproach to the Codes\\tSegmentation\\tCodes should not apply to smaller, less well-resourced downstream search services \\tRecommend that Ofcom’s reviews the “Search Risk Profile” and risk assessment process more generally to ensure that smaller, less well-resourced downstream search services do not inadvertently fall into the “multi-risk” or “specific risk” categories when larger services, which control the majority of the content shown on downstream services, will already be subject to these obligations. Approach to the Codes\\t\\tLicense agreements for content from general search engines could conflict with Code requirements\\tDownstream search services enter into licensing agreements with general search services that provide access to services such as APIs, images, and search advertisements from the general search service’s advertising platform. As a standard practice the agreements limit how the licensee can use such content/services, which could mean smaller entities could not comply with Codes without being in breach of the Licensing Agreement. This is avoided if the responsibility [for safety duties] lies with the licensor. Automated content moderation (User to User) \\t\\tMeasure 4A and 4G should only apply to the general search service that owns the content, not a downstream licensee\\tProposed measures 4A and 4G should only apply to the general search service that owns/controls the underlying content on the downstream search service; they should not apply independently to the same content on the downstream search service, which is unable to undertake “content moderation” (including automated content moderation) under a Licensing Agreement. Requiring downstream search providers to undertake content moderation may materially breach the terms of their Licensing Agreements causing legal liability for the licensees. User reporting and complaints (U2U and search) \\t\\tThese measure should be adjusted for licensing agreements\\tProposed measures 5A, 5D, 5F, 5G and 5H, in so far as they relate to deindexing, downranking, reranking and use of proactive technology, each need to be adjusted to take account of Licensing Agreements and that the licensors will already be subject to these obligations in relation to the same content. Approach to the Codes\\t\\tDDG cannot secure by contract that the proposed measures will be met. Contrary to paragraph 11.67 in Annex 4, DuckDuckGo cannot secure by contract that the above measures are met. Cumulative Assessment\\t\\tThe codes a disporportionate and potentially very burdensome for downstream search services\\tUpstream/general search services will have already complied with the safety duties. There should be flexibility to disapply sections of the Codes that do not apply because the risk is not present, the functionality of concern is not in the product, because the user profile doesn\\'t demand it (i.e. not accessible to children) or the safeguard is applied by another party in the supply chain. For the same reason, downstream search services should not be \\'multi-risk\\'. Content moderation (User to User) \\tFOE \\tContent moderation - include best practice on FOE and due process \\t\"- Content moderation should protect already vulnerable or marginalised groups (activitsts, radicalised or queer communities, people posting, non-Western languages)\\n- Moderators should be trained on above\\n- Access to rules, policies, complaints processes in user\\'s chosen language\"\\nUser reporting and complaints (U2U and search) \\t\\tSupport proposals. More clarity and specifics on appeals process for instances of wrongful removal.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9885706305503845,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Support proposals.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9834073185920715,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'More clarity and specifics on appeals process for instances of wrongful removal. User reporting and complaints (U2U and search) \\t\\tFocus on accuracy of complaints removals, not only speed\\tDon’t put the burden on users, ensure services are incentivised to prioritise accurate decisions. More guiance on how to balance removal accuracy and swift takedowns. Ofcom should implement enforcement provisions to encourage a prioritisation of accuracy\\nUser reporting and complaints (U2U and search) \\t\\tTransparency by government and Ofcom of government initiated removals\\t\"Greater transparency of government complaints, by Ofcom and gov\\'ts, by requiring companies to publish governemnt requests leading to content and user removals. ECHR ruling saying not proportionate for governments to require ercryption to be removed or limited to target criminals.\"\\nAutomated content moderation (User to User)\\tPublic/Private\\tEncrypted messaging should be considered private communications\\tEncrypted messaging should be considered private communications\\nGovernance and accountability  \\t\\tAgrees with who the measures apply to and approach is proportionate to risk of harm\\tWelcomes that smaller services in most cases will only need to name a person accountable to the most senior governance body and that most measures only relate to larger multi-risk services. Governance and accountability  \\t\\tAsks for targeted guidance and support for small businesses\\tNotes that for most small businesses the accountable person is likely to be the business owner and they would need more approriate guidance to assess and manage risks.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9629752039909363,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Would like to see this published following the codes of practice. Also notes that greater clarity would help small businesses avoid expensive external advice. Content moderation (User to User) \\t\\tSmall businesses may need support with implementation and examples of best practice\\tSystems and processes will not be at the same level of sophistication as those of larger services. This means that they will need greater support in relation to implementation and clear examples of best practice, so that cost-effective measures can be put in place, particularly for those that are lower risk. Content moderation (User to User) \\t\\tSmall businesses more likely to use human moderation and targetted guidance to help them adhere to the requirements\\tSmall businesses use human moderation to a greater extent than a larger ones, and therefore, expect the guidance will help those businesses adhere to the requirements by making suitable adaptations to their existing processes and policies. Approach to the Codes\\t\\tPositive about codes not being applied to everyone and flexibility offered \\tIt is positive to see that the consultation’s aim in mitigation of harms is not a one size fits all approach and that implementation will be largely applied to services with greater harm and risk levels, given the costs associated with compliance. Also agrees that businesses are able to adapt existing mechanisms, processes, and policies\\nContent moderation (Search)\\t\\tShould be proportionate to service\\'s ability, esp where small businesses use human review rather than automated tools. Highlights that small businesses use human review and reporting and if all search services are expected to de-index or down-rank illegal content, it would be disproportionate. Asks for more details and accompanying guidance on how small businesses relying on human review can implement this. User reporting and complaints (U2U and search) \\t\\tConcerns around small businesses setting up an appeals process due to limited resources. Small businesses have limited resources and there should be guidelines on how the appeals process could be handled efficiently, with a clear and easy to follow criteria set on handling appeals to avoid disproportionate impact on resources. User reporting and complaints (U2U and search) \\t\\tAsks for clear timeframes to deal with complaints\\tAppropriate guidance to be shared which would indicate expected timeframes for a complaint response, or if not, give a clear steer about reasonable timeframes, particularly for smaller businesses taking into account their limited availability of resources. For example, acknowledging receipt of complaints, which may be automated, may take longer to set up if the systems do not allow doing so at present. User reporting and complaints (U2U and search) \\t\\tSupports dedicated reporting channel for fraud for all larger services with medium or high risk of fraud\\tSubmits evidence from own research on the experience of small businesses impacted by fraud. Supports DRC for fraud and and for trusted flaggers such as HMRC, DWP, and NCA \\nTerms of service and Publicly Available Statements\\t\\tAgrees with the proposal, but requests for tools for small businesses. Similar to the format of data privacy statements, asks that Ofcom considers similar tools to that of privacy statement generators where businesses can input information to help them to be customised to their needs. This could help to alleviate any uncertainty as well as reduce some of the associated compliance burden. User access to services (U2U) \\t\\tSupports the proposal, but asks that ban is also reviewed through human interaction and that ban is extended to IP addresses\\tTo prevent users getting classified in error, where there are automated systems they should be also reviewed through human interaction. It would be appropriate to ban usernames, emails and IP addresses. It is likely that a username ban on its own or an email ban would not be a significant deterrent, so a combination of factors should be considered where possible. User access to services (U2U) \\t\\tAsks that username ban restricts new users from taking up the banned username to limit impact on other law-abiding users\\tConsideration should also be given to usernames which remain available following association with certain banned groups, and the reputational impact on other users if that username becomes available and they unknowingly take it up. Cumulative Assessment  \\t\\tAgrees with the proposal, but requests for more support for small businesses and adopting a staggered approach\\tWhere small businesses will identify multiple and significant risks, asks that Ofcom work with those small businesses to help them comply, consider the costs involved for them, and adopt a more staggered approach taking into account their immediate capabilities and what they can do over time\\nRecommender system testing (U2U) \\t\\tArgues that recommender systems do not pose increased risk of harm and that depends on the content that is being recommended. Hence disproprotionate to apply this to their service. \"Ofcom states that recommender systems “have an increased risk of harm related to encouraging or assisting suicide and hate offences”. Service argues that the risk factor should explicity recognise that this is highly dependent on the content that is being recommended. Service only provides travel related content, and does not provide personalised user specific homepage with user-generated content or a live feed. Service is not designed to be binged.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9906885623931885,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'EVen if users engage with harmful content on the platform, their recommender system is not designed to create recommendation on harmful content. Hence, disproportionate to impose requirements on a service to collect safety metrics when implementing changes to their recommender settings in circumstances where that service has evaluated the effect of its recommender settings and concluded that it does not materially affect the likelihood or impact of the risks posed by\\nillegal content.\"\\nAutomated content moderation (User to User) \\t\\tQuestions the approach to assessing risk of image-based CSAM and suggests that services look at the frequency of actual occurrence of imapct in conjunction with the nature of platform \\tCurrently Ofcom states that if the service allows for image/videos to be uploaded and that it has two or more relevant risk factors, then a service is likely to be medium risk for image-based CSAM and must conduct hash matching. However, service rejects this idea and suggests that the likelihood of risk materialising should be based on actual occurrence and the nature of the platforms. Suggests Table at page 28 of the RAG Annnex is revised. Approach to the Codes\\t\\tMore guidance needed on who is a \\'user\\' \\t\"Service highlights that the definition of user used by them relates to number of unique users who make a booking through the platform, While they also look at another definition - Number of logged-in users who engage with the platform but not ultimately book, service argues that correct metric for them will be users who transact/make a booking as they don\\'t think their content poses risks to a person who is simply navigating. A person who is navigating without making a booking is not a \\'user\\' because they don\\'t receive messages from supply partners, or pay for anything that could be potentially a fraud listing. Further, the number of people who visit the website either without logging in or logged-in without transaction, is unreliable as the service cannot de-duplicate visits from the same person. Effectively, multiple visits by same person will be counted as several users while in undesirable. \"\\nApproach to the Codes\\t\\tSuggests that multi-risk definition should only apply when service poses at least one high risk and a further medium risk of harm\\tSuggests that multi-risk definition should only apply when service poses at least one high risk and a further medium risk of harm\\nContent moderation (User to User) \\t\\tBroadly supports proposals, esp that Ofcom will not take a view on individual pieces of content. Provides evidence and examples of current practices within Content Moderation\\t\"Provides links to published materials on content moderation policies - transparency report, and content guidelines.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9931209683418274,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Gives an overview of its current content moderation process - \\n(1) Content is checked by machine learning algorithm, approved within seconds. Content checked includes images, guest reviews, partner responses. Two core machine learning models with 43 different language capabilities. 1 core model for textual content and 1 for images. Core models are content classifier models designed to detect illegal content and content that violate the service\\'s policies. (2) Content not approved by machine learning models is directed to moderators who review within 5 business days. (3) Algorithms also perform daily checks, so content published can be removed. (4) Service also selects a random sample of automatically approved content to moderators to ensure quality of automated approvals meet acceptable range. (5) Where a content has been removed, user an appeal the decision or edit content to submit a new version. (6) Content moderators receive 6 hrs/month of training. \"\\nUser reporting and complaints (U2U and search) \\t\\tProvides link to its reporting form\\tAny user of the service can report content that they think are illegal using the reporting form. User access to services (U2U) \\t\\tHas ongoing practice to suspend/terminate user accounts\\tService alerady has the right to suspend or terminate user accounts if they identify fraudulent activity or severe offences. Enhanced user control (U2U) \\t\\tMeasures disproportionate to type of service provided where limited interactions occur between users\\t\"Provides examples of features that reduce or mitigate risks posed by the service, arguing that enhanced user controls should take into account the type of service provided and the functionalities of the service -\\nUsers only permitted to upload specifc content - travellers can only submit reviews and photos of travel experiences where fake content is automatically moderated, supply partners can only upload photos and content on the travel experiences where fake content is moderated through ML models. Users don\\'t come on the platform to chat with other users, and the service does not have features that allow content to become viral. Service argues that its content moderation, reporting, and right to suspend/terminate user accounts are sufficient measures to prevent harms.\"\\nEnhanced user control (U2U) \\t\\tMore guidance needed on who is a \\'user\\' to determine whether a service is large\\t\"Service highlights that the definition of user used by them relates to number of unique users who make a booking through the platform, While they also look at another definition - Number of logged-in users who engage with the platform but not ultimately book, service argues that correct metric for them will be users who transact/make a booking as they don\\'t think their content poses risks to a person who is simply navigating. A person who is navigating without making a booking is not a \\'user\\' because they don\\'t receive messages from supply partners, or pay for anything that could be potentially a fraud listing. Further, the number of people who visit the website either without logging in or logged-in without transaction, is unreliable as the service cannot de-duplicate visits from the same person.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9839582443237305,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Effectively, multiple visits by same person will be counted as several users while in undesirable. \"\\nEnhanced user control (U2U) \\t\\tSuggests that multi-risk definition should only apply when service poses at least one high risk and a further medium risk of harm\\tSuggests that multi-risk definition should only apply when service poses at least one high risk and a further medium risk of harm\\nRecommender system testing (U2U) \\t\\tArgues that new risk assessment triggered by significant change to recommender systems should not apply when the service has concluded that the system itself does not affect  any material risk posed on the service. Where recommender systems do not affect the risks posed on the service because of the limited functionality and type of industry the service operates in, then it should be considered sufficient that the service has an up-to-date understanding of the risks posed by the service, rather than having to collect safety metrics everytime the recommender system is changed. Automated content moderation (User to User) \\t\\tShould be applied only to services that are at high risk \\tShould be applied only to services that are at high risk \\nAutomated content moderation (User to User) \\t\\tRestrict analysing content using ACM at the point of upload\\tOfcom requirement to analyse content at the time technology is implemented amounts to general monitoring. Instead ACM should be required at the time of upload and not proactively monitor content on an ongoing basis. Asks ofcom to state this explicitly within the code. Automated content moderation (User to User) \\t\\tDisagrees with using ACM to other types of content, esp. terrorism as undermines users\\' rights to expression of freedom\\tDisagrees with using ACM to other types of content, esp. terrorism as undermines users\\' rights to expression of freedom\\nAutomated content moderation (User to User) \\t\\tACM should only apply to genuinely public communications as it imapcts on users\\' privacy and security of scanning communications that are intended to be private. As in the one-line summary.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934830069541931,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Says setails provided in response to Q14.2 (using old response format) but no 14.2 found in the document. Automated content moderation (User to User) \\t\\tArgues disproportionate to impose this on all services\\t\"Although service may encounter a certain type of fraud (e.g. posting fraudulent listings), do not see heightened risk of content amounting to an offence concerning articles for use in\\nfraud.\"\\nAutomated content moderation (User to User) \\t\\tConsiders the recommendations to be too prescriptive\\t\"Service has a host of effective existing measures in place to identify a range of fraudulent behaviour. As currently drafted, there may be a possibility that platforms chose to do the bare minimum to enjoy the ‘safe harbour’ provided by the Code of Practice rather than taking the risk of pursuing an alternate approach - even if they can demonstrate that it is more robust. \"\\nGovernance and accountability  \\t\\tProvides example of current practice relating to content moderation - Content moderators receive monthly training and content policies are updated from time to time. Content moderators spend approximately 6 hours a month receiving training, reviewing content guidelines and policy clarifications, reviewing their errors and asking questions. Booking compiles FAQs and clarifies grey areas on a regular basis.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8841792345046997,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'When new policies are launched, training decks and videos are provided to explain the new content policies and the appropriate actions the content moderator should take. Governance and accountability  \\t\\tBroadly supports the governance measures proposed\\tBroadly supports the governance measures proposed\\nGovernance and accountability  \\t\\tSupports that requiring services to engage external audit may not be proportionate\\t(a) internal audit functions frequently act independently from the remainder of the business; (b) internal audit functions often have a far more detailed understanding of the risks posed by the service - this leads to more effective auditing of the measures taken to mitigate those risks; and (c) engaging with external audit teams can divert resources away from other projects aimed at mitigating risks posed by a service. Approach to the Codes\\t\\tDisagrees with consultation and outlines concerns for the affect on freedom of speech\\tFocuses on implications of the proposed regulatory framework for freedom of expression and the right to privacy online and believes that the Act’s requirements for online platforms to surveil and restrict online speech will do significant damage to the free flow of information and ideas that the internet has facilitated. Points out that the UK already has laws in place to protect individuals - Protection from Harassment Act 1997, Public Order Act 1986, Malicious Communications Act 1988,  Communications Act 2003, Crime and Disorder Act 1998, Race and Religious Hatred Act 2006\\nContent moderation (User to User) \\t\\tConcerned by the impact Ofcom’s proposals will have on freedom of expression and privacy online, in relation to the removal of ‘user to user’ content\\tFocuses on the suggestion that platforms may be required to break the privacy and security provided to private messaging by end-to-end encryption is troubling. They day that this will create vulnerabilities within messaging services for criminals to exploit or could open the door to a greater level of surveillance. 4 International human rights bodies have recognised the importance of end-to-end encryption. References the Network Enforcement Act 2017 in Germany for evidence\\nContent moderation (Search)\\t\\tConcerned by the impact proposals to moderate search engine content will have on freedom of expression and access to information online\\tMentions many of the concerns set out in the response to Question 18 apply to this section also. However they welcome Ofcom’s decision not to recommend ‘blanket deindexing’ and acknowledgement that this would not be proportionate\\nAutomated content moderation (User to User) \\t\\tDo not agree with automation but welcome Ofcom’s decision not to recommend platforms use automated hash-matching systems\\tOutlines that wherever surveillance is carried out, it should be targetted and based on suspicion in line with the principles generally adhered to in liberal democracies. Concerned that automated systems are likely to result in the removal of lawful content\\nAutomated content moderation (User to User) \\t\\tConcerned that guidance setting out whether content is communicated ‘publicly’ or ‘privately’ could impact on messaging services or functions, and could require services to scan users’ messages\\tConcerned that Ofcom’s guidance makes no distinction between large ‘group chats’, such as those facilitated by messaging services such as WhatsApp, which are protected by end-to-end encryption and large open discussion forums. Concerned of possibility that ‘group chats’ will be considered public content. They mention concern that the guidance states that privately communicated content could later be considered to be communicated publicly, as any designation “may change over time” (Annex 9, 5)\\nAutomated content moderation (User to User) \\t\\tThey welcome Ofcom’s decision not to use automated content-moderation technology for combatting terrorism content given the likelihood of over-removal and the inherent threat to freedom of expression\\tThey believe that social media platforms are ill-equipped to make determinations on the legality of speech, particularly when it comes to making judgements on when expression may or may not fall foul of speech-related criminal offences. Make references to the Terrorism Act 2000. They welcome Ofcom’s decision not to mandate platforms to use automated content-moderation technology to detect and remove material of this nature\\nUser reporting and complaints (U2U and search) \\t\\tThey believe Ofcom should go further in setting minimum standards for appeals and complaints processes\\tOutline that current processes are often ineffectual, automated, lack clear process and content is rarely assessed in the full context in which it was posted. They have included evidence from The Santa Clara Principles (2021)\\nTerms of service and Publicly Available Statements\\t\\tThey welcome Ofcom’s approach ensuring that rules platforms use to moderate content on their sites are accessible and transparent but they have some freedom of expression concerns\\tOutline that content policies should reflect human rights principles and avoid limiting expression beyond the limitations of the law. They believe when setting out rules, platforms should make the text easy to understand.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9972409009933472,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Rules should also be clearly defined and refrain from being subjective. Users should be actively notified by the platform as to any rule changes as this does not happen at the moment on most platforms\\nGovernance and accountability  \\t\\tSupport a safety by design approach but would prefer a higher entry point of governance for new entries into the UK market\\tThey strongly support a safety by design approach being at the heart of the responsibilities of service providers, particularly around child protection. But they would support a higher entry point of governance for new entries into the UK market, where a pattern of responsible behaviour has not been able to be demonstrated to date\\nGovernance and accountability  \\t\\tThey agree with the types of services that Ofcom propose the governance and accountability measures should apply to\\tThey agree with the types of services that Ofcom propose the governance and accountability measures should apply to. Governance and accountability  \\t\\tThey are not aware of any additional evidence of the efficacy, costs and risks associated with a potential future measure to requiring services to have measures to mitigate and manage illegal content risks audited by an independent third-party\\tThey are not aware of any additional evidence of the efficacy, costs and risks associated with a potential future measure to requiring services to have measures to mitigate and manage illegal content risks audited by an independent third-party\\nGovernance and accountability  \\t\\tThey are not aware of any additional evidence of the efficacy, costs and risks associated with a potential future measure to tie remuneration for senior managers to positive online safety outcomes\\tThey are not aware of any additional evidence of the efficacy, costs and risks associated with a potential future measure to tie remuneration for senior managers to positive online safety outcomes\\nApproach to the Codes\\t\\tThey support a risk-based approach to regulating services but they outline that Ofcom should be responsive to the rapidly changing digital ecosystem\\tThey have not provided evidence but they write that in the democratic sphere, there have been examples of smaller services being sources of illegal content such as false communications, which have then rapidly seeded these into services with larger audiences so this should be recognised\\nApproach to the Codes\\t\\tThey have confirmed that they do agree with Ofcom\\'s definition of large services\\tThey have confirmed that they do agree with Ofcom\\'s definition of large services\\nApproach to the Codes\\t\\tThey do agree with Ofcom\\'s definition of multi-risk services\\tThey do agree with Ofcom\\'s definition of multi-risk services\\nApproach to the Codes\\t\\tThey do not have any comments on the draft Codes of Practice themselves\\tThey do not have any comments on the draft Codes of Practice themselves\\n\" Default settings and user support (U2U)\"\\t\\tThey do agree with Ofcom\\'s proposals\\tThey do agree with Ofcom\\'s proposals\\nEnhanced user control (U2U) \\t\\tThey support the proposal but point out that services should be able to demonstrate that verification of identity is water-tight and not being used in a way to spread illegal content\\tThey agree with providing the ability to block users and limit comments, being made available on all platforms, and being easily accessible and well advertised. They go on to mention that verification status can be a useful tool for office holders, and when properly regulated, is an important means to improve trust. But when verification status is monetised greater credibility can be given to accounts that impersonate elected officials or candidates, or which spread false communications\\nEnhanced user control (U2U) \\t\\tThey agree with Ofcom\\'s proposal\\tThey agree that the first two proposed measures should include requirements for how these controls are made known to users but further comment that these options and functionalities should be clearly flagged to all users, including when first setting up an account or profile on a service\\nEnhanced user control (U2U) \\t\\tThey do agree that there are situations where the labelling of accounts through voluntary verification schemes has particular value or risks\\tThey agree and then mention \\'see response to Q37\\'\\nUser access to services (U2U) \\t\\tThey do agree with the proposals but mentioned that there are multiple offences that should have users banned from a platform\\tThey believe that there are grounds for requiring users who frequently/persistently undertake other dissemination of illegal content, harass or intimidate via a service, or who persistently establish accounts to disseminate false communications, should also be required to be barred from the use of a service. They recognise the difficulty in balancing the rights of freedom of expression with such requirements, and believe that the bar should be set appropriately high for blocking usage to be an Ofcom requirement. They also believe that there are grounds for requiring a service to be able to demonstrate it is consistently enforcing its own terms and conditions around user behaviour and that sanctions for spreading illegal material \\nUser access to services (U2U) \\t\\tThey do not have any supporting information or evidence for the options available to block and prevent a user from returning to a service\\tThey do not have any supporting information or evidence for the options available to block and prevent a user from returning to a service\\nUser access to services (U2U) \\t\\tThey believe that it is appropriate to set a blocking period which is commensurate to the nature of the offence. They believe that it is appropriate to set a blocking period which is commensurate to the nature of the offence. Service design and user support (Search) \\t\\tThey do agree with Ofcom\\'s proposals\\tThey do agree with Ofcom\\'s proposals\\nCumulative Assessment  \\t\\tThey agree that the overall burden of Ofcom\\'s measures on low risk small and micro businesses is proportionate\\tThey agree that the overall burden of Ofcom\\'s measures on low risk small and micro businesses is proportionate\\nCumulative Assessment  \\t\\tThey agree that the overall burden is proportionate for those small and micro businesses that find they have significant risks of illegal content and for whom Ofcom propose to recommend more measures\\tThey agree that the overall burden is proportionate for those small and micro businesses that find they have significant risks of illegal content and for whom Ofcom propose to recommend more measures\\nCumulative Assessment  \\t\\tThey agree that the overall burden on large services is proportionate\\tThey agree that the overall burden on large services is proportionate\\nStatutory Tests \\t\\tThey agree that Ofcom’s proposed recommendations for the Codes are appropriate in the light of the matters to which Ofcom must have regard\\tThey agree that Ofcom’s proposed recommendations for the Codes are appropriate in the light of the matters to which Ofcom must have regard\\nApproach to the Codes\\t\\tThey believe that measures should be applied universally\\tThey believe that measures should be applied universally\\nGovernance and accountability  \\t\\tThey write that the proposal is a great improvement but there needs to be more consequences for those who breach the codes\\t\"They say that the mechanism is heavily reliant on not only the dedication and hard work of employees, but also a thorough understanding of the harms. They write that Antisemitism is often referred to as “the oldest hatred” there are nuances that people do not understand; to an uneducated onlooker a picture or a statement might seem perfectly benign, but to many Jews \\nit would not be seen as such.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9964553117752075,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They mention an example was the defence by Jeremy Corbyn of a mural called “Freedom for Humanity”.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9893894791603088,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They also mention it is reliant on the individual having no bias towards all relevant groups - they refer to one instance where a member of Ofcom’s Online Harms team expressed sentiments with regard to Israel. They also believe mechanisms should be put in place to investigate any anonymous or dummy accounts that may be used by previously banned users\"\\nGovernance and accountability  \\t\\t\"They mention it is worrying that \\nsmaller organisations do not have certain requirements placed on them. They say there is no reason as to why smaller service providers should not be subject to staff training for content moderation to enable them to take down illegal harms \"\\tAs mentioned in a previous answer, antisemitism can be complicated to fully understand. Many people do not understand it and its monitoring requires comprehensive training to identify certain phrases and statements. Smaller services should also enable their users to block others who are targeting them or using offensive language and prevent some from commenting on their posts. Not making these rules universal across all services will merely enable them to become hubs of illegal harms\\nGovernance and accountability\\t\\tRecommendation of a stronger gender-based analysis to risk assessments factoring in assistance by subject matter experts. Technology and internet-facilitated hate and gender-based violence disproportionately impact  women and girls, including trans women; 2SLGBTQIA+ communities; indigenous communities; vulnerable youth, including girls and young women, and vulnerable boys and young men at increased risk of sex-torsion. Recomendation: 1. A stronger gender-based analysis to risk assessment and regulation of new technologies, must be applied to counter the disproportionality. 2.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9931676983833313,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'A stronger, proactive approach to addressing emerging threats as opposed to putting in place reactive measures. Governance and accountability  \\t\\tAgree with our proposals if efforts can be made to control the availability of 3D printed firearms, weapons blueprints or files. Chapters A8. Firearms and other weapons (specifically Sub Section titled, ‘3D printing of firearms’) Page 98 and Page 316 of the document titled, ‘Annex 10: Online Safety Guidance on Judgement for Illegal Content.’\\xa0Yes if efforts can be made to control the availability.]\\xa0Once again Files for 3D weapons the more we can do with providers to limit these in terms of availability to minors the better. Better for the minors who may go on to criminalise themselves by turning the blueprint into a weapon and better for the public that we work together to minimise the number of these weapons that are created and then available on our streets. Governance and accountability  \\t\\tProposals in relation to governence and accountability measures in Codes of Practice\\tAgree with our proposals\\nGovernance and accountability  \\t\\tCotent moderation performance targets risk interfering with the correct application of content moderation procedures\\tBusinesses may be incentivised to rush content modertation, or manipulate or artificially influence moderation metrics to meet business targets rather than focussing on correctly identifying and removing illegal content. This would have a disproportionate impact on freedom of speech, as well as requiring services to respond to higher volumes of user complaints / challenges against take-down decisions. Approach to the Codes\\t\\tThe Codes are inflexible, will quickly become outdated and risk inappropriately homogenising platforms\\' approach to compliance in a way that makes their T&S functions not fit for purpose. \"The Codes may lock in practices which quickly become outdated or fail to keep pace with evolving harms, because they incentivise compliance over innovation. The Codes may have the effect of homogenising platforms\\' approach to compliance in a way which does not reflect their varying designs, practices and business models, forcing platforms to adopt methods which are not relevant or effective. The requirement to and the process for justifying alternative approaches / deviation from the Codes measures is unclear and may disproportionately impact smaller, alternatively structured platforms. The burden of justifying deviation from the Codes should remain lightotherwise it risks discouraging platforms from innovating on safety in ways that accurately address their services risks. Further guidance for services who want to deviate from the Codes Measures would be welcomed on topics such as burden of proof for effectiveness; how Ofcom will assesss the safety benefits of features and characteristics,  and considerations around reasonableness and proportionality.\"\\nApproach to the Codes\\t\\tThe thresholds are too low / i.e. they disproportionately impact a large number of services which should not be categorised as multi-risk. The threshold for designation as a \"multi-risk\" service as only requiring presence of 2 of the fifteen illegal harms is too high and will disproportionately affect a large number of services. Approach to the Codes\\t\\t7 million monthly UK user threshold is arbitrary, too low, and does accurately reflect penetration of the UK market relative to some of the larger services. \"Rather than using an arbitrary figure derived from the EU DSA framework, UK should seek a definition which looks at the UK market\\'s unique characteristics (does not expand on this point or provide examples). The definition of large service should take account of different types of users. Distinquishes between logged in and logged out userrs who have very different user experiences on Reddit (logged in users have access to user empowerment and content creation tools that logged out users do not).': [{'label': 'POSITIVE',\n",
       "               'score': 0.9958614706993103,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Definition should also take into account revenue and employee numbers. Cites the Digital Trust & Safety Partnership\\'s SAFE Framework https://dtspartnership.org/wp-content/uploads/2021/12/DTSP_Safe_Framework.pdf.\"\\nApproach to the Codes\\t\\tThe threshold for multi-risk services is disproportionately low. The threshold requirement that a service scores as medium risk for at least two illegal harms is too low and disproportionately wide I.e. it will capture most platforms. Suggests a higher threshold of medium risk for 5 or more harms. Approach to the Codes\\t\\tThe Codes are overly broad and risk homogenising services\\' approach to compliance and failing to keep up to date with the evolution of how illegal harms manifest online. \"Reddit\\'s community governance structure and trust and safety processes focus on data and behavioural patters and less on content and have proven to be effective. Cites Weisenthal Center\\'s 2023 Digital Hate and Terrorism Report Card which ranked Reddit as a\"\"top industry leader\"\" in combatting those harms https://www.digitalhate.net/inicio.php?year=_2023  States Reddit\\'s trsut and safety features incorporate safety by design, and that expecting Reddit to adopt codes of practice which do not take account of that design would be disproportionate and reduce the efficacy of its procedures. States the codes disincentivise innovation which creates the risk that services will lose pace with the evolution of illegal harms online.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8725665807723999,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nApproach to the Codes\\t\\tThe cost ranges vary significantly and do not take account of companies who do not have resources to scale up a dedicated compliance function. States that the cost ranges of the measures vary significantly. States the cost estimate do not take into account \"opportunity cost\". States that smaller companies do not have large compliance teams on call and have to reallocate engineers, designers and other critical staff, taking them off projects which may be critical to business operations and result in reduced or deferred revenue. Content moderation (User to User)\\t\\tReddit has concerns that many of the codes measures are overly prescriptive, do not account for diversity of service types and are inflexible. \"Reddit has concerns that many of the codes measures are overly prescriptive, do not account for diversity of service types and are inflexible. Keyword detection of fraud is not scalable, and diverts valuable trust and safety resource where moderators have to respond to false positives. Content basesd approaches to certain discussion topics would be higher risk for false positivies, where data and signals based approaches would be more appropriate.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9899870753288269,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Some of the Codes measures are completely unsuitable for certain services. Reddit gives the example of the requirement to provide all users the ability to block all comments on their post, which is completely incongruous with the entire purpose of posting on Reddit, which is to initiate a conversation. Reddit further notes that it believes its community moderator model improves safety by empowering its users to take control of the process of content modertation, is scalable and limits opportunities for toxic virality and other harms to spread across the site.\"\\nAutomated content moderation (User to User)\\t\\tFraud keyword detection does not reflect industry best practice and hash matching for CSAM detection, whilst standard practice, may become problematic over time\\tFraud keyword detection is not as effective as behavioural signals proagrammes, which are more reliable and future proof. Keyword lists are likely to fall behind trade craft as bad actors constantly change and evolve language to evade detection. Hash matching and URL detection are generally accepted as standard practice for CSAM detection. However Reddit flagged that there are risks associated with the use of hash databases which are not adequately audited and can become polluted over time with incorrectly hashed or non-violative content. Reddit further noted that the EU as well as other jurisdictions are considering whether proactive CSAM hash matching is compatible with privacy rights. If the EU in particular determines that proactive CSAM hash matching is not compatible with privacy rights UK could end up mandating a process that is being prohibited in other jurisdictions. Automated content moderation (User to User)\\t\\tHash matching, whilst important, is not an infallible approach to CSAM removal as it cannot detect new (1st gen) CSAM and it is susceptible to manipulation and inaccuracy. In H12023, hash matching was only responsible for 6.6% of CSAM removals from Reddit\\'s site - the remainder was identified through other methods including \"proprietary machine learning\" or user reports. Hash matching is only as good as the database it relies on, and issues around governance, availability and reliability of the databases can affect an organisation\\'s ability to run hash-matching effectively. Reddit gave the example in 2022 of the GIFCT hash-bank containing hashes of Rick Astley\\'s \"never gonna give you up\" video, which Meta could not explain. Reddit also noted that when it declined to join the GIFCT due to governance concerns, GIFCT stopped Reddit\\'s access to the hash database on the basis that it was no longer a member. Reddit finally noted that the issues with accuracy of hash-matching require a high level of human moderation / quality control which puts a disproportionate burden on smaller services with smaller headcounts. Automated content moderation (User to User)\\t\\tSmaller services may have issues accessing good quality hash databases due to financial or political reasons. Reddit also noted that when it declined to join the GIFCT due to governance concerns, GIFCT stopped Reddit\\'s access to the hash database on the basis that it was no longer a member. Automated content moderation (User to User)\\t\\tThe burden of keyword detection for fraud  may be disproportionately onerous for smaller platforms because it requires a high level of human review / quality control. Notes that the error rate for fraud keyword detection tends to be high, which could cause congested review queues for human moderators, and draining resources that could be more effectively applied targeting real harms. Notes offenders regularly evolve their vernacular and the requirement to regularly update and review the model may disproportionately affect smaller services and impact safety.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924587607383728,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'States signals such as frequency of posting, viral or high-volume sharing of the same post across different communities by a single user, number of reports relating to the post, high levels of removals by content moderators, numner of \"down votes\" and age of user may be more accurate signals. Enhanced user control (U2U) \\t\\tThe requirement to provide users the ability to block all comments on their post is completely incongruous with the entire purpose of posting on Reddit, which is to initiate a conversation. The requirement to provide users the ability to block all comments on their post is completely incongruous with the entire purpose of posting on Reddit, which is to initiate a conversation. Enhanced user control (U2U) \\t\\tReddit\\'s users value the privacy that Reddit affords by not requiring them to register to use the site\\tReddit argues privacy increases user\\'s sense of safety on the site which may be relevant to the requirement to use user verification schemes. Cites its own research that shows women in particular value digital anonymity and are more likely than men to take advantage of that feature. Research linked here https://www.reddit.com/r/redditsecurity/comments/tyiymt/prevalence_of_hate_directed_at_women/ \\nApproach to the Codes\\tNew measures\\tLikely legal breach of Ofcom\\'s duty without additional measures to protect children from terrorism\\t\"Necessary special protection for children against exposure to terrorism content is missing from OFCOM’s proposals …  If OFCOM were to promulgate the Codes in their current form this would very arguably result in a breach of OFCOM’s duty under the Online Safety Act 2023. In summary, the Act requires OFCOM to generate a terrorism content Code that pursues higher standards of protection for children than for adults.\"  \\nApproach to the Codes\\tSegmentation\\tApplying more measures to large or multi-risk is at odds with evidence that terrorist content occur particularly on small services\\t\"The approach is that stronger responsibilities are placed on large or multi-risk services than on small services. OFCOM’s rationale for this is that if a service is at risk of a single kind of illegal harm, that risk is more likely to be well understood across the organisation. However, this is at odds with OFCOM’s own assessment, which acknowledges that small platforms, with fewer resources to identify and moderate terrorism content, are particularly at risk of exploitation from terrorist actors.\"\\nGovernance and accountability\\tNew governance measure\\tServices should periodically review risk of children accessing terrorist content\\t\"Under ‘Governance and accountability’, all services (or failing that, large or multi-risk services) should periodically review the risk of children accessing terrorism content on their service\" (This is the full response on this)\\nContent moderation (User to User)\\tNew content moderation measure\\tAll services should prioritise avoiding children accessing terrorist content\\t\"Under ‘Content moderation’, all services (or failing that, large or multi�risk services) should prioritise, and required to demonstrate how they are prioritising, the avoidance of children encountering terrorism content on their service.\" (This is the full response on this)\\nDefault settings and user support (U2U)\\tNew terrorism measure\\tGrooming measures should apply to services with terrorist risks\\t\"Under ‘Default settings and support for child users’, the requirements that are currently required for Child Sexual Exploitation and Abuse (CSEA) and ‘other duties’ should also extend to terrorism (this obligation affects only a specific category of services).\"(This is the full response on this)\\nEnhanced user control (U2U) \\tNew terrorism measure\\tEnhanced user controls measures should extent to torrirism\\t\"Under ‘Enhanced user controls’, the requirements that are currently required for CSEA and ‘other duties’ should also extend to terrorism (this obligation affects only a specific category of services).\" (This is the full response on this)\\nApproach to the Codes\\t\\tCodes overly focus on reactive rather than proactive measures. Codes measures recommended focus on reactive measures which put the onus on survivors and victims to report themselves.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9900137186050415,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Automated content moderation (User to User)\\t\\tHash matching recommendation should be extended to include intimate image abuse\\tHash matching recommendation should be extended to include intimate image abuse\\nApproach to the Codes\\t\\tAssumption throughout codes work that services will act in good faith, leading to overrealiance on cost to assess measures. There is an underlying assumption that tech companies will comply and will adopt best practise approaches.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910005927085876,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is not backed up by evidence or example, as most services do not operate with a business model that would support this type of voluntary compliance. For example, research shows many survivors of DA don\\'t recieve responses when they report DA content (and if they do recieve responses they are often dissatisified with the response. THis is despite services having Ts&Cs which ban this type of content. [citations provided]\\nApproach to the Codes\\t\\tThe measures currently recommended focus on take-down rather than proactive safety by design\\t\"Many of the measures focus on take down of illegal content. While the ‘takedown’ approach that Ofcom has focused on will indeed assist individuals on case-by-case bases to take down \\ncontent once it has already been posted, a better approach would be to think \\nabout upstream systems.\"\\nUser access to services (U2U) \\t\\tStrikes/blocking measure focuses on freedom of expression whilst not considering the other human rights impacts\\tBlocking measure emphasises the impact it may have on users as \\'speakers\\' without effectively balancing other human rights impacts of strikes and blocking in terms of protecting users from harm. Approach to the Codes\\t\\t\"The consultation is too business-centric and not sensitive enough to the costs and resources required to cope with the extent of harms affecting women and girls. \"\\tThe consultation appears to assume how companies will comply satisfactorily, or even exceed the recommendations set out by Ofcom, despite points raised furing the Bill\\'s parliamentary journey which often criticsed businesses for not going far and fast enough to protect children online. SLT criticise the approach of Ofcom to ask consultation respondents to list evidence of online risks and harms. SLT would have put greater onus on businesses to show how they have assessed risks and how design and safety features already mitigate those risks (ie. risk assessment should already have been done by businesses). The consultation document and the codes of practice are inaccessible to the majority of people and organisations. Despite good stakeholder engagement from Ofcom, the format makes it too difficult for individuals, services and third sector organisation to participate. Approach to the Codes\\t\\tThe proportionality assessment lets small and single-risk services off the hook, despite some of the most significant harm existing on those services. Some smaller sites are where women and girls experience some of the worst harm, and yet those services will be exempt from applying the mitigations that would help to protect them. SLT think it is wrong to assume that large and multi-risk are the priority services. Size of company may relate to the scale of the problem, but does not diminish the severity of the individual harm. Approach to the Codes\\t\\t\"The proposed measures (1) are too process-driven, (2) place too much responsibility on the victim, (3) do not ask enough of services to identify and deter offenders and (4) place too much precedence on freedom of speech. \"\\tThe measures ought to present a stronger focus on achieving safety for users, rather than the completion of a set of processes. Too many suggested measures are reactive and provide respite after the harm has occurred, rather than preventing the abuse in the first place. They also put the responsibility on women and girls to protect themselves, rather than putting the responsibility on authorities and offenders. Measures to block users or disable comments should include signposting to speciaist services, such as the National Stalking Helpline. There is an inconsistency between the Register of Risk, which presents how anonymity may embolden offenders and increase the risk and harm of stalking, whilst the codes conclude how identity verification would be too much of an infirngement on users\\' privacy and freedom of expression. Freedom of speech trumps safety of users and the prevention of crimes. Automated content moderation (User to User) \\tFraud Keyword detection\\tRevolut’s view is that fraud keyword detection is a solution but not the solution\\tRevolut broadly supports the proposals for service providers to implement fraud key-word detection systems to combat fraud. However, we are concerned that the pro-posals for other solutions - for example AI / machine learning solutions or any hybrid approach - will not be mandated for use. Revolut’s view is that fraud keyword detection is a solution but not the solution. It will not be effective in isolation because it is a sim-plistic approach to fraud prevention that will be ineffective in countering the highly so-phisticated techniques that fraudsters use to scam victims. Automated content moderation (User to User) \\tFraud Keyword detection\\tRevolut believe AI is the way forward\\tThese criminals use their knowledge and expertise to develop highly advanced AI tech-nology to defraud victims and financial institutions in the UK. Revolut and other institu-tions are already using AI to fight against AI. It is the only way to effectively combat the technological sophistication of the tools they are using and the scale in which they are deploying this technology. Fraudsters are nimble and adaptable.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9677048921585083,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They move quickly from different typologies and constantly adapt their modus operandi in an attempt to avoid detection. Automated content moderation (User to User) \\tFraud Keyword detection\\tSolely implementing fraud keyword detection systems - both direct matching and fuzzy matching - will not be enough to effectively catch all fraud occurring online, particularly on social media platforms\\tOur belief is that solely implementing fraud keyword detection systems - both direct matching and fuzzy matching - will not be enough to effectively catch all fraud occurring online, particularly on social media platforms. Bad actors will find ways to circumvent keyword detection tools by continually adapting their terminology. Keyword lists could easily be guessed or even leaked - despite the proposal to refresh them every six months - meaning scammers have the tools to easily circumvent this approach and tar-get victims. Even if there was a continuous feedback loop with models that adapt these lists in real-time (which we would see as the bare minimum), by itself this would still not be enough. Scammers could deploy their own machine learning tools to avoid using these potential words and circumvent any keyword detection solutions. We have noted how fraudsters are adaptable and constantly innovate their approach to evade detection. If keyword de-tection was introduced, we would likely just see a move towards encrypted messaging (e.g. using Whatsapp) and increased hacking and impersonation of legitimate social media accounts, more easily bypassing controls set out that look for new or suspicious profiles targeting other users. Automated content moderation (User to User) \\tFraud Keyword detection\\tWhen looking at all typologies of fraud it is difficult to see how this can be the sole so-lution\\t\"When looking at all typologies of fraud it is difficult to see how this can be the sole so-lution. When considering purchase scams - primarily enabled by online marketplaces - what keywords could be used to differentiate between legitimate items and fake/non-existent items? How would this work across platforms that offer public comments, di-rect messaging and encrypted messaging (e.g. Meta - Facebook/Instagram, Face-book/Instagram Messenger and Whatsapp)? How would this work against scammers who use genuine ads to target others, or who compromise genuine accounts? The ar-guments outlined by Ofcom in 14.312 can broadly be applied to nearly all typologies of fraud in our view. It appears predictable that by just deploying this solution, you are enabling fraudsters to adapt and diversify their approach, rendering this approach ineffective not just in the long-term, but in the short-term too. Therefore Ofcom would be failing to achieve the target of fraud prevention and the platforms would have fulfilled their regulatory re-quirements (and subsequently avoiding any fines) without making any meaningful im-pact in actually reducing fraud. You therefore will not be able to use the financial incen-tive of fines to enable these platforms to take any meaningful action to prevent fraud\"\\nAutomated content moderation (User to User) \\tFraud Keyword detection\\tPropose a solution - AI\\tAs mentioned, Revolut believes that fraud keyword detection is one small part of the solution, it just can’t be used in isolation. We want a layered control set with multiple layers so that if one becomes ineffective you still have other levels. We would recom-mend for the largest categorised services with large user bases, that Ofcom mandates the use of advanced machine learning technology and other AI systems to be used to address the scale of the fraud epidemic on these platforms, alongside the fraud key-word lists becoming real-time feedback loops where they are updated constantly, not bi-annually as suggested in the consultation. Additionally, as we will outline, there are more steps that can be taken to prevent fraud and namely this involves greater coopera-tion between online platforms and financial services firms to data share on fraud and close the knowledge gap between fraud incidences online and fraudulent transactions. There are many voluntary initiatives around this at the moment but they have so far not been successful. If Ofcom could use its new powers to enable mandatory data sharing, it would likely be the appropriate incentive to compel these online platforms to act and would be the best way it can use its powers to improve fraud prevention\\nAutomated content moderation (User to User) \\tFraud Keyword detection\\tPropose further steps for online platforms\\t\"In addition to technologies deployed to tackle adverts, there are a lot more steps that these platforms could implement that would make it more difficult for fraudsters to use these platforms to enable the fraud, and to improve education of fraud for its users. These measures could include: \\n•\\tIntroduction of additional friction into the online journey for advertising products on Facebook Marketplace (similar to KYC for onboarding of FS customers)\\n•\\tMandatory inclusion of relevant Companies House data to enable the creation of a Facebook Business account \\n•\\tProactive blocking of bad actors and content - especially anything linked to spe-cific suspicious activity with new IP, devices, email addresses, accounts \\n•\\tMore proactive customer education practices and/or friction into the processes (e.g. similar to fraud interventions provided by FIs, perhaps on Facebook Mar-ketplace customers should have to answer questions and watch educational content before completing a payment for an item)\\n•\\tSocial media platforms promoting fraud education content free-of-charge or at a reduced fee, instead of charging for advertising - enabling Government Stop, Think, Fraud campaign and UK Finance’s Take 5 to Stop Fraud campaign as ex-amples \"\\nAutomated content moderation (User to User) \\tFraud Keyword detection\\tPropose mandatory data sharing\\t\"Revolut is also supportive of more mandatory data sharing between online platforms and PSPs - who are primarily the main two stakeholders in the fraud value chain. The challenge currently is that both parties are working in silos. When Revolut considers a payment from one of our customers, or indeed an account that is receiving a payment, we do not know anything about the information that this customer could have seen online. We are unaware of the legitimacy of any advert on an online marketplace - who posted it, does the name align with the payee, how long have they been a user, have they had any previous reports around fraud. If a scammer has impersonated a family or a friend, we do not know if there has been any new logins to their device, any new locations or any new profiles, or whether that person has recently bulk messaged contacts. If a customer has seen an investment opportunity, we do not know whether the advert has been verified by the FCA, whether it has been endorsed by a celebrity (or even whether it is legitimately the real celebrity who has endorsed it), how many users have interacted with the scam advert and how many people have been contacted by scammers proven to be exploiting advertising. Essentially, there is a significant challenge in forming a link between the enabler and the payment. Currently this is disconnected and PSPs are relying on users signposting this information to us at the time of the transaction. However, we know that victims who are under the spell of scammers are coached to provide incorrect information and mis-lead us in order to authorise payments. In short, we would support Ofcom proposing a solution whereby as part of the require-ments to prevent fraud, this includes agreements to enable the data outlined above to be shared between online platforms and PSPs. We like the idea of a Confirmation of Payee approach to this, where a customer could provide a link to a profile or advert, then we could do a CoS check with Meta where a risk score or signal is given to indi-cate the legitimacy of the person or the advertisement. This is a dynamic solution that will improve collaboration between the fraud value chain and improve detection and prevention of scammers using online platforms to defraud customers. \"\\nAutomated content moderation (User to User) \\tFraud Keyword detection\\tRevolut shares Ofcom’s view about the devastating impact of investment scams, and the data we highlighted above corroborates this view\\t\"Revolut shares Ofcom’s view about the devastating impact of investment scams, and the data we highlighted above corroborates this view. 59.5% of all Revolut UK customer fraud losses in 2023 came from investment scams, and specifically when looking at Meta platforms, this figure rises to 61.3%. If Ofcom wants to use its powers to signifi-cantly reduce fraud losses, then tackling investment scams should be seen as a priori-ty. Revolut agrees with Ofcom that fraud keyword detection is not the sole solution to tackling investment scams, but as mentioned we believe that Ofcom must apply this logic to all typologies, not just investment scams. We have mentioned above the many ways in which online platforms - particularly social media firms - can support fraud pre-vention and investment scams will fall into this. \"\\nAutomated content moderation (User to User) \\tInvestment scams via paid for advertising\\tanecdotally from victim testimonies it is clear that paid-for ads are used to promote scams and to reach victims at scale\\tRevolut also recognises that Ofcom is launching a separate consultation that will focus on paid-for advertising and its use of promoting investment scams. Revolut currently does not have data which can differentiate between the amount of investment scams that came from paid-for adverts vs social media posts from users, but anecdotally from victim testimonies it is clear that paid-for ads are used to promote scams and to reach victims at scale. Perhaps most concerningly when considering paid-for ads, these are scams that the platforms themselves are profiting from because the scammers are still paying the platforms to advertise on their platform. It is inconceivable that platforms can not only enable fraud, but actually profit from it, and Revolut would urge that Ofcom addresses this as a matter of priority. Our data suggests that tackling investment scams is perhaps the best way in which Ofcom can successfully improve fraud preven-tion in the UK. Revolut is keen to work with Ofcom on this topic and in addition to re-sponding to the separate consultation, is happy to provide any further materials which could be useful. Approach to the Codes\\tOverall approach to our regulatory regime\\tIt is a concern for Revolut that fraud prevention does not have a unique approach to any other type of online harm, especially given the scale of the problem and the increasingly sophisticated and diverse ways in which criminals are defrauding innocent victims. it is clear that fraud prevention was not the original focus of this legislation - it was only added in 2022 to the scope of the Bill. Whilst we broadly support the powers being given to Ofcom to prevent fraud and the potential for fines of up to 10% of global revenue be-ing issued to online platforms who fail to prevent fraud, this is a very blunt instrument being used to address a variety of online harms. It is a concern for Revolut that fraud prevention does not have a unique approach to any other type of online harm, especially given the scale of the problem and the increasingly sophisticated and diverse ways in which criminals are defrauding innocent victims. We need a tailored strategy to tackle the fraud epidemic in the UK, not a copy and paste approach for all online harms\\nApproach to the Codes\\tOverall approach to our regulatory regime\\tProvides data on their UK customers who were scammed\\t\"[Gives examples of the shift of UK government focus on fraud]. Revolut’s data reinforces these positions. In 2023, 86.5% of all authorised fraud on Revolut’s UK customers came from online platforms, with Meta platforms (Facebook, Instagram, Whatsapp) being responsible for 60.5% of all fraud volumes. 59.5% of the money lost on Meta came via investment scams, which are often enabled via adverts on Facebook and Instagram. This means not only that Meta is failing to prevent this fraud, it is likely profiting from the fraud it enables - as these scammers will use paid-for adverts through Meta platforms to reach victims. Media interventions by Barclays, TSB and Lloyd’s show that this is not just a Revolut issue, it’s an industry issue. There has been a clear shift in perceptions towards a view that these firms are not do-ing enough to prevent fraud and financial incentives are the only way in which they will actually take their fraud problem seriously.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9919260144233704,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nApproach to the Codes\\tOverall approach to our regulatory regime\\tConcerned about the lack of joined up regulatory solution\\t\"However, we are not seeing a joined up regulatory solution that reflects this shift in perception. The Payments Systems Regulator (PSR) is introducing a mandatory reim-bursement regime where UK PSPs will be required to reimburse 100% of all authorised fraud losses up to £415,000, starting in October 2024. This regime suggests that finan-cial institutions are solely responsible for preventing fraud and will mean they are on the hook for reimbursing victims for scams that originate on social media platforms and SMS. We do not see any clear collaboration between Ofcom and the PSR towards their shared objectives of reducing fraud rates in the UK. The PSR’s October 2024 implemen-tation date is not aligned with the powers of the Online Safety Act coming into force, the PSR’s proposal to collect fraud origination data is not connected to Ofcom’s Codes of Conduct and there is no connection between Ofcom’s fines with fraud prevention tar-gets for online platforms. Even if Ofcom did issue significant fines to online platforms, these fines would ultimately not be used to reimburse victims, meaning that PSPs re-main solely responsible for the financial burden of fraud reimbursement. Revolut is concerned that both regulators are working in silos, with the PSR focusing entirely on ensuring customers are reimbursed and Ofcom focusing on ensuring online platforms implement fraud prevention systems. Neither solution is a silver bullet for fraud prevention, especially when they are operating in isolation. [Revolut continues to give examples of the risk of failing to achieve fraud prevention rargets]\"\\nApproach to the Codes\\tOverall approach to our regulatory regime\\tConcerned that codes will lead to marginal reducation in fraud\\tLooking at Revolut and UK Finance data, it’s likely that a handful of Big Tech firms are enabling the vast majority of all authorised fraud in the UK, equating to hundreds of mil-lions of pounds of fraud each year. There is a risk that in response to the Online Safety Act, these firms introduce new systems that comply with Ofcom’s targets, lead to mar-ginal reductions in fraud rates, and consequently are not issued any fines. However, in this scenario, these firms would still be enabling hundreds of millions of pounds of fraud per year. PSPs will still be picking up the bill for these firms’ failure to prevent fraud. But most importantly, hundreds of thousands of victims will still experience the devastating emotional impact of fraud. Whilst fraud rates will have technically reduced on paper, this is still not solving the clear imbalance of financial incentives between fraud enablers and PSPs, nor is it achieving the objectives of significant improvements in fraud prevention in the UK. Even if Big Tech firms do face significant fines, these fines are not linked to reimbursement. Approach to the Codes\\tOverall approach to our regulatory regime\\tBelieve Ofcom needs to be given more powers. \"We believe there needs to be an evolution in the powers that Ofcom has been given through the Online Safety Act, where Ofcom has the ability to issue fines to online plat-forms that are a percentage (e.g. 50%) of the total value of fraud they enable and then these fines are used either to directly reimburse customers or to reimburse the PSPs who reimburse customers through the PSR’s regulatory regime. We believe this reflects a fair representation of the fraud value chain, and would mean that the liability for reim-bursement is shared equally between the enablers of fraud and the PSPs who enable fraudulent transactions. If online platforms faced yearly multimillion pound fines for failure to prevent fraud, we are confident they would finally take the issue seriously. Revolut recognises that this is not the vision that the Government or Ofcom has laid out for how financial incentives will work to prevent fraud. But if we are to achieve signifi-cant reductions in fraud, then a shared liability regime, with aligned incentives between the two main stakeholders in the fraud value chain, is the only effective way to achieve this. This would ensure that the enablers of fraud do have financial skin in the game, and therefore are effectively incentivised to prevent all fraud - the very argument used by the PSR in support of its mandatory reimbursement regime. \"\\nApproach to the Codes\\tOverall approach to our regulatory regime\\tSummary of their thoughts around how Ofcom should tackle fraud online\\tThe UK financial services sector clearly has considerable financial incentives to reduce fraud rates in the UK, meaning we are very supportive of Ofcom’s objectives. We want to work closely with Ofcom in the delivery of the Online Safety Act, precisely because put-ting financial incentives on the enablers of fraud will have a far greater impact on reduc-ing fraud rates than putting the sole burden on PSPs, when the fraud has already been committed and victims are already under the spell of scammers. Shared liability is the only way in which we will achieve significant fraud reduction tar-gets and prevent millions of innocent victims from facing the financial and emotional impact of this devastating crime. Revolut’s data clearly highlights where action needs to be targeted and we therefore urge Ofcom to use their powers to ensure that the online platforms who enable the vast majority of fraud are finally financially incentivised to actually prevent fraud. Banks and Financial Institutions should be the last line of de-fence against fraud, not the only line of defence. User reporting and complaints (U2U and search) \\t\\tNeed to balance ease and accessibility with provision of comprehensive complaints systems\\tThe Crime Survey of England and Wales estimates that only 13% of fraud instances are reported. While the provision of easy to find, access, and use complaints systems is important to encourage greater reporting of acts of fraud, the diversity of possible harms means that a more simplistic reporting system would not be sufficient. As such, the need for accessibility and useability of reporting systems must be balanced with a need to ensure they are comprehensive. User reporting and complaints (U2U and search) \\t\\tComplaint systems must be inclusive, considerate, and tailored. Complaint systems must not be designed for an average users but instead to be accessible by all. Given the diverse nature of online harms, a \\'one-size-fits all\\' approach is unlikely to be most appropriate, and this means user reporting systems must be tailored, including in the responses sent to complainants, whereby automatic Reponses should only be used in some instances, and more careful handling preferenced in others. This is particularly the case whereby complainants may be in distress or may be vulnerable, particularly if they have been a recent victim of fraud. User reporting and complaints (U2U and search) \\tCommunicating with victims of fraud\\tCommunications with those reporting fraud should be tailored\\tCommunications with those reporting potential fraud should be supportive and victim-centric\\nUser reporting and complaints (U2U and search) \\tDedicated reporting channel for fraud\\tIncentives for trusted flaggers in DRCs. While DRCs for fraud are welcome, sufficient incentive for trusted flaggers must be provided. This includes these flaggers having faith that reporting potential fraud leads to tangible actions and therefore the overriding risk of regulatory action is real. User reporting and complaints (U2U and search) \\tDedicated reporting channel for fraud\\tWider lists of trusted flaggers should be used for DRCs. Given the fast moving nature of fraud and fraudsters, it is imperative to identify and address fraud as soon as possible. As such trusted flagger lists should be large enough to engage such content. Relevant, reputable, independent organisations are proposed as trusted flaggers, such as: UK Finance, Trading Standards, Which?, Money Saving Expert, Age UK and Victim Support. If Ofcom cannot/won\\'t broaden the list of flaggers, then regulated services should be encouraged to establish voluntary DRCs with such organisations. User reporting and complaints (U2U and search) \\tImplementation, monitoring, and KPIs\\tThere must be relevant monitoring of the impact of the proposal(s)\\tIt would be beneficial to identify how Ofcom defines success of the proposals\\' implementation and impact, ideally through relevant KPIs. Monitored impacts will need to be at individual channel and holistic levels.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9865022897720337,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"A two-yearly review process would be insufficient for large regulated entities. Governance and accountability  \\t\\tOnline safety should internally reside at a senior or leadership level, and should be holistically well-integrated across functions within tech companies\\tIn large tech companies, T&S functions are often deprioritised compared to other key functions, and are often brought in late in the process of designing or developing new products or features - if considered at all. In smaller companies, T&S is often treated as a late addition rather than being embedded from the start. Instead of both approaches, T&S should be situated at the senior or leadership level internally and be holistically well-integrated across different functions and levels to ensure online safety is an intrinsic consideration throughout development and operational processes. Governance and accountability  \\t\\tSmall services should remain in scope\\tSmall and high-risk platforms are the site of much harmful and illegal content, and so - even in the case of unprofitable platforms - online safety should not be deprioritised relative to commercial interests or growth. This can assure online safety measures are implement across the digital landscape. Governance and accountability  \\t\\tIndependent, third-party audits should be mandated from the start. Given the perpetual reluctance of tech companies to effectively self-regulate, a sectoral lack of transparency and avoidance of scrutiny, and the contemporary phenomena of such companies restricting access to data for independent researcher, it is suggested that independent third-party audits should be mandated from the outset, particularly for the highest risk services. Approach to the Codes\\t\\tReferred to Online Safety Act Network Statement\\tThe response, in answering the question regarding overall satisfaction with the approach to develop the illegal content Codes of Practice, refers to the Ofcom-critical Online Safety Act Network statement. It is argued that the codes are too prescriptive and rules-based, offering a 'safe harbour' that may limit companies to minimum compliance rather than encouraging more proactive safety measures. The codes should not just add measures where there is evidence, particularly in consideration of the risk-based outcomes intended by the Act. It is recommended that Ofcom implement a systemic regulatory approach that is rooted in 'safety-by-design' principles that emphasises the need for all services - regardless of size - to effectively address identifies risks. Such a regime would encourage services make use of more proactive tools such as product testing, mitigating measure implementation during design and development, and the application of monitoring and measurement metrics to measure the effectiveness of mitigating measures in reducing harm. Approach to the Codes\\t\\tTransparency requirements (about user numbers) for in-scope services \\tThere should be imposed transparency requirements on services to identify: how they calculate their user numbers, the regularity of required reviews, and how Ofcom plans to independently verify them. This is to prevent services from claiming exemptions for aspects of their service or attempting to sub-divide their user numbers to evade meeting higher thresholds. Approach to the Codes\\t\\tUser number metrics are too blunt to identify large services \\tThe use of 10% of population size does not adequately reflect risk level, with a number of smaller platforms disproportionately likely to be a source of immense risk in regard to hate and terror content. ISD recommends either a lower threshold for 'large' services (e.g. 1-2m+ users), or introducing a 'medium' category. Approach to the Codes\\t\\tIncentive for platforms to downplay risks\\tIn this categorisation system there is a danger that services will be incentivised to downplay individual risks to evade triggering the additional requirements imposed on multi-risk services\\nApproach to the Codes\\t\\tDoesn’t accurately reflect severe, single risk platforms\\tSome platforms may only pose a specific single risk, but to an exceptionally severe extent, and so not be classed as multi-risk and so may avoid more stringent requirements. As such, obligations should be determined through a comprehensive risk assessment processes to enable a more nuanced evaluation of the nature and severity of platform risks, and so ensure proportionality to  the actual risks presented by each service. Approach to the Codes\\t\\tReferred to Online Safety Act Network Statement\\tThe response, in answering the question regarding comments on the draft Codes of Practice themselves, refers to the Ofcom-critical Online Safety Act Network statement. For a more detailed breakdown of this response from the OSAN, see no.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9971831440925598,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"10. Content moderation (User to User) \\t\\tContent moderation should not be over-relied upon\\tContent moderation, as a primarily reactive measure, should not be over-relied upon to effectively mitigate risks given the variety, extent, and prevalence of online harms. Content moderation (Search)\\t\\tContent moderation should not be over-relied upon\\tContent moderation, as a primarily reactive measure, should not be over-relied upon to effectively mitigate risks given the variety, extent, and prevalence of online harms. Content moderation (User to User) \\t\\tOfcom should introduce baseline expectations and consistent measures to assess CM impact\\tContent moderation efforts are often characterised by a lack of genuine transparency, both at individual and macro (e.g. for independent external assessment of proportionality, consistency, and effectiveness) levels. The typical use of self-selected metrics and measures of success without objective assessment suggests Ofcom should introduce baseline expectations and consistent measures to assess impacts of content moderation in mitigating risks and reducing harms, and allow for cross-industry comparison. Content moderation (Search)\\t\\tOfcom should introduce baseline expectations and consistent measures to assess CM impact\\tContent moderation efforts are often characterised by a lack of genuine transparency, both at individual and macro (e.g. for independent external assessment of proportionality, consistency, and effectiveness) levels. The typical use of self-selected metrics and measures of success without objective assessment suggests Ofcom should introduce baseline expectations and consistent measures to assess impacts of content moderation in mitigating risks and reducing harms, and allow for cross-industry comparison. Automated content moderation (User to User) \\t\\tStrongly agree with statement A9.19\\tRecommend that factors such as size, purpose, accessibility, and the nature of relationships between users of a channel/community be taken into account when making assessments about public or private spaces online. Automated content moderation (User to User) \\t\\tGuidance does not include number threshold to distinguish private and public spaces\\tThe current guidance draft does not specify a user number threshold to determine whether an online space is public or private (leaving it up to services). This could lead to cross-platform inconsistencies, user confusion, and potential loopholes for services to avoid requirements. It may also not be possible o accurately determine location of users able to access content, and so it may be difficult for Ofcom or others to understand availability of content to UK users\\nAutomated content moderation (User to User) \\t\\tRequire companies set clear and reasonable thresholds to identify public and private spaces \\tIf Ofcom is unwilling to apply a blanket threshold to distinguishes between public and private spaces online, they should require companies set clear and reasonable limits based on the nature of their platforms and risk assessments that consider platforms' specific features/functionalities (e.g. E2EE), risks, and potential vulnerabilities.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9912694692611694,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"They should also encourage companies make clear to users which aspects of the platform as more public and private, as well of the consequences of this for user privacy and T&Cs enforcement. Using risk assessments, Ofcom should then assess whether the limit set by the platform is appropriate and sufficiently mitigates any risks or harms identified. User reporting and complaints (U2U and search) \\t\\tUser content reporting alone is not sufficiently effective \\tContent reporting by users alone is not an effective means to ensure user safety, and responsibility for identifying illegal content should not be shifted from services to users. Instead it should complement more proactive safety-by-design measures. User reporting and complaints (U2U and search) \\t\\tUser must be allowed to provide contextual information\\tUsers should be allowed to provide contextual information along with any reports and complaints. This is particularly important in cases of persistent abuse or harassment where services often fail to grasp broader context of targeted users' experience. User reporting and complaints (U2U and search) \\t\\tServices must provide sufficient information explaining why a decision has been made\\tGiven a typical lack in transparency regarding decision-making processes in content moderation, Ofcom should require services provide sufficient information explaining why a particular decision has been made (i.e. user content has been moderated, or the service has decided to not take action on a user report). Additionally Ofcom should require services allow for appeals for users when their reports are not actioned. Recommender system testing (U2U) \\t\\tSupport proactive measures to assess potential risks associated with recommender systems \\t Support proactive measures to assess potential risks associated with recommender systems as part of an overall safety-by-design approach, and recommend ensuring such assessments are carried out prior to their use. This should be repeated in the case of any changes made, and repeated regularly to fully consider and effectively mitigate risks at an early stage. Recommender system testing (U2U) \\t\\tServices must consider whether the presence of untested recommender systems is appropriate\\tServices must consider how appropriate it is to have untested recommender systems, or whether they can be safely deployed without the ability to test the impact of any future changes, either to optimise for business or safety related objectives. Enhanced user control (U2U) \\t\\tCannot solely rely on these tools to mitigate risk \\tThe responsibility for risk mitigation should not be shifted onto users. Instead, such controls should be used to complement a broader approach to online safety. Enhanced user control (U2U) \\t\\tExtra tools for victims of online abuse or harassment. Services could consider providing bulk reporting, blocking, and muting tools for victims of online harassment or abuse, especially for high-profile, vulnerable, or marginalised users. Enhanced user control (U2U) \\t\\tLabelling accounts can be a useful tool, but requires transparency\\tLabelling accounts can provide users with additional cues to assess the veracity of information or the trustworthiness of an account. However, services must ensure transparency in explaining how verification works and apply it consistently across platforms. Enhanced user control (U2U) \\t\\tMisuse or inconsistent application of account labels can pose risks. Overly broad labelling categories may be unhelpful, obscuring variations in the trustworthiness of different sources.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9909847974777222,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'If all groups are labelled in the same way (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8537521958351135,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'government sources, journalistic sources etc.), it may overlook factors such as variation in editorial standards, or the role that certain (often undemocratic) government entities/accounts may have a demonstrated track record of disseminating disinformation. User access to services (U2U) \\t\\tAccount strike policies are opaque to external actors\\tIt is not possible for civil society researchers to understand or assess the ways in which account strike policies are implemented or enforced. The consistent presence of hate and abuse suggests that, while there is a place for such measures, they do not seem to significantly disincentivise certain users from perpetuating abuse or harassment. User access to services (U2U) \\t\\tEase of new account creation nullifies account bans\\tGiven how easily new accounts are created when previous ones have been banned or suspended (a phenomena that is often communicated with pride), such methods should not be entirely relied upon to deal with persistent networks of harmful accounts. More proactive methods, such as specialist harm-specific teams assessing an accounts\\' wider network for similar illegal content, would be more appropriate \\nUser access to services (U2U) \\t\\tAccount strikes inappropriate response for sharing highly illegal content\\tAccount strikes are deemed to be an inappropriate response for the sharing of highly illegal content such as terrorism content of CSAM. User access to services (U2U) \\t\\tUse of expedited appeals processes\\tTo mitigate risks to freedom of expression associated with erroneous content removal, wrongly assessed to be illegal content (often as a result of automated content moderation systems), services could consider an expedited appeals process\\nCumulative Assessment  \\t\\tReferred to Online Safety Act Network Statement\\t\"The response, in answering the question regarding proportionality of overall burden of measures on low risk small and micro business, refers to the Ofcom-critical Online Safety Act Network statement, as well as their answers to questions 7 and 8. Concerns are highlighted that Ofcom\\'s approach to proportionality primarily considers costs rather than risks.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9898539185523987,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'As such, the appropriateness of Ofcom\\'s recommendations is questioned, as it is argue that Ofcom\\'s proposed regime does not align with the objectives of the Act; particularly the central tenet of safety-by-design. As such, it is proposed that Ofcom implement a more systemic regulatory approach rooted in risk assessment and safety by design principles. Additionally, in the responses to Q7 and Q8, ISD recognise a need to shift focus from cost to services to one of risk to consumers. This would include a shift from reactive to proactive mitigation techniques, a prevention of overreliance on Ofcom\\'s risk profiles, an operationalisation of forward-looking and future-proofed methods, and a consideration of how  highlighted harms can intersect with each other and lead to unintended outcomes. \"\\nCumulative Assessment  \\t\\tReferred to Online Safety Act Network Statement\\t\"The response, in answering the question regarding proportionality of overall burden on small and micro business that find they have significant risks of illegal content and therefore face more measures, refers to the Ofcom-critical Online Safety Act Network statement, as well as their answers to questions 7 and 8. For a more detailed breakdown of this response from the OSAN, see no. 36\"\\nCumulative Assessment  \\t\\tReferred to Online Safety Act Network Statement\\t\"The response, in answering the question regarding proportionality of overall burden on large services facing more measures, refers to the Ofcom-critical Online Safety Act Network statement, as well as their answers to questions 7 and 8. For a more detailed breakdown of this response from the OSAN, see no. 36\"\\nStatutory Tests \\t\\tReferred to Online Safety Act Network Statement\\t\"The response, in answering the question regarding appropriateness of Ofcom\\'s proposed recommendations for the Codes in light of matters to which Ofcom must have regard, refers to the Ofcom-critical Online Safety Act Network statement, as well as their answers to questions 7 and 8. For a more detailed breakdown of this response from the OSAN, see no. 36\"\\nTerms of service and Publicly Available Statements\\t\\teBay provided information about their User Agreement and Policies such as information about Prohibited and Restricted Policies \\teBay explains that by engaging with eBay’s services, users are subject to eBay’s User Agreement and User Privacy Notice. The User Agreement explicitly requires that users refrain from violating any laws, third party rights and/or eBay policies. Users are also bound by policies referenced by and incorporated into the User Agreement. By integrating laws and third parties’ rights in the User Agreement in this way, eBay has discretion to impose contractual sanctions (e.g., block or cancel items, restrict or suspend accounts, etc.). eBay also provides detail on policies they have developed to address Prohibited and Restricted Items. These policies are designed to protect consumers and provide clarity to sellers about what can and cannot be sold on our platform (and under what conditions). Content moderation (User to User) \\t\\teBay experts work around the clock to quickly remove potentially problematic items\\tHighly-trained eBay experts across the globe work around the clock to quickly remove potentially problematic items. eBay report the removal of 2.8 million potentially counterfeit and prohibited items in 2022. User reporting and complaints (U2U and search) \\t\\teBay have the option for users to report under their \\'Report Item\\' function with 773,000 items removed in 2022\\tIn 2022, eBay removed 773,000 items based on reports from our community through our Report Item functionality. As eBay continue to invest in proactive detection technology, the number of reports received through the “Report Item” functionality declined in 2022 by 24% compared to 2021 and by 54% compared to 2020. Automated content moderation (User to User) \\t\\teBay acquired 3PM Shield, a provider of advanced AI-based marketplace compliance solutions in 2023 \\teBay acquired 3PM Shield, a provider of advanced AI-based marketplace compliance solutions in 2023. This enhances eBay’s ability to address suspicious or harmful seller behavior, and potentially problematic items. Approach to the Codes\\t\\teBay are concerned by Ofcom\\'s  compliance timeline \\teBay emphasize that  that Ofcom recognize the very real constraints associated with implementing major product changes on global platforms, especially complex transactional marketplaces like eBay. eBay note that Ofcom’s current timeline, platforms are required to submit risk assessments in Q4 2024 at the same time as Ofcom is finalizing the Illegal Harms Code and submitting it to the Secretary of State for approval. This is a concern because Q4 the busiest period of the year for e-commerce platforms. Approach to the Codes\\t\\tRequest for more time to comply with OSA duties\\teBay argue that sort of grace period would be required in terms of Ofcom’s enforcement of the new duties. Given the constraints set out above, and depending on the extent of any changes required, this might need to take up to 12 months \\nAutomated content moderation (User to User) \\tPublic vs private communication\\tContent shared privately can become public\\t\"One survivor said that contents that are supposedly private or not easily shareable by the recipient can also be communicated publicly. For example, a user records content on his/her laptop, that’s beyond private sharing and considered as public already\" (This is the full response on this question)\\nACM Automated content moderation (Search) \\t\\tSupportive of measure\\tSupportive of measure. \"There are many websites now that are used for CSAM distribution. CSAM URL detection is needed\"\\nACM Automated content moderation (Search) \\tUser verification\\tShould be mandatory user verification\\t\"Survivors agree that there should be a verified identification ... This should also verify whether the account user is an adult or a minor. ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9888355135917664,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This would also ensure accountability. ...': [{'label': 'POSITIVE',\n",
       "               'score': 0.9755144715309143,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'There is still a risk because even identification can be falsified. But it should be a minimum requirement when a person is signing up for an account on tech platforms.\"\\nUser reporting and complaints (U2U and search) \\t\\tLanguage help needed for non-English speakers to make complaints and easily clickable button for complaints\\t\"“For UK users, affected persons, and interested persons”: this includes all people even beyond the UK as long as they are affected or interested as well.\" There needs to be help for those who are not English speakers (eg AI translator or actual person translator). This is needed to make it \"easy to access\" for children.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927423000335693,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"Easy to access shall be a button that is clickable and when you open it, a page will show up where you can directly give your comments/complaints. There has to be an option too to attach photo or video as supporting evidence or reference.\" Illegal content should be taken down\\nTerms of service and Publicly Available Statements\\t\\tPhilipino survivors agree with Ofcom’s terms of service proposals\\t\"One survivor mentioned that she would actually just check the box whenever she sees terms and services since there is a lot of content written on it. All survivors agree with Ofcom’s terms of service proposals.\" (This is the full response)\\nDefault settings and user support (U2U)\\t\\tPhilipino survivors supportive of grooming measures\\t\"Looks like all the important measures for users under 18 years of age are included ... Can we detect and verify if the user of a platform is 18 years old and above? There are children under 18 years old who do not put their real age when they sign up for a social media account. The proposals mentioned a switched off “automated location information display”. Is there a way for companies to still detect children online through their social media accounts in case they are in danger or in need of rescue? \"\\nUser access to services (U2U) \\t\\tPhilipino survivors supportive of user banning for CSAM, after due process\\tOne survivor: \"... sharers of CSAM should be blocked ... \" Another survivor: \"... to prevent a user from returning to a service, they need to follow due process. For instance, if one’s content is detected as CSAM, it should be temporarily banned or closed. After that, an investigation should take place to identify if that content was made for CSAM purposes or was taken unintentionally. If the content was proven to be made for CSAM purposes, then they can permanently delete the content and ban the user, as well as all the accounts (email, username, etc.) linked to that profile. At the same time, we may not know that they’re doing the same content on every online platform they use. On the other hand, if the content was unintentional, they can just delete it and warn the user.\"\\nUser access to services (U2U) \\t\\tPhilipino survivors support permanent banning if CSAM is intentional\\t\"They should be blocked permanently because there is always a possibility that they will do this again. There are posts on Facebook that are unintended (e.g., a child was captured without wearing clothes). But there are also intended contents that are CSAM ... If it was proven that an account was producing and distributing CSAM, they should be blocked or deleted permanently ... That user would probably just create a new account so all in all platforms, all users must be required to submit identification cards so they can be easily tracked and reported. There should be verified registration, so people won’t be able to create dummy accounts for CSAM purposes. ...\"\\nCumulative assessment\\t\\tAll services should comply and overall burden should be proportionate\\t\"Any service should be compliant whether it is big or small. ... All services must adhere to their terms of service such as detecting and reporting harmful content. ... Google Meet – the survivor perceives this platform as a small low risk service.\" One survivor: \" ... I do agree that the overall burden of both should be proportionate because we can’t really predict to what apps/websites they’re using. We don’t know that some users might use this small low risk service because they perceive that people would think that they only uses this for school purposes.\"\\nCumulative Assessment\\t\\tPhilipino survivors highlight the following platforms as needing to do more: Bigo live, Kumu, Snapchat\\t\"Platforms recommended by survivors that need to have more measures to ensure safety by design:\" Bigo live [with more detail included on this service and how used], Kumu, Snapchat\\nn/a\\tReporting to law enforecment\\tGood quality mandatory reports on CSAM would help local law enforecement\\t\"If the mandatory reporting by tech platforms of suspected CSAM included the IP address and PORT information, these reports would likely be more actionable. The US RE�PORT Act that is currently under review with the Senate is an excellent example of potential legislation that would strengthen law enforcement reports and improve their ca�pacity to investigate suspected child sexual abuse and ulti�mately hold perpetrators accountable.\"\\nAutomated content moderation (User to User) \\tNew CSAM detection\\tNeed for measures beyond identifying only known CSAM\\t\"There is an urgent need for platforms and service providers to adopt advanced technological solutions that proactively identify and mitigate the dissemination of harmful content ... The 2023 WeProtect Global Alliance Global Threat Assessment outlines several technological alternatives for deployment that can prevent, disrupt, and detect CSAM: Client-side scanning ..., Homomorphic encryption, ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922135472297668,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Intermediate secure enclaves ...': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8699667453765869,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\" SafeToWatch mentioned as one form of client-side scanning. Automated content moderation (User to User) \\t\\tRisk if only focus on known CSAM of incentivising more new CSAM\\t\"the overemphasis of detecting known CSAM and deprioritizing the detection of new CSAM including livestreaming has the potential to incentivize the new production of CSAM …\"\\nAutomated content moderation (User to User) \\t\\tSuggestion that signal sharing by platforms would reduce CSEA\\t\"Lack of signal sharing by tech platforms of livestreamed abuse is a key gap to identify and block child sexual abuse offenders. “The Tech Coalition is launching Lantern, the first cross-platform signal sharing program for companies to strengthen how they enforce their child safety policies.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929046034812927,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nGovernance and accountability  \\t\\tProposals good start, but further measures needed\\tTransparency requirements needed, with global collaboration and further developing overseas regulatory network. Also need tracking of implementation of measures. Specific suggestions included for transparency report\\nApproach to the Codes\\t\\tAll services should implement all measures\\tImperative that even smaller or seemnigly less risky companies adjere to every measure. Companies should prioritise being platforms safe by design. Eg recent instances of sextortion offenders meeting children on larger platforms but moving to smaller ones to continue the exploitation, including in children\\'s school acounts where they are able to reach other children. Approach to the Codes\\t\\t7 million users too high\\tAustalian codes impose their most stringent sections on providers with a minimum threshold of 2.5% - we should align with this. Complying with safety technology should be a basic requirement for online services.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9971593618392944,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Certain technology are free of charge or low cost. Approach to the Codes\\t\\tAll services should implement all measures\\tChapter 6 shows offenders can exploit less developed or smaller online services, so every company should adhere to every measure. \"Even service that would assumably be less risky should have deploy safety technology to prevent child exploitation\"\\nAutomated content moderation (User to User) \\t\\tServices should be required to invest in new technologies for CSAM\\t4G of Codes say platforms should only implement perceptual hash matching technology if feasible. However in Australia, eSafety codes that include a requirement for providers to invest resources into make a technology feasible. Ofcom should adopt a similar approach. Similarly for CSAM URLs. eSafety also sets a timeline for when the technology is to be developed. User reporting and complaints (U2U and search) \\t\\tThe list of trusted flaggers should be extended\\tTrusted Flaggers should include more police forces. Eg, Police Scotland is heavily involved in identifying and apprehending offenders, with examples given of this involvement. Trusted Flaggers could also be expanded, as with the DSA, to include civil society, NGOs and SMEs with appropriate training. Approach to the Codes\\t\\tMany technologies are available free of charge, eg Microsoft\\'s PhotoDNA\\tMany technologies are available free of charge, eg Microsoft\\'s PhotoDNA, others are low cost. Some tools would reduce content moderation by preventing harms arising. Ofcom must also consider the costs associated with parents going to jail. Cost also to society from underage CSAM offenders some of whom are finding this material by accident and become addicted to it. Content moderation (User to User) \\t\\tSuggestions included for factors for prioritisation of review\\tUnder prioritisation, \\'privately\\' sent content should be included as a top priority for internal review. Evidence described of private channels being used for CSEA. Another red flag is \"profiles posting or sharing illicit content or using illicit keywords when the accounts are from the UK and Philippines respectively\". See also a separate report included with response which contains key indicators of livestreamed CSEA. Automated content moderation (User to User) \\t\\tOfcom should go further, including deploying image-classifers, machine learning and AI\\tImage-classifers, machine learning and AI are not only recommended by the WeProtect Global Alliance, but are also currently in use on many major platforms. It is possible to assess and remove material even in an encrypted environment, including Apple\\'s NeuralHash and secure enclaves within company services can decrypt a message. WhatsApp example mentioned.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934626817703247,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"See also \\nAutomated content moderation (User to User) \\t\\tPublic vs private communciation guidance could be clearer\\tGuidance is unclear whether it relates to 'privately' or 'publicly' communicated content. In general, nearly all content is easily shareable becaues someone can take a screenshot or record the scree. Further clarity needed. Automated content moderation (User to User) \\t\\tCSAM hash matching technology is free or cheap\\tPhotoDNA is free, and has an extremely low error rate (reference given). Also other free ones.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9938066005706787,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Other tools are mentioned, which are cheap. Governance and accountability  \\t\\tAgree that having a named person responsible for managing risks and reporting to their board about illegal harms is a good idea\\tAgree that having a named person responsible for managing risks and reporting to their board about illegal harms is a good idea. Board reporting should also include analysus of fraud occuring on platform, as this will improve their understanding of their role. Cites example of UK banks publishing reports on fraud originating online. Content moderation (User to User) \\t\\tRespondent does not believe that costs to platforms to find and remove illegal content relating to fraud should be a barrier to most firms\\tRespondent does not believe that costs to platforms to find and remove illegal content relating to fraud should be a barrier to most firms. Cites the the costs of compute power lowering over time, and new tools such as AI to find even complex content, platforms should be able to build in robust content checking processes. \" Default settings and user support (U2U)\"\\t\\tRespondent mischaracterises the contents of Measure 7A as age verfication, agrees with the proposal but suggests a broader range of platforms should be in scope. Respondent mischaracterises the contents of Measure 7A as age verfication, agrees with the proposal but suggests a broader range of platforms should be in scope. Suggests all platforms in scope of the OSA even those not aim at children should assume all unverifie users are children and prodvide child safe content by default (including lack of access to harmful but legal content) unless the platform verifies the user as an adult. Enhanced user control (U2U) \\t\\tRespondent mischaracterises the contents of Measure 9C as identiy verfication, agrees with the measures, and suggests that verification should be free, manadatory, and used to unlock certain features \\tRespondent mischaracterises the contents of Measure 9C as identiy verfication, agrees with the measures, and suggests that verification should be free, manadatory, and used to unlock certain features. Suggests that Verfied users should be clearly visable, that users should be able to filter out unverified users from content feed/contact suggestions.': [{'label': 'POSITIVE',\n",
       "               'score': 0.6300197243690491,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Certain features should be disabled to unverified users including; trade or offering goods/services to prevent fraud. Verfication would not need to be real name viaible, pseudonymous verified could increase privacy with personal data being needed to be shared with platforms (respondent notes the platform requirements of full passport scans cause data protection and privacy risks as well as cyber secuirty risks). Platforms that allow trading should offer support messaging to users to encourage safety behaviours such as only messaging on platform and not alternative channels where contact details can be spoofed, with offers of protection for on-platform trades used as an incentive. Enhanced user control (U2U) \\t\\tUsers should have full visibility of how to get verified, and of who else on the platform is verified. Users should have full visibility of how to get verified, and of who else on the platform is verified. Transparency will maximise the effectiveness of the controls. If users cannot see who else is verified, they will continue to fall for scams from unverified/impersonation accounts. Enhanced user control (U2U) \\t\\tVerfied account provide particular value to accounts of high-profile persons who get abuse from anonymous accounts. Verfied account provide particular value to accounts of high-profile persons who get abuse from anonymous accounts. These accounts could be blocked from a feed or commenting/interesting with content meaning the person would not see the abuse. Cumulative Assessment  \\t\\tRespondent agrees and supports the OSA; broader use of age verification and identity verification will lead to better online safety for all users, and particularly children. Respondent agrees and supports the OSA; broader use of age verification and identity verification will lead to better online safety for all users, and particularly children. For small companies providing U2U services online, being safe should not be prohibitive to their business models\\nCumulative Assessment  \\t\\tRespondent considers the measures to be proportionate\\tRespondent considers the measures to be proportionate. Cumulative Assessment  \\t\\tRespondent considers the burden on large services proporationate large platforms have the resources to implement safer services and should lead by example\\tRespondent considers the burden on large services proporationate large platforms have the resources to implement safer services and should lead by example\\nUser reporting and complaints (U2U and search) \\t\\t\"Supportive of Ofcom’s assessment of how to mitigate the risk of illegal\\nharms online. More specifically supportive of prosal in section 16 (vol 4) on reporting and compaints\"\\tSupportive of Ofcom’s assessment of how to mitigate the risk of illegal harms online.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9917409420013428,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'More specifically supportive of prosal in section 16 (vol 4) on reporting and compaints. especially for vulnerable users such as children.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9609107971191406,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Making an easy and accessible way to complaint and report a requirement for services to comply under the Act is an important advancement. User reporting and complaints (U2U and search) \\t\\tRespondent does question whether ease of reporting per se is the only element that needs improvement \\tRespondent does question whether ease of reporting per se is the only element that needs improvement as research on this and dicussions with social media companies shows that despite improvments to reporting processes user do not feel compelled to report. User reporting and complaints (U2U and search) \\t\\tNotes that the little to no research specifically on the reporting of illicit and harmful content such as illicit drugs by young people. Notes that the little to no research specifically on the reporting of illicit and harmful content such as illicit drugs by young people. Cites Ofcom media lives research findings that 14% of young people aged 12-17 have reported content, and highlights thatcertain  user groups may not even want to report such content. Goes onto cite Volteface research into factor that might factor into young poeple not reporting sales of drugs. User reporting and complaints (U2U and search) \\t\\tNotes the role of user reporting in relation to content moderation nd highlights the lack of research specifically on the reporting of illicit and harmful content such as illicit drugs by young people. Notes the role of user reporting in relation to content moderation nd highlights the lack of research specifically on the reporting of illicit and harmful content such as illicit drugs by young people. cited a study on cyber harrassment coping behaviours as potentially indicitive. Cited evidence suggests that increased reporting of illicit drug content would be benefical, but cautions against over relying on the measure to establish illegal content judgement guidance or assess prevelance\\nContent moderation (User to User) \\t\\tAgree that services should set performance targets for content moderation and search moderation, however they are concerned about the potential impact on content moderator wellbeing and subsequently user protection. Agree that services should set performance targets for content moderation and search moderation, however they are concerned about the potential impact on content moderator wellbeing and subsequently user protection. They note the emotional toll on content moderators associated with performance targets which poses risks to their psychological health and safety and user safety. The recomendation on this does not consider the impact on moderators and the risk of occuptational burnout, potentially impacting how quickly and accurately moderators will be able to remove illegal content. Respondent provides a list of references/citations\\nContent moderation (Search)\\t\\tAgree that services should set performance targets for content moderation and search moderation, however they are concerned about the potential impact on content moderator wellbeing and subsequently user protection. Agree that services should set performance targets for content moderation and search moderation, however they are concerned about the potential impact on content moderator wellbeing and subsequently user protection. They note the emotional toll on content moderators associated with performance targets which poses risks to their psychological health and safety and user safety. The recomendation on this does not consider the impact on moderators and the risk of occuptational burnout, potentially impacting how quickly and accurately moderators will be able to remove illegal content. Respondent provides a list of references/citations\\nContent moderation (User to User) \\t\\tRespondent raises concerns about user safety where performance metrics for content moderators may potentially impact their psychological health resulting in occupational burnout. Provides list of sources. Respondent raises concerns about user safety where performance metrics for content moderators may potentially impact their psychological health resulting in occupational burnout. Provides list of sources. Content moderation (Search)\\t\\tRespondent raises concerns about user safety where performance metrics for content moderators may potentially impact their psychological health resulting in occupational burnout. Provides list of sources. Respondent raises concerns about user safety where performance metrics for content moderators may potentially impact their psychological health resulting in occupational burnout. Provides list of sources.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.990362286567688,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approach to the Codes\\t\\ttransparency of prioritisation of content review should also include measures of hidden coercion. agree on applying transparent policy about prioritisation of content review. This policy should also include measures on hidden coercion in addition the virality of content, severity and likelihood that content is illegal, icluding if its flagged by trusted flagger. UCL have forthcoming work on coercion online which sho negative reaction to user content , not moderated by U2U service, negatively impacts experiences of LGBTQ+ users. Ofcom should consder seeking views of marginalised groups for prioirty factors in content review. Approach to the Codes\\t\\toffline options to reporting and complaints would be beneficial aswell as a dedicated reporting catergory or channel for women and girls facing domestic abuse or gender based abuse. Encourage services to use offline options in their complaints processes such as telephne no or written complaints alongside the digital process. Offers route of escelation who have submitted device to police as evidence. There is no guidacne in the UK about when police must return phones or devices. Also for those with stalkerware or spyware installed on their phones. Agree that dedicated reporting categories beyond fraud could be useful. Domestic abuse services and police acting as trusted flaggers for individuals seeking support for stalking and harrassement with clear link to illegal content. Welcome comment on dedicated channel for women and girls face domestic abuse or other forms of gender based violence, could be explored ahead of the 2025 consultation. Content moderation (User to User) \\t\\trefer to response 12.1\\trefer to response 12.1 (appraoch the illegal content code of practice)\\nAutomated content moderation (User to User) \\t\\trefer to response 12.1\\trefer to response 12.1 (appraoch the illegal content code of practice)\\nApproach to the Codes\\t\\tbureacratic burdern too heavy and implementation cost should be reduced\\t\"The bureaucratic burden is very heavy. Please consider our 2022 paper “ “Some\\nmeasures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill”as well as our additional comments below as means to reduce the implementation costs. The paper argues that the bill is too costly and favours totalitarian cencorship, providing recommendations on how to tackle this and reduce cost as well as protecting epilepsy sufferers, reduce enforcement costs, tackle fake news adn advertising and protect children.\"\\nApproach to the Codes\\t\\tbureacratic burdern too heavy and implementation cost should be reduced\\t\"The bureaucratic burden is very heavy. Please consider our 2022 paper “ “Some\\nmeasures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill” (The paper argues that the bill is too costly and favours totalitarian cencorship, providing recommendations on how to tackle this and reduce cost as well as protecting epilepsy sufferers, reduce enforcement costs, tackle fake news adn advertising and protect children) as well as our additional comments below as means to reduce the implementation costs\"\\nApproach to the Codes\\t\\tbureacratic burdern too heavy and implementation cost should be reduced\\t\"The bureaucratic burden is very heavy. Please consider our 2022 paper “ “Some\\nmeasures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill” (The paper argues that the bill is too costly and favours totalitarian cencorship, providing recommendations on how to tackle this and reduce cost as well as protecting epilepsy sufferers, reduce enforcement costs, tackle fake news adn advertising and protect children) as well as our additional comments below as means to reduce the implementation costs\"\\nApproach to the Codes\\t\\tbureacratic burdern too heavy and implementation cost should be reduced\\t\"The bureaucratic burden is very heavy. Please consider our 2022 paper “ “Some\\nmeasures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill” (The paper argues that the bill is too costly and favours totalitarian cencorship, providing recommendations on how to tackle this and reduce cost as well as protecting epilepsy sufferers, reduce enforcement costs, tackle fake news adn advertising and protect children) as well as our additional comments below as means to reduce the implementation costs\"\\nContent moderation (User to User) \\t\\tagree\\tagree with our proposal but no argument or evidence provided \\nAutomated content moderation (User to User)\\t\\tmetadata labelling could help to effectively remove harmful content without censorship. Metadata labelling in accordance to global standards can enable effective removal of potentially harmful content without censorship of lightweight filters. This reduces the areas which NCA would need to actively police and review content. Its the only way there could be an effective cypertipline which adhered to Ofcom transparency and openness requirement. Outcome 21 can only be implemented in accordance with global standards. Client side protections require economies of scale whch does not create commercial barriers to new entrants or protected silos for incumbents. failure to implement these measures could risk youthful behaviour resurfacing in a child\\'s adult life.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9818511009216309,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Has been identified as a long term security risk by foreign state actors building dossiers for blackmialing at a future time. Automated content moderation (User to User)\\t\\tHashmatching and URL detection are feasible if done in association with metadat labelling using SafeCast HeadCodes. evidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Automated content moderation (User to User)\\t\\tHashmatching and URL detection are feasible if done in association with metadat labelling using SafeCast HeadCodes. evidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Automated content moderation (User to User)\\t\\tMetadata labelling using SafeCast HeadCodes could move all CSAM URL detection requirements to be reserved solely for the Dark Web\\tevidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Automated content moderation (User to User)\\t\\tMetadata labelling using SafeCast HeadCodes could move all CSAM URL detection requirements to be reserved solely for the Dark Web\\tevidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Automated content moderation (User to User)\\t\\tMetadata labelling using SafeCast HeadCodes could move all CSAM URL detection requirements to be reserved solely for the Dark Web\\tevidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Automated content moderation (Search)\\t\\tMetadata labelling using SafeCast HeadCodes could move all CSAM URL detection requirements to be reserved solely for the Dark Web\\tevidence provided in the 'some measures to improve the effectiveness, enforceability and universality of the UK Online Safety Bill' makes case for use of SafeCast HeadCodes as metadata labels in video content. This would reduce volume of data needing to be reviewed under hash matching and URL detection for terrorism content, these actions will take place on the content on the dark web and away from day to day contact with children and vulnerble people protected by lihgtweight filters on mobile device. As HeadCodes have been accepted by SMPTE digital standards, its possible CSAM elimination measures could be implemented without censorship via the early implementation of SafeCast HeadCodes as Self Applied Content Ratings to be included whenever any video is uploaded. this was requested by DCMS in 2016 and NPCCC nd NCA, with Simon Bailey (Child protection lead at NPCC at the time) giving evidence to Home Affairs Select Committee in 2018. Governance and accountability\\t\\trefer to two page submission to the consultation provided, specifically the DCMA take-downs based on fake websites \\trefer to evidence provided re DCMA take-downs based on fake websites reported by Tax Policy Associates and our two page “Illegal Harms Consultation - Response from SafeCast”\\nGovernance and accountability\\t\\tagree that proposals are proportionate\\tOfcoms proposals are proportionate\\nGovernance and accountability\\t\\ttheir paper and comments in response to other questions \\trefers to 2022 paper 'some measures to imrpove the effectiveness, enforcability and universality of the UK Online Safety Bill' The paper argues that the bill is too costly and favours totalitarian cencorship, providing recommendations on how to tackle this and reduce cost as well as protecting epilepsy sufferers, reduce enforcement costs, tackle fake news adn advertising and protect children. Approach to the Codes\\t\\tsupportive of approach, emphasises importance of hash-matching, continued progress, and, the focus on good practice to leverage existing knowledge, foster collaboration and set high standards.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910709857940674,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"Supportive of approach - focus on core areas of design, operation and use of digital services is critical to designing and implementing effective tailored responses to specific threats. Welcomes targeted measures - a raft of effective content mod measures exist to detect and remove CSEA content  and platforms at risk of CSEA should implement these minimum standard tools. ‘Hash-matching’ techniques have significantly accelerated the identification and removal of known child sexual abuse material from the internet, automated or semi-automated AI classifiers are incredibly useful for detection, reporting, removal and blocking of new/unknown CSAM. Progress needs to continue (services with tech experts and industry) to enhance accuracy of classifiers to detect ‘unknown’ CSA (including livestreamed content) and grooming in both non-encrypted and encrypted video sharing environments. Report management increasingly efficient - 84% of the companies surveyed had at least partly automated processes for forwarding reports of CSA online.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9908815622329712,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The focus on good practice within industry will leverage existing knowledge to remain on the “digital front foot”, foster collaboration and set high standards.\"\\nApproach to the Codes\\t\\tAgrees with approach - highlights importance of flexible implementation to account for size, impact and market share of platforms + higher scrutiny for platforms popular with children \\tDifferent services and technologies have different risk profiles, criteria should include size, impact, market share AND there should be a higher level of scrutiny of platforms that are particularly popular/used by children. When explaining risk mitigation measures adopted, tech companies should be able to provide evidence of safety design approach in product design and engineering decisions. Effectiveness of regulation requires flexible implementation of risk mitigaiton mreasures which allow private sector to adopt solutions based on the specific risks on their services. Severity and immedidate impact of harm, imminent threat should attract most stringent risk mitigations measures. Industry stakeholders should have proactive, robust and comprehensive risk mitigation measures in place to detect, report, block and remove new and known CSAM, as well as to report it to the authorities. There needs to be explicit + strict obligations on high priority issues (CSEA) and for platforms with larger market share or high-risk profile. Approach to the Codes\\t\\tGlad to see alignment with EU DSA\\tGiven the transnational nature of the internet, international alignment on standards and rules is an important part of the Global Strategic Response to tackle illegal online harms, such as online child sexual exploitation and abuse. Not only does it ensure consistency in terms of the requirements and expectations for platforms and digital services in responding to online illegal harms, but it also reduces the burden on industry and facilitates a more effective and efficient response, reducing administrative burden. Approach to the Codes\\t\\tagrees with definition of a service that is multi-risk\\tThe Alliance agrees that with the definition of a service as being multi-risk where it is found to be high or medium risk for at least two kinds of illegal harms. Approach to the Codes\\t\\twould like to see specific measures for tackling emerging and evolving threats surrounding child sexual exploitation and abuse online\\t\"Ofcom\\'s proposed Codes of Practice represent a positive step toward protecting children online by tackling illegal content. It\\'s crucial to acknowledge that the evolving nature of online threats necessitates further development to effectively address emerging dangers.We would like to see specific measures for tackling emerging and evolving threats surrounding child sexual exploitation and abuse online, such as AI-generated illegal content (including deepfakes or manipulated images) and illegal content in Extended Reality (XR) environments (where immersive experiences can heighten the impact of harm). Generative AI tools have become mainstream in a short period of time. While calling for remedies for specific harms, the Codes should also be adaptable and regularly reviewed to address evolving challenges. \"\\nContent moderation (User to User) \\t\\t\"happy to see all digital services will be required to implement \\ncontent moderation processes to take down illegal content quickly and effectively.\"\\thappy to see all digital services will be required to implement content moderation processes to take down illegal content quickly and effectively. Content moderation (Search)\\t\\tagrees that all search services should have systems and processes that are designed so that search results that are illegal content are deprioritised or deindexed for UK users. agrees that all search services should have systems and processes that are designed so that search results that are illegal content are deprioritised or deindexed for UK users. Automated content moderation (User to User) \\t\\tpartly agree -  some concerns regarding private communications and further innovation needed for automated systems\\t\"Supportive of the proposal for both smaller and larger services that have a specific risk of hosting or facilitating CSEA deploy hash matching. Auto systems are essential for detecting, reporting, blocking and removing CSAM and URLs, particularly re scale and ever-increasing volume of CSAM online. Agrees with monthly user threshold (700,000) seems proportionate, and that U2U that are medium and high risk should be covered by Code. Particularly supportive that file storage and file sharing sites will be covered by code. In addition to hash matching other forms of detecting CSEA online should be encouraged - development of AI classifiers has been incredibly useful for unknown/new CSAM, could play potentially vital role tackling emerging threats such as AI generated material, livestreaming, abuse in extended reality. Video classifiers are least mature/lower accuracy rates making auto triage difficult. Without innovation or new solutions, this could significantly increase pressure on moderators and analysts given greater proportion of video that is being reported (NCMEC in 2022 received 88 mill reports with media files 37 being videos). Some concers re scope of measures - preference for anonymity, ease of access, availability of material or victim-survivors means E2EE likely to grow. There are options to perform client side scanning (scanning for matches to CSAM database before message encrypted), homomorphic encryption (allows operations to be performed without data decryption) and intermediate secur enclaves (decrypt message at server by third party and use tools to detect CSAM). Supports future-proof approach (covers all types of threats and adopts a tech neutral approach) if the measures do not cover private communications and E2EE environments it would be useful to hear proposals on how illegal online harm should be tackled on such services. \"\\nAutomated content moderation (User to User) \\t\\tuse of E2EE group messaging services to conduct CSA is liekly to grow - Ofcom should be mindful of these risks and ensure they are considered in overall implementation of OSA\\tE2EE messaging services and other forms of “private communications” continue to be preferred by many perpetrators due to the lower perceived risk of detection. Given that many perpetrators prioritise anonymity, ease of access, and availability of  material or victim-survivors when considering how and where to conduct CSA online, it is likely that the use of E2EE group messaging services will grow in the future. Ofcom should be mindful of the risks of increasing use of \\'private\\' E2EE messging services to perpetrate abuse and evade law enforcement in the absence of other safeguards and ensure they are considered in the overall implementation of the OSA. Automated content moderation (User to User) \\t\\taccuracy of hash mashing should be regularly reviewed - whilst some tools are free, the costs of applying hash matching tools for smaller services need to be considered\\tAccuracy of perceptual hash matching should be regularly reviewed to ensure it is consistent with current best practice and reflects the development of relevant tools. It is important to consider the costs of applying CSAM hash matching tools to smaller services. While there are a number of hash matching technologies that are available to services free of charge (for example Microsoft’s PhotoDNA Cloud Service is “a free service for qualified customers and developers” and Google’s CSAI Match is licensed “to a number of other technology companies free of charge”), the acquisition, deployment and management of such systems will require headcount. Automated content moderation (User to User) \\t\\t\"effectiveness difficult to assess with little comparable data across industry - sharing of trust and safety technologies and more consistent transparency reporting, policies and processes will be essential in facilitating a more effective response.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9945815205574036,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\t\"There are signs of offenders moving away from the curation of personal collections and preferring ‘on-demand’ access to content via the sharing of links (leading to CSAM). CSAM published and hosted in different jurisdictions, complicates evidence-gathering - offenders may not longer have a specific volume of content in their possession, one of several factors histroically used to assess the level of risk they posed. Provides link to report on industry players current responses to the threat of link sharing (all different). There is little comparable data available on how companies are responding, which makes it difficult to assess the efficacy of responses, (OECD’s 2023 Report on Transparency). The action taken by industry can depend on where the links take users.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9594452381134033,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Collaboration with leading safety technology organisations forms an essential part of the response for leading industry players. The Internet Watch Foundation’s (IWF) URL List is cited as a helpful tool in identifying potential harms and blocking access to illicit webpages and material. Project Arachnid in Canada is also highlighted as an effective technology to combat link-sharing. Sharing of trust and safety technologies and more consistent transparency reporting, policies and processes will be essential in facilitating a more effective response. Constant technological innovation and the increased deployment of artificial intelligence will be required to respond to the scale and complexity of the threat.\"\\nAutomated content moderation (Search) \\t\\tagree with proposal - would welcome further clarification on why the measure is recommended for general search services only and does not include vertical search services\\tagrees with the proposal to require both small and large search services at low, medium and high risk to deindex URLs which have been previously identified as hosting CSAM or which include a domain identified as dedicated to CSAM from the search index. Deindexing CSAM�linked URLs offers a multifaceted approach to protecting children. It aims to reduce accessibility, disrupt offender behaviour and ease the load on law enforcement.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9903469681739807,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Additionally, it sends a strong message against easy CSAM distribution/access and requires platforms to play their role in reducing its normalisation. We would welcome further clarification on why the measure is recommended for general search services only and does not include vertical search services. User reporting and complaints (U2U and search) \\t\\tagree with proposal - user reporting cannot substitute for effective and proactive safety by design measures\\t easy to find, easy to access and easy to use complaints system are particularly important - also welcome the direct reference to particularly vulnerable users or those with accessibility needs. Our Global Threat Assessment has highlighted disabled children as being particularly vulnerable to child sexual abuse and exploitation, and so it is critical that they can access clear and tailored reporting mechanisms. However, data from hotlines such as IWF and NCMEC demonstrates that the vast majority of their reports come from industry rather than user reports. While clear and accessible user reporting is essential, it will not substitute for effective and proactive safety by design measures in relevant platforms and services. The onus on addressing and remedying child sexual abuse and exploitation should not be on child users of platforms or services, nor on parents and caregivers. Terms of service and Publicly Available Statements\\t\\tagree with proposal - provides some further detail about how CSAM should be concisely and clearly dealt with by ToS\\t\"Regarding child sexual abuse content specifically, the terms of services for all digital services \\nshould clearly state that the platform has a zero-tolerance approach regarding child sexual \\nexploitation and abuse on its services. Terms and conditions should be concise and clear in \\ndefining what child sexual abuse material entails (photographs, videos, live streaming, grooming and digital or computer generated images, including the current emerging threat of AI-generated content), that such material is prohibited, how users can report child sexual abuse material and what the consequences will be for posting such content (ban from platform, account deletion, referral to law enforcement, investigation and possible prosecution). For platforms that are marketed to and/or regularly used by children, terms of service and publicly available statements should be provided in versions accessible to child users.\"\\n\" Default settings and user support (U2U)\"\\t\\tagree with specific measures to apply to all users under the age of 18 but concerned regarding abilities to identify child users (age assurance)\\t\"agree with specific measures to apply to all users under the age of 18 but concerned regarding abilities to identify child users - more comprehensive approach is needed for implementation of age assurance technologies on U2U services. Increasing number of children are accessing explicit content, chatting ot strangers or being coerced into sharing images of themselves. Research shows - children as young as 7 stumbling across adult pornography online, with 61% of 11-13 years olds describing as mostly unintentional. Alliance believes age assurance one of tools to create digital products safe by design. Age estimation and verification tools some of the safety by design solutions with most potential to reduce risk of online grooming. TEchnology is still relatively nascent. There are many different methods for AA - it is important for consumers to be provided with a choice to be in control of privacy (e.g. from traditional methods ID, mobile number, credit card checks to evolving tech face estimation, identity apps, social media proofing). Alliance supports clear guidance regarding improving the standards of default settings for child users of both smaller and larger U2U services where there is specific risk of CSEA (e.g. limiting network expansion prompts, hiding children accounts from connection lists, blocking the ability of unknown accounts to send direct messages, hiding location). Also supporting of suggested measures to improve provision of supportive information to children, incl informing children of risks is they disable default settings, but ALSO be cautious that not too much onus is placed on children in take these decisions, platforms still ahve responsibility to ensure tools and products are sfet by design and offenders/potential offenders are prevented from contacting children in first place. \"\\n\" Default settings and user support (U2U)\"\\t\\tseveral points within the user journey - firstly before account creation, when content is removed due to illegality (there is no-one size fits all approach, and onus must remain on platform) \\t\"There are several key points within the user journey where informing under-18s about the risk of illegal content is crucial. Firstly, before account creation - what is/is not allowed and what to do in those cases (e.g. reporting/flagging tools).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992402195930481,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We should also be cautious that not too much onus is placed on users to report. Platforms have a responsibility to deploy systems that detect illegal content online and hire enough content moderators, provide training, and support them to ensure that they have the tools and resources to act swifty to identify and takedown harmful content. It\\'s important to remember that there\\'s no one-size-fits-all approach, and the most effective strategy will likely involve a combination of these different methods. \"\\nUser access to services (U2U) \\t\\tlimitations and advantages of each method - best approach depends on specific needs of service and type of uses trying to block\\t\"Options for blocking include by [1] username [2] email [3] IP address [4] device fingerprint [5] combination. Methods should be appropriate for reason (e.g. violation of terms, security concerns), comply with relevant laws and ethical guidelines, users should be informed about process and have opp to appeal. Best approach depends on need of the service and type of users trying to block (should weigh limits and adv before applying = effective application). Limitations & Advantages:\\nusername- easy, directly identify user at source but easy to come back and create new account\\nEmail- more difficult but can still set up mor addresses adn accounts\\nIP- all access from a specific location or device is blocked - users can easily change IP address (VPN). Risk that users using same IP address legall are blocked (public Wi-Fi networks). device fingerprints- sophisticated uses combination of hardware (operating system, model of device, serial numbers etc) software, behavioural data - but privacy concerns and regulatiosn exist. USers can mitigate tracking (privacy-focused browers adn tools like anti-fingerprinting extensions).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934359788894653,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'combined- layered more robust - how can increase complexity. \"\\nUser access to services (U2U) \\t\\tstrict but proportionate sanctions in line with UK criminal law and for accidental sharing\\t\"Due to the illegal nature of the harm, we would support strict but proportionate sanctions in line with UK criminal law. Consideration should be given to an appropriate and proportionate \\nresponse to accidental or “non malicious” sharing of known CSAM (e.g. inadvertently sharing \\nwhile seeking advice on how to report).\"\\nUser access to services (U2U) \\t\\thuman review step in the process (most important), consistent investment and development in systems, and avenues to appeal misclassifications can help mitigate risk of misclassification\\t\"There is a risk of lawful content being misclassified as child sexual abuse material by automated \\nsystems, leading to potential infringement on user rights - there are some key elements to help digital services avoid and remedy misclassification. The most important of which is to always implement a human review step in the process.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9783238172531128,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'By ensuring that flagged content is reviewed by trained human moderators before taking action, it allows for nuanced judgment and reduces false positives. It is the responsibility of those developing and deploying automated systems to detect child sexual abuse material to consistently invest in developing more sophisticated AI models that can better distinguish between CSAM and lawful content. This might involve incorporating context, understanding artistic merit, and considering user intent.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.5562228560447693,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Content moderation policies and the decisions made based on them need to be clearly \\ncommunicated. Users should be provided with avenues to appeal misclassifications. \"\\nService design and user support (Search) \\t\\tagree - but would welcome all digital services to invest more in preventative response by disrupting searches and signposting appropriate support services (not just large)\\t\" supports the proposal that search requests where the wording clearly indicates that the user may be seeking to encounter CSAM and which use terms that explicitly relate to CSAM should provide content warnings and support resources to users. This form of secondary prevention is a key part of a preventative approach to reducing CSEA online. The 2023 Global Threat Assessment underscores that secondary prevention initiatives are  typically focused on individuals at higher risk of violent perpetration. A study assessing the \\ncontribution of Stop it Now! Helplines in the UK, Ireland and the Netherlands shows they “can \\nprovide cost effective, quality advice and support [...] to prompt behaviour change in adults and \\nstrengthen protective factors which can reduce the risk of offending”. Use of such support \\nservices is also increasing, Lucy Faithfull Foundation - people seeking advice or support via online self-help, or their confidential helpline has trebled since 2020. While it is positive that all large services are in scope of this proposal, we would welcome all \\ndigital services to invest more in a preventative response by disrupting searches on their services and signposting appropriate support services.\"\\nCumulative Assessment  \\t\\tagree - size, impact and market share should also be considered AND a higher level of scrutiny of platforms marketed/regularly used by children\\t\"platforms and digital services which are most at risk of causing serious and immediate \\nharm through facilitating the most abhorrent crimes, such as CEA online, should be prioritised. Size, impact and market share should also be considered when determining how to prioritise the regulation of such a wide variety of platforms. In addition to these criteria, there is also a strong case to be made for a higher level of scrutiny of platforms that are marketed to and/or regularly used by children. It is essential that such platforms adhere to the rules and provide products which provide a safe, positive experience for child users, based on a sophisticated understanding of likely risks. Larger services with a broader, global user base have a responsibility to ensure services are safe by design, that their teams have the resources and infrastructure to quickly detect and tackle harm and provide support to their more vulnerable user.\"\\nGovernance and accountability  \\t\\tThere may be downsides for corporate dynamics\\t\"there may be some potential downsides to this:, s.a. less people willing to take over\\nsenior positions that involve content moderation, or more pressure on employees not for the good reason (being ensuring compliance) but to ensure remuneration of their manager. Corporate dynamics may be impacted negatively under this perspective.\"\\nApproach to the Codes\\t\\tTo avoid DSA uncertainty - clearer definition of average user base and indication of how services should calculate it is required\\t\"As the categorisation of large services is connected with the size of average user base, we believe useful that a more precise definition of ‘average user base’ is to be provided. With the\\nimplementation of the DSA, many services have struggled with providing a uniform, consistent\\nmethodology in calculating their monthly active recipients, as the Commission decided not to\\nissue any guidelines on the right interpretation of ‘monthly active users’ and the correct\\ncalculation methodology. To avoid similar discrepancies (and potential discrimination) we would welcome a clear definition of the user base and an indication of how services should approach\\ntheir monthly calculation.\"\\nApproach to the Codes\\t\\tSending acknowledgement of report is important but an indicative timeline for response is excessive; Ofcom should define who trusted flaggers (TF) are and maintain an official list of TF for services. \"Similarly to the DSA, to provide a clear definition of who trusted flaggers are and what their eligibility criteria are. Regarding 5C. Appropriate action – sending indicative timelines: we believe it is important to send an acknowledgment message to the complainant, but sending also an indicative timeline seems a bit excessive. Especially because the timeline might vary on the service workload and on the specificities of the single case, specificities that can only be assessed during the review of the complaint. As long as the service is capable of meeting its timely internal SLAs to process complaints, no further obligations should be imposed on the service.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9884234070777893,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Regarding 5I. Dedicated reporting channels (Trusted Flaggers) - OFCOM to include the list of confirmed TF in a dedicated, updated, public database (managed by OFCOM), with dedicated email addresses for each TF, which the TF will use to notify services of illegal content. This will allow the services to associate that email address (or other identification information) to a certified Trusted Flagger, and so channel that specific report through the fast track.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9905281066894531,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Also to set up a process to ensure these TF maintain their competencies, and if not, a process for revoking their TF status. Also, if more TF are later added/removed to the list, OFCOM shall inform the services of this addition/removal.\"\\nUser reporting and complaints (U2U and search) \\t\\tProviding an indicative timeline for response is excessive and complicates automated acknowledge of report\\t\"we believe the provision of timeframes for complainants is excessive and would complicate the\\nservice’s automated acknowledgment response system because providing a timeframe for the\\nspecific case would require first analysis of the circumstances and a subsequent, manual\\nestimation of the time that would require the service to review the complaint.\"\\nTerms of service and Publicly Available Statements\\t\\tbalance needs to be struct between density/granularity of information and user-friendly, easy-to-read Terms. Too much transparency can also help bad actors take advantage. \"We welcome the inclusion in the Terms of relevant provisions regarding (a) how individuals are protected from illegal content, (b) information about any proactive technology used for compliance with the illegal content safety duties, and (c) policies and processes that govern the handling and resolution of relevant complaints. However, we deem that a balance should be struck here between the density and granularity of such information (which could be, for their nature, quite technical and a little understandable by the average user) and the need to propose user-friendly, easy-to-read Terms. A good compromise could be to keep the Terms succinct (yet complete) and redirect to more detailed materials, through the use of hyperlinks, where users can learn more about these topics. Also, too much transparency about the service’s policies and processes can offer the flank to\\nabuses by bad actors, we are known to be among those who read the Terms in-depth and take\\nadvantage of the service’s details to bypass its systems.\"\\nContent moderation (User to User) \\t\\tOfcom should consider reducing the burden on service providers by widening the “reasonable grounds” threshold for inferring an account’s affiliation to a proscribed organisation. \"Agree = U2U services should employ a strikes and blocking system against users where they are found to have posted or shared illegal content, or committed or facilitated illegal behaviour (proportionate and well-evidenced as a deterrent). Agree =  insufficient evidence to include broad measures that would recommend that services immediately block accounts which post any illegal content. Beyond the technical and rights-based challenges highlighted, more research is needed on the unintended effects of ‘deplatforming’ extremist actors and on measuring overall harm reduction (risks include migration to unmonitored spaces, radicalising effects on communities, and driving innovative moderation evasion). To revise = Recommendation that U2U services should immediately remove accounts run by or on behalf of proscribed organisations  - this is proportionate given high likelihood that content generated by these accounts would amount to priority illegal offence. In practice there are relatively few ‘official’ accounts verifiably run by proscribed organisations (they tend to be on stable platforms resistant to takedown e.g. TOW, on terrorist servers, or on encrypted messaging apps). Further service providers are unlikely to have the necessary tools to distinguish between ‘official’ and ‘supporter-run’ accounts (particularly smaller platforms). Ofcom should consider reducing this burden on service providers by reformulating the “reasonable grounds” threshold for inferring an account’s affiliation with a proscribed organisation and by considering expanding interpretation of being “run by or on behalf of” to include accounts that profess to be a proscribed organisation (through explicit statement or through any use of official symbology or terminology). This can be justified given affiliation declared in this way, although unverified, will nonetheless benefit a proscribed organisation if the account is run by supporters rather than members. \"\\nAutomated content moderation (User to User) \\t\\tURL detection for terrorism content should be used for beacons and aggregators and should overcome use of link shorteners. Tech platforms could utilise TAT\\'s TOW Database and Beacon Channels tracker for URL detection, Terminology Database for keyword detection (with safeguards for human rights). Ofcom could also consider recommending image and symbol detection for terrorism content (TAT offers an Image database). \"URL detection for terrorism content should be focus on beacons (platforms to project content to widest audience possible, e.g. social media, VSPs, messaging platforms, gaming platforms) and aggregators (centralised databases where content can be found e.g. paste sites, social media and forums). URL detection should also focus on disrupting outlinks that lead users to TOWs, which pose one of the most significant threats to the global effort to tackle terrorist use of the internet. Since TOWs are difficult to remove at the infrastructural level, disrupting the dissemination of outlinks to TOWs essential. A modified URL obtained via a URL shortener service can help bypass specific URL blocks by platforms. This strategy can be countered by platforms blocking online domains for terrorist-operated websites entirely, rather than blocking specific URLs to individual pieces of content. TAT’s Knowledge Sharing Platform contains resources that platforms could utilise for the URL detection of terrorism content including - TOW database; Beacons Channels tracker. Further recommendation - recent resolve network report, pro IS users rely on volume and speed (use of auto tools for rapid generation and dissemination of banks of URLs). Platforms could therefore make use of behavioural based cues (abnormal posting volume, or other mechanism in place for spam behaviour to disupt terrorist use of service. Tech platforms could utilise TAT\\'s Terminology Database for keyword detection. Recommendations for keyword detection should incorporate safeguards for human rights, including a balance with human review, and consideration of other contextual identifiers. Ofcom could also consider recommending image and symbol detection for terrorism content. TAT offers their  Image Compendium that platforms could use to detect images and symbols relating to terrorism. \"\\nAutomated content moderation (User to User) \\t\\tsummarises the utility and efficacy of hash-matching for terrorism content addressing concerns around ‘context’ and freedom of expression - Video and GenAI poses challenges\\tHuman oversight still necessary - costs are very platform dependent. TAT isn\\'t in a position to assess accuracy of existing tech (as do not have access to hash databases or tech companies systems - more info on GIFCT link provided). However, there is evidence that hash-matching offers an effective solution for moderating terrorist content in many contexts, particularly where the content is stable and unlikely to be changed as it proliferates (e.g. a still image or document). This is especially so when used in tandem with access to a database of known terrorist material, such as that currently being built as part of the Terrorist Content Analytics Platform (TCAP). Video content poses a much more complex challenge for hash detection, especially where footage is edited or excerpted (or livestreaming). Furthermore, as online actors seeking to disseminate terrorist propaganda begin adopting advanced generative AI tools to produce content, the opportunities for hash matching against known material are likely to correspondingly decrease. There is no particular distinction between terrorism material and CSAM in terms of the difficulties and variations between or within types of files (images/videos/documents) or the relevant hashing protocol. No hash-matching solution can ever be fully accurate.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924296736717224,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Governance and accountability\\t\\tSupportive of aspects of governance proposals. Supportive of all services having minimum requirements and basic obligations, and are pleased to see there must be a named person responsible for compliance with illegal content dutias and reporting and compliance duties, and all U2U and search services have certain requirements. Governance and accountability\\t\\t\"Services should be scoped in based on risk and functionality, not size. Governments must normalize adherence to basic online safety standards – such  as keeping illegal CSAM off services – as an expected “cost of doing business” in the technology \\nindustry.\"\\t\"Basing child safety proportionally on the popularity of a product does not exist in the physical world, and should not exist online. There are several examples of very small service providers that have an incredibly outsized impact \\non harm. These often include file hosting services or fast-growing start up social networking \\napplications. There needs to be flexibility in the legislation/regulations to ensure these edge cases \\ncan be scoped in as needed. Canada’s recently announced legislation contemplates bringing \\nsmaller companies into scope once risk is established. Two fundamental priorities that should matter for all online services, regardless of size, are proposed in response to question 8. Some scaling of the exact requirements may help account for the size of the user base and the company. We hope that even small services have minimum requirements to \\nensure they address illegal content. Basic obligations will lead to more consideration of issues \\nduring development, and help prevent online services from being misused. Any measure of size may create arbitrary distinctions between companies just above/just below the line, which may create confusion if a user base fluctuates rapidly, and \\nmay in some cases create incentives to disperse services over multiple platforms.\"\\nGovernance and accountability\\t\\tServices aimed at children are likely to be smaller, and as they are aimed at children may attract adults who intend to harm children, but are less likely to meet a user threshold based on the entire population. \"As well, services aimed at children are likely to have smaller user bases. Platforms aimed at \\nchildren can attract some adults who intend to harm children, but such platforms are intended \\nfor, and generally used by, a smaller portion of the population. Such services are less likely to \\nmeet a user threshold that is based on the entire population, and a more appropriate measure \\nwould be the number of child users. One example is Wizz app, aimed at 13-24 year olds. Cybertip.ca has received 180+ reports concerning Wizz since 2021, leading to a Cybertip.ca alert in early 2024. As stated in the alert, compared to 2022, we received 10 times as many reports about the app in 2023. Reports about Wizz increased faster than any other platform. Most reports concerned sextortion involving male victims. The majority of victims reported to Cybertip.ca were between 15 - 17 years old. The app has 15 million users worldwide.\"\\nGovernance and accountability\\t\\tResponse sets out basic requirements C3P believe services should have to incorporate into their business models. \"Tracking new illegal content – Any service should be obligated to track new kinds of ille�gal content and unusual increases in illegal content. Companies can leverage automation \\nto do this efficiently. Code of conduct – A code of conduct is a basic obligation in many other contexts and will \\nhelp promote safer company cultures. The code of conduct could be as simple as a one�pager that makes employees aware of the company’s commitment to removing CSAM, \\nprioritizing child safety, and having no tolerance for harassment, illegal material, etc. Adequate compliance training – Training is essential to the success of compliance programs and should be a basic requirement on all services. Annual risk review – modified: The annual risk review process is only required of large companies with more than 7 million monthly UK users. As recognized in Volume 2, smaller \\ncompanies may be targeted because of their size and the perception that they have less \\nrobust safeguards. As noted in the opening of this response, smaller services aimed at \\nchildren may still pose significant risks and warrant regular risk reviews. Looking at the most common file-hosting services used to distribute CSAM based on Project Arachnid data, it is unlikely that any of these services — most of which would not be \\nrecognized by average citizens — would meet the 7 million UK user threshold. We strongly recommend considering either a lower user threshold or a threshold based on total bandwidth. In the alternative, perhaps the annual risk review could be scaled down for smaller companies while still requiring some reporting so they turn their minds to this issue on a regular basis, as technology evolves and new risks emerge.\"\\nGovernance and accountability\\t\\tProvides CSAM-specific example of the need for policies, training, and monitoring illegal content\\tExample given of R v YesUp ECommerce Solutions Inc., 2020 CarswellOnt 19731 (ONCJ) (copy provided with submission). The company included online advertising and home internet services, and grew to file hosting and web hosting.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9966872334480286,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"In 2012 it drew up two abuse policies, neither of which addressed CSAM or other child safety concerns. In 2012, the file host service Lumfile began renting 32 of YesUp's servers for file hosting. Between May and October 2012, YesUp received over 200 notices that their services were making CSAM available. YesUp took no action and continued to provide technical support. The servers were eventually seized by Police. The reported legal decision notes, “Between July and August 2012 alone, Lumfile.com saw 3,289 new members; an increase of 42%. In the same time frame, the number of files uploaded to Lumfile.com rose by 288,001, or an increase of 260%. By October 2012, Lumfile.com had 59,954 registered users, and 1,817,142 files uploaded to its servers” (paras 43 and 44). over 395,994 pictures or movies of confirmed CSAM were made available for download, and over 19 million CSAM files were downloaded. Police identified 11,901 user accounts at Lumfile.com that were engaged in downloading CSAM. The numbers suggest the site was targeted by the community of perpetrators and was quickly used for illegal purposes after its launch.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.6337305903434753,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'YesUp was fined in 2020. C3P state this \\'illustrates the need for basic responsibilities for companies of all sizes.\\'\\nGovernance and accountability\\t\\tSignposts to response in 3i. Signposts to response in 3i. Governance and accountability\\t\\tSignposts to response in 3i. Signposts to response in 3i. Approach to the Codes\\t\\t\"Understand the approach of placing additional requirements on larger companies but the concept of basing child safety proportionally on the popularity of a \\nproduct does not exist in the physical world, and should not exist online. \"\\tThe concept of basing child safety proportionally on the popularity of a product does not exist in the physical world, and should not exist online. Understand the approach of placing additional requirements on larger companies as a general proposition in any regulatory framework. Approach to the Codes\\t\\tConcerned that the best interests of children are being outweighed by competition/innovtaion concerns. Concerned that without guardrails and fundamental requirements for entities that allow user-generated/uploaded content, the best interests of children are being outweighed by concerns over competition and innovation. Suggesting safety stifles innovation suggests that unsafe companies have an advantage over safe ones. An apporoach is needed that incentivises safe innovation. Approach to the Codes\\t\\tPrioritising child safety is consistent with the UK government\\'s obligations under the United Nations Convention on the Rights of the Child\\t\" Prioritizing child safety is consistent with the UK government’s obligations under the \\nUnited Nations Convention on the Rights of the Child to make the best interests of the child a \\nprimary consideration in all actions, including regulatory actions, concerning children.\"\\nApproach to the Codes\\t\\tTech has been unregulated too long and the scale of harm too extensive to take an overly cautious approach. Volume 4 indicates the proposals within are first steps.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9951364398002625,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Industry actors have not taken voluntary action, and in the wake of this, CSAM has flourished online. Ending impunity in relation to CSAM must be an urgent priority. Approach to the Codes\\t\\tDisagree with the proposal. \"Preventing CSAM should be an obligation on all platforms, even if they are small, and even if \\nCSAM is the only risk for the platform. \"\\nApproach to the Codes\\t\\tCompanies (examples provided) do not have policies to tackle illegal content or mitigate harm and become platforms of choice for perpetrators. \"Those that are not major players do not have policies to tackle illegal content or \\nmitigate harm, and become platforms of choice for perpetrators.\"\\nApproach to the Codes\\t\\tMeasures to detect CSAM are available for free for smaller platforms and can be employed easily. Measures to detect CSAM are available for free for smaller platforms and can be employed easily. Approach to the Codes\\t\\tExamples provided of small services that C3P issued CSAM notices to in the last year. Examples of services with number of CSAM notices issued. Approach to the Codes\\t\\tExample service provided which may be considered small but resulted in large scale harm. \"Site name provided in response. The site was crawled by C3P\\'s project arachnid in March 2022 and 53 links hosting archive files assessed to be CSAM were discovered, and removed following removal notices. Some reappeared at a later date. Extra information was extrapolated from the files showing that: \\n- The 53 links found to be hosting CSAM on this service facilitated the creation of more \\nthan 75.1 million new copies of CSAM or harmful images on local computers worldwide. - The content made available from the links were classified. This revealed that approxi�mately 45% of the 4,291 files being made available were classified as pre-pubescent \\nCSAM, while the remainder was classified as “harmful/abusive”. - In total, the 53 archive files were downloaded 628,883 times before Project Arachnid is�sued takedown notices. - Some of the files had been live on the internet since 2015. - Among the 53 archive files, the most downloaded archive file contained a lengthy compi�lation video of a known victim of CSAM. That archive file was downloaded 61,250 times. - In addition to the above, some download links contained computer programming code \\ndesigned to help users automate the mass downloading of the links on dark web forums.\"\\nApproach to the Codes\\t\\tNotes that comments are focused on Volume 4 as the codes reflect the proposals therein. Notes that comments are focused on Volume 4 as the codes reflect the proposals therein. Content moderation (User to User)\\t\\tAgree that all U2U services should be required to take down illegal content swiftly and recommends proposals to apply to all U2U services (rather than just large and multi risk services)\\t\"Recommend applying the following proposals to all U2U services (they are currently limited to large and multi-risk services):\\n- Prepare and apply a policy about the prioritisation of content for review. - Ensure people working in content moderation receive training and materials that enable \\nthem to moderate content efficiently. \"\\nContent moderation (User to User)\\t\\tHaving a policy an providing training are not overly onerous, and are closely related to the requirement to have systems to take down illegal content. The training may be less extensive for smaller companies, but there must still be some training. it is difficult to envision the systems functioning effectively without a policy and training. Volume 4 acknowledges the need link between training and giving effect to content moderation policies at 12.174. Content moderation (User to User)\\t\\tRecommend Ofcom provide guidance on the need for regular training. The tactics offenders use to harm children evolves, and there must be ongoing training that updates trainees on emerging risks. Content moderation (Search)\\t\\t agree that all search services should be required to have systems designed to deindex or downrank illegal content, and recommend extending proposals to all serach services (rather than just large and multi risk services)\\t\"We recommend applying the following proposals to all \\nsearch services (they are currently limited to large and multi-risk services):\\n- Prepare and apply a policy about the prioritisation of content for review. - Ensure people working in search moderation receive training and materials that enable \\nthem to moderate content effectively. Our reasoning for recommending the policy and training requirements extend to all parties is the same as in Question 18. Additionally, having a policy and training is especially important given the flexibility search services have to deindex or downrank according to their own determinations of what is appropriate in the circumstances.\"\\nContent moderation (Search)\\t\\tThe proposal is missing a consideration of the privacy violation of the person depicted when content is not deindexed or downranked. \"The focus is on minimising the risk of individuals en�countering illegal content through searches, but when that illegal content is CSAM or other im�agery of a survivor of CSAM, there is also a need to protect that individual’s privacy and safety. Protection for victims’ rights is acknowledged in 12.62 in relation to removing illegal content from \\nU2U services.\"\\nAutomated content moderation (User to User)\\t\\tAgree that automated content moderation should be required for user to user services. However, the proposals rest on the misunderstanding that automated CSAM moderation is costly or difficult to employ for smaller companies. \"Agree that automated content moderation should be required for user to user \\nservices. However, the proposals rest on the misunderstanding that automated CSAM moderation is costly or difficult to employ for smaller companies. Costs addressed under Q22. \"\\nAutomated content moderation (User to User)\\t\\tSmaller services may not track their user number. Smaller services may not track their user number. Automated content moderation (User to User)\\t\\t70,000 users is a high threshold for CSAM measure. For the 70,000 UK user threshold, it is very possible that some organizations are not currently set up to know this number. 70,000 UK users is also a high threshold considering the amount of CSAM that could spread on a service of that size. Automated content moderation (User to User)\\t\\t\"The notion that automated content moderation is privacy invasive overlooks the fact that\\nallowing CSAM to be uploaded and shared is massive violation of the privacy of the person \\ndepicted.\"\\t\"The notion that automated content moderation is privacy invasive overlooks the fact that\\nallowing CSAM to be uploaded and shared is massive violation of the privacy of the person \\ndepicted.\"\\nAutomated content moderation (User to User)\\t\\tProvides link to 2021 Project Arachnid report to highlight the problem of reappearance of known CSAM. Noted that nearly half of all media (48%) that triggered the issuance of a removal notification to an ESP, had previously been flagged on that ESP’s service by Project Arachnid, illustrating the problem of known CSAM appearing on services that do not employ adequate moderation or upload prevention. Automated content moderation (User to User)\\t\\tRefers to example above of a service with less than 70,000 worldwide users with a high volume of CSAM. The service (name provided in response) had under 70,000 worldwide users, yet police identified over 11,000 accounts engaged in download CSAM; over 395,000 pictures or movies of confirmed CSAM were made available for download, and over 19 million CSAM files were downloaded. The file hosting service example provided in response to question 13 also illustrates on CSAM can proliferate at an immense scale from a small service. Automated content moderation (User to User)\\t\\tThe delineation between public and private is vague. We understand a pragmatic approach, but there could be some suggested thresholds or a consideration of the specific nature of groups that are formed to share CSAM. Automated content moderation (User to User)\\t\\tLarge Telegram groups are used to share CSAM. WeProtect (link provided) has noted that “the use of E2EE group messaging services is likely to grow in the future… [which] could lead to wider distribution of child sexual abuse material”. To the extent these groups have “access restrictions”, it should be noted that those restrictions are generally intended to evade law enforcement and are in place due to the illegal nature of the group. Automated content moderation (User to User)\\t\\tThe sharing of CSAM is a violation of the privacy of the individuals depicted. The sharing of CSAM is a violation of the privacy of the individuals depicted. Automated content moderation (User to User)\\t\\t\"Project Arachnid data suggests perceptual hash matching is highly accurate and an effective way to remove \\nCSAM. \"\\t\" Project Arachnid has issued over 39 million removal requests based on hash matching. False positives have been rare. Link provided to Project Arachnid 2021 report (see response), showing roughly half of media actioned by Project Arachnid is removed within 24 hours. \"\\nAutomated content moderation (User to User)\\t\\tShield by Project Arachnid is available to smaller companies at no cost. \"Several smaller companies are using C3P hash lists for free, and there is no specialist \\nknowledge required. Project Arachnid handles up to 10s of millions of images every day from smaller providers. As acknowledged in 14.64, the costs are likely to be significantly lower, and perhaps even negligible for smaller services. As further acknowledged in footnote 202 of Volume 4, NGOs will often work with smaller companies to provide no or low-cost options, and that is what C3P does.\"\\nAutomated content moderation (User to User)\\t\\tA recent Meta report (link provided) reveals very few false positives. Meta states that it uses media matching technology and conducts human audits on samples of detected media. There were 6.6 million CSAM matches made related to EU citizens with 29,000 appeals. Of the appeals, 3,700 pieces of content were restored. This gives a real-world baseline for accuracy rates on a very high volume provider. 29,000 appeals from users equates to an appeal rate of 0.43%. Automated content moderation (User to User)\\t\\tC3P\\'s hash list is available to ESPs who request access for the purpose of filtering or removing CSAM. \"C3P makes its hash list available to ESPs who request access for the purpose of filtering \\nor removing CSAM. C3P will work with the company to put in place the right contractual terms and other measures to enable access.\"\\nAutomated content moderation (User to User)\\t\\t\"Project Arachnid can match URLs, and the list of URLs available in a similar manner to hash value lists, at \\nno cost.\"\\tProject Arachnid can match URLs, and the list of URLs available in a similar manner to hash value lists, at no cost. Automated content moderation (Search)\\t\\tAgree that URLs identified as hosting CSAM or as being part of a website entirely or predominantly dedicated to CSAM should be deindexed from the search index of a relevant service. This should include cached pages. Obligations should be in place to identify identical sites. Problematic sites can readily shift content to new web addresses. So the initial problematic site is shut down, and within hours/days a new site emerges with the same content depicted, and so on and so on. Obligations should be in place to help ensure swift identification and deindexing of these identical sites given that this is a known phenomenon\\nAutomated content moderation (Search)\\t\\tExample provided of a known CSAM site which shifted content to new web addresses. Example provided of a known CSAM site which shifted content to new web addresses. User reporting and complaints (U2U and search)\\t\\t\"Agree with proposals  for simple complaints mechanisms and taking appropriate \\naction on complaints.\"\\t\"Agree with proposals  for simple complaints mechanisms and taking appropriate \\naction on complaints.\"\\nUser reporting and complaints (U2U and search)\\t\\tAgree that context may engage user privacy rights. Section 16.85 - Companies have at times asked children to prove their age, or in the case of non-consensually distributed intimate images, asked the victim to provide identification to prove it is them in the image/video. While companies must comply with data protection laws, not all will take the necessary care with personal information, so there must be limits to what companies can require before they take action. User reporting and complaints (U2U and search)\\t\\tProvide 2020 report as for the need for CSAM-specific reporting functions. Link provided to 2020 report to explain the need for  CSAM-specific reporting functions within various functions of popular platforms (e.g., within a post, reporting a user, within messages/chats, and other publicly visible content when not logged in). Some survivors use these functions to report their own CSAM and flag it for removal. User reporting and complaints (U2U and search)\\t\\tCompanies should be made aware of the need to keep the information of complainants, particularly those complaining about sexual content, confidential to ensure the safety and security of those complainants. \"In the US, an incident occurred whereby Google publicized the names of individuals making a \\nDMCA request. In an instance C3P is aware of, the search result complained of was removed, but \\nthe name of the survivor was made publicly available as the person making the DMCA request. C3P recognize that the UK GDPR would likely apply to ensure such information would not be \\npublicized in the UK. (Response to be kept confidential to avoid it becoming known that this may be a a way to search for information about CSAM survivors)\"\\nTerms of service and Publicly Available Statements\\t\\tThe proposals are a good starting point. Terms of service are often the basis for Project Arachnid removal requests on harmful-abusive material (which encompasses harmful images of children that do not meet a criminal law threshold but may nonetheless violate an ESP’s terms). Default settings and user support for child users (U2U)\\t\\tConcerned that children can change default settings, and measures only apply to those with some form of age assurance. The UK has an Age Appropriate Design Code and other tools such as the UK GDPR which ought to be leveraged to ensure meaningful change is implemented earlier than the time age assurance mechanisms are in place. Awaiting the age assurance consultation before the measures in Chapter 18 are realized unduly delays the protection of children. the case can be made for implementing some measures for all users until such time as reliable age assurance measures are in place. Default settings and user support for child users (U2U)\\t\\tThose with medium and high risk of grooming, regardless of size, should be in scope. Limiting these measures to platforms with a high risk for grooming and large platforms with a medium risk for grooming is too narrow. Default settings and user support for child users (U2U)\\t\\tChildren under 12 should not be able to change their settings. C3P\\'s concern is the vulnerability of children in that age range to online grooming offences. Statistics Canada, has said “the risk of cybervictimization increases with age, from 12 to 17, mirroring the increased frequency in the use of social networking, video and instant messaging as youth age\" (link provided to source in response). Specific to online sexual exploitation, a Statistics Canada report released in 2022 indicated that 84% of victims of online sexual offences against children were aged 12-17 (link provided to source in response). The 12-14 age group is incredibly vulnerable to online luring. For sextortion, boys aged 15-17 are the biggest target (link provided to source in response)\\nDefault settings and user support for child users (U2U)\\t\\tAn additional protection could be removing the functionality that invites users to make known their other social media handles as part of the account creation process. \"The app Wizz invites users as part of the account creation process to make known their \\nother social media handles. \"\\nDefault settings and user support for child users (U2U)\\t\\tDefault parameters for children using the livestreaming functions would be important. \"Similar protections could be included in the livestreaming function as are set out for \\ndirect messaging. In addition, information could be displayed for the child before engaging in \\nlivestreaming of the risk, for example, that it could be recorded by the other user.\"\\nEnhanced user control (U2U)\\t\\tAgree with proposals providing users controls to block/mute users and disable comments on their own posts, but would recommend further measures. \" Concerned that Ofcom is not recommending that services extract or retain \\ninformation about blocked accounts (see section 20.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7715262174606323,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '44). A counter consideration is that if \\nplatforms kept track of blocked accounts, that is likely to surface bad actor accounts more \\nquickly. Treating the complaints as one-offs misses the overall pattern. - Recommend considering the interconnected nature of many platforms and the need for information to be shared across various services operated a given company. - There should also be an opportunity to know that someone is coming from the same IP address (and therefore likely the same person) - there must be a more proactive way to protect end users in these cases. - Users should have more options for geo-fencing. They should be able to limit who can contact them outside of a certain geographical radius such as their own city. More should be done as youth will block the user and the user (or group of users) will simply set up a new account and recontact the youth. This is especially in sextortion incidents. The levels of aggression and violence often wear children/youth down\"\\nEnhanced user control (U2U)\\t\\tthere should be requirements for making these controls known to users, including any changes to how the controls works or added controls. The controls themselves must be very visible and not buried. \"there should be requirements for making these controls known to users, including \\nany changes to how the controls works or added controls. The controls themselves must be very \\nvisible and not buried.\"\\nUser access to services (U2U)\\t\\tRecommend adding to the proposal \"or a user connected to or part of organized crime, such as a financial or sexual extortion ring\". \"Police organizations worldwide are experiencing a surge of reports tied to organized \\ncrime operating outside the reach of local law enforcement. These reports relate to extortion \\ntactics employed against vulnerable users, particularly but not exclusively children . Many police \\nagencies have stated publicly these crime enterprises are based in Nigeria. Regardless of where \\nthey are based, once platforms start getting reports from users about these types of tactics, in our view the platforms should be able to implement mechanisms to block these predatory users from accessing UK users. \"\\nUser access to services (U2U)\\t\\tIn C3P’s view, it is reasonable for a user who has shared known CSAM to lose the privilege of ever returning to that service. \"In C3P’s view, it is reasonable for a user who has shared known CSAM to lose the \\nprivilege of ever returning to that service. \"\\nUser access to services (U2U)\\t\\tUsing quality hash/URL datasets and supplementing with human moderation will assist. This is especially important for adolescent CSAM. \" Human moderation alone, without the additional flag associated to a \\nhash set that indicates the youth is a part of an identified series, may otherwise result in a human moderator overlooking it. For example, Meta has publicly stated that their moderation approach is to “err on the side of an adult” if the age of the person depicted is questionable, illustrating the complete abdication of any concern for the adolescent child. (Link to article provided).\"\\nService design and user support (Search)\\t\\t\"Agree with the proposals but detecting CSAM searches and deploying warnings should \\napply to all search functions, not only large services.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9930691719055176,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\t\"Agree with the proposals but detecting CSAM searches and deploying warnings should \\napply to all search functions, not only large services. \"\\nService design and user support (Search)\\t\\tAgree with cited evidence. Services need a keyword/symbols data source likely to return CSAM results. Agree with the evidence Ofcom has cited about the role search services can play in providing a pathway to CSAM. We note that section 22.40 is important. Online communities of all  types develop their own vernacular and coded references; CSAM and CSEA perpetrators are no  different. We add that services need a source of data that provides keywords and symbols that  might be expected to return illegal results, but are associated with CSAM. This information must be updated on a regular basis as new terminology is introduced to evade detection. Cumulative assessment\\t\\tFrom a survivor perspective it may not be proportionate to allow small services to harm until they become larger. From a survivor perspective it may not be proportionate to allow small services to harm until they become larger. Cumulative assessment\\t\\tThe burden is relatively light considering the risks. \"Overall, the measures are still the minimum required to address the worst kinds of \\nharm on the services; the burden is relatively light considering the risks, and this is why we have \\nrecommended extending certain requirements (such as having policies and training) to smaller \\nplatforms. \"\\nCumulative assessment\\t\\tIt is not disproportionate to require more transparency and precision. Large services have considerable resources and capacity to do more.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9928187727928162,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It is not disproportionate considering the scale of harm when safety mechanisms were voluntary. Content moderation (User to User)\\t\\tRight to communicate privately without interference is a critical element of freedom of expression and the right to privacy. End-to-end encryption and anonymity as “functionalities” posing particular risks - could lead to more risk-averse approaches to risk assessments and therefore content moderation, impacting freedom of expression. Content moderation (User to User)\\t\\tRight to communicate privately without interference is a critical element of freedom of expression and the right to privacy. End-to-end encryption and anonymity as “functionalities” posing particular risks - could lead to more risk-averse approaches to risk assessments and therefore content moderation, impacting freedom of expression. Governance and accountability\\tSegmentation\\tThere is a fundamental difference between public interest platforms and commercial ad-revenue-driven platforms, which – even if the user numbers were the same – pose very different risks to users. Data collection practices are also the foundation for the development of recommender systems that are listed as a relevant functionality for the determination of risk in Volume 2. Attending only to recommender systems but not to one of their essential components is a missed opportunity to establish the causal link between business models and risks of online harms. Governance and accountability\\tSegmentation\\tMore challenging for smaller companies to be able to find or designate an individual willing to take on criminal liability for compliance with these duties. More challenging for smaller companies to be able to find or designate an individual willing to take on criminal liability for compliance with these duties. Likely to further increase dominance by a small number of very large platforms over the UK market, stifling startups and innovation.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.8345592021942139,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'E.g. Unclear how OSA responsibilities will apply to platforms like Wikipedia wh rely on a global community of volunteer moderators\\nGovernance and accountability\\tSegmentation\\tWelcomes imposing more requirements on large or multi-risk platforms only, and the rationale of not applying the same requirements for U2U and search services though it believes obligation on small services is still substantial  \\tSmaller services may be incentivised to take a conservative approach and implement automated forms of compliance that can result in negative impacts on freedom of expression due to stringent compliance measures and associated costs. Requests Ofcom to narrow and tailor obligations, giving particular attention to the implications for public interest platforms, community-led moderation approaches, and smaller niche businesses. Content moderation (User to User)\\t\\tWe welcome clear and easily understandable guidance for services on setting and implementing content policies and appeals processes. values Ofcom’s flexible approach, which recognises that different services will rely on different forms of moderation which can include a combination of both human review and automated systems - rather than proscribing a strict approach. Content moderation (User to User)\\t\\twelcome the language in terms of requiring the “awareness” of services for taking down content, and appreciate that there is no general requirement for services to have dedicated reporting mechanisms (except Fraud)\\tcreation of dedicated reporting mechanisms and well-trained flaggers would be an additional compliance burden for services. The inclusion of enforcement agencies as trusted flaggers ( for fraud ) can be problematic,as it creates a privileged channel for government control of content ( see link in footnote 15) \\nContent moderation (User to User)\\t\\tProvided for recommendation for best practices \\t\"Moderators should receive \\n- Training on how to interpret and consistently apply for T&Cs\\n- Training on how to escalate contentious cases \\n- decent pay and support for psychological wellbeing \"\\nContent moderation (User to User)\\t\\tImpact on vulnerable and marginalised communities and encourages Ofcom to explore differential impact for such communities via research etc\\tEmphasise the disproportionate impact on vulnerable and marginalised communities who are at a higher risk of over-enforcement  and having their content removed for allegedly violating policies. For example, research has found that it can be difficult for both human moderators and automated moderation systems to understand the nuances in content relating to activism and counter-speech,  which can lead to the censorship of marginalised voices, impacting rights to freedom of expression and access to information. Refer to footnotes 17 and 21  in the response \\nAutomated content moderation (User to User)\\t\\tAgree with restricting the use of CSAM hash matching to publicly communicated content only and urges Ofcom to retain a narrow scope for the use of perceptual hash matching. Suggest clarity on what would be classed as a ‘private communication’ in annex 9 \\tAutomated systems using hashing technology to detect known CSAM imagery are relatively reliable and can be deployed at scale. Such systems should still be regularly reviewed and assessed for accuracy and impact on users.': [{'label': 'POSITIVE',\n",
       "               'score': 0.8718149662017822,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'However, the risk of false positivies can be heightend if used in private communications. Urges Ofcom to be wary of scope creep and the requirement of platforms to deploy hash matching to other forms of illegal content, as this would create more room for error and risk the widespread removal of legal content. Affected users should always be informed when a decision that affects them is made by automated systems, and should always be allowed to request a human review of the decision. Platforms can improve accuracy of ACM by ensuring that databases containing known illegal content, which are scanned using a hashing algorithm, are verified by a trustworthy and independent party or ensuring that hashing systems can still flag known illegal images that have been cosmetically altered. Automated content moderation (User to User)\\t\\tAgree with restricting the use of CSAM hash matching to publicly communicated content only and urges Ofcom to retain a narrow scope for the use of perceptual hash matching. Suggest clarity on what would be classed as a ‘private communication’ in annex 9 \\t\"Automated systems using hashing technology to detect known CSAM imagery are relatively reliable and can be deployed at scale. Such systems should still be regularly reviewed and assessed for accuracy and impact on users.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9931334853172302,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'However, the risk of false positivies can be heightend if used in private communications. Urges Ofcom to be wary of scope creep and the requirement of platforms to deploy hash matching to other forms of illegal content, as this would create more room for error and risk the widespread removal of legal content. Affected users should always be informed when a decision that affects them is made by automated systems, and should always be allowed to request a human review of the decision. Platforms can improve accuracy of ACM by ensuring that databases containing known illegal content, which are scanned using a hashing algorithm, are verified by a trustworthy and independent party or ensuring that hashing systems can still flag known illegal images that have been cosmetically altered.\"\\nAutomated content moderation (User to User)\\t\\tAgree with restricting the use of CSAM hash matching to publicly communicated content only and urges Ofcom to retain a narrow scope for the use of perceptual hash matching. Suggest clarity on what would be classed as a ‘private communication’ in annex 9 \\t\"Automated systems using hashing technology to detect known CSAM imagery are relatively reliable and can be deployed at scale. Such systems should still be regularly reviewed and assessed for accuracy and impact on users.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933098554611206,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'However, the risk of false positivies can be heightend if used in private communications. Urges Ofcom to be wary of scope creep and the requirement of platforms to deploy hash matching to other forms of illegal content, as this would create more room for error and risk the widespread removal of legal content. Affected users should always be informed when a decision that affects them is made by automated systems, and should always be allowed to request a human review of the decision. Platforms can improve accuracy of ACM by ensuring that databases containing known illegal content, which are scanned using a hashing algorithm, are verified by a trustworthy and independent party or ensuring that hashing systems can still flag known illegal images that have been cosmetically altered.\"\\nUser reporting and complaints (U2U and search) \\t\\t\"Welcomes Ofcom’s guidance on the need for services to introduce robust reporting and complaints mechanisms. Any content  available to  non-registered users should have relevant reporting and complaints systems which are also available to registered users\"\\tproviders of online services should make reporting and complaint routes available for both users and non-users, given the fact that harmful content may impact a broad range of individuals who are not users of a particular online service, particularly those which are smaller or medium-sized. User reporting and complaints (U2U and search) \\t\\tSets out  steps to enhance the transparency, accessibility of R&C\\t\"Transparency: Clearly explain the complaint process stages and expected timelines to users. Notification: Inform the responsible party about the complaint, detailing the review process and potential outcomes.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.991904616355896,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Rebuttal Opportunity by accused party \\nJustification of decisions or actions taken based on terms of service violations. Human Review Option: Allow parties to request a human review if automated tools were used. Safeguards: Implement measures to prevent misuse or abuse of complaint systems. Data Handling  outlining how data resulting from complaints will be stored, assessed, and deleted, \\nClear Language: Use unambiguous, non-technical language that is understandable to average users. Multilingual Support: Translate instructions into all languages used by the online service, including minority languages and immigrant communities. Accessibility: Ensure compatibility with assistive technologies for users with disabilities. Predefined complaint categories and an open category for uncertain cases. Contextual Detail: Allowing users to add context \\nReceipt Confirmation: Confirm receipt of complaints, ideally with a reference number for easy follow-up. Complaint Copies: option to download or receive a copy of their complaint (with consent). Transparent Appeals: Design appeals mechanisms to be as clear, accessible, and transparent \\nAppeal Rights: Clearly inform affected parties of their right to appeal decisions \\nRegular reminders about reporting and complaints procedures (e.g., through pop-ups or notices). Prompt Users specifically in relatation to  suspicious content.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.735406219959259,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"\\nUser reporting and complaints (U2U and search) \\t\\tvulnerable users such as children may require education on the danhgers online and how to report and complaint. There are steps providers should consider to help children access and use such mechanisms effectively \\t\"Age-Appropriate Content: Develop child-friendly digital safety content during sign-up or periodically. Include games or quizzes to reinforce understanding. Simplified Complaint Mechanisms: Create straightforward reporting processes for underage users using clear language and visual aids. Adult Proxy Complaints: Enable responsible adults to make complaints on behalf of children when necessary.\"\\nTerms of service and Publicly Available Statements \\t\\tWelcomes the obligations in terms of services and guidance on how to organise the substance and present terms of service to make them clearer and accessible. Suggests enhancing ToS and PaS \\tHigh-Level Summaries of terms of service or policies, with options for more detailed information.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9480910301208496,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Clear Definitions of key terms with examples and thresholds for prohibited content. Publish lists of organizations or individuals whose content violates policies. Inform users about enforcement measures for content violations and repeat offenses.Clearly explain how user data will be used, including for complaints and appeals. Explain if public figures are treated differently in terms of enforcement.Clarify allowances for journalistic violations and assessment criteria.Provide reasonable notice of new policies or changes.Ensure users explicitly acknowledge terms beyond pop-up banners. Host ToS and PaS in a centralized location with clear signposting to different types of documents and information.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923700094223022,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Ensure easy access to ToS through interface design.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9710463285446167,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Implement search functionality within documents. Use unambiguous, non-technical language that average users can understand.Tailor language for children using the service, incorporating graphics, videos, or creative means.Translate terms into all languages used by the online service, including minority languages and immigrant communities.Translate terms into all languages used by the online service, including minority languages and immigrant communities. Terms of service and Publicly Available Statements \\t\\tWelcomes the obligations in terms of services and guidance on how to organise the substance and present terms of service to make them clearer and accessible.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.974050760269165,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Suggests enhancing ToS and PaS \\tHigh-Level Summaries of terms of service or policies, with options for more detailed information. Clear Definitions of key terms with examples and thresholds for prohibited content. Publish lists of organizations or individuals whose content violates policies. Inform users about enforcement measures for content violations and repeat offenses.Clearly explain how user data will be used, including for complaints and appeals. Explain if public figures are treated differently in terms of enforcement.Clarify allowances for journalistic violations and assessment criteria.Provide reasonable notice of new policies or changes.Ensure users explicitly acknowledge terms beyond pop-up banners. Host ToS and PaS in a centralized location with clear signposting to different types of documents and information.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922588467597961,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Implement search functionality within documents. Use unambiguous, non-technical language that average users can understand.Tailor language for children using the service, incorporating graphics, videos, or creative means.Translate terms into all languages used by the online service, including minority languages and immigrant communities.Translate terms into all languages used by the online service, including minority languages and immigrant communities. \" Default settings and user support (U2U)\"\\t\\tWhile awaiting additional proposals from Ofcom in relation to AA, responder reiterates concerns about potential adverse impacts on individuals’ human rights posed by these AA tech. \"While awaiting additional proposals from Ofcom in relation to AA, responder reiterates concerns about potential adverse impacts on individuals’ human rights posed by these AA tech. These are \\nPrivacy Risks: Sharing official IDs or biometric data poses privacy risks, even if not retained by the service provider. Malicious actors could exploit such exchanges. Anonymity Impact: Requiring ID uploads removes anonymity, affecting vulnerable or persecuted groups’ ability to access and share information fearlessly. Freedom of Expression: Demanding official IDs may hinder expression for vulnerable users lacking access due to financial constraints or other reasons. Machine Learning Errors: Age estimation tools, even with small errors, can limit freedom of expression by blocking access to information. Non-Discrimination Risks: Machine learning tools may be less accurate for certain racial groups or genders, impacting non-discrimination rights. Two-Tiered Internet: Age verification systems risk creating an internet divide and deterring legal content access for adults.\"\\n\" Default settings and user support (U2U)\"\\t\\tWhere required to use AA, providers must have certain things in place to protect users \\t\"Where required to use AA, providers must have certain things in place to protect users. These are:\\nData privacy standards: Ensure strict data privacy for users sharing personal IDs or sensitive biometric data.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923382997512817,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Avoid retaining such data beyond necessary age checks. Alternative verification: Offer alternative methods for age verification to users who cannot or choose not to share personal IDs or biometric data. Appeal process: Allow users to appeal age determinations made by automated tools. Provide alternative verification options if the tool’s decision is disputed. Human Rights assessment: Evaluate age verification measures for potential human rights impacts and biases. Address any identified issues before implementation. Transparency and Effectiveness: it is valuable for online service providers to collect and publish data on the effectiveness of age verification in preventing children from accessing harmful or illegal content. Also, monitor how underage users may circumvent age checks intentionally.\"\\nUser access to services (U2U) \\t\\tProviders must consider human rights impact of sanctions affecting user access, especially freedom of expression and association. Sanctions can take various forms, including content restrictions, warnings, removals, and user suspensions. These actions may be determined by human moderators or automated tools. Be cautious about imposing unwarranted sanctions, whether by automated tools or human moderators. These actions can disable or remove lawful content, affecting information exchange. Passing suspected illegal content to law enforcement poses privacy risks and should only be done when explicitly required by law. Human reviewers should make these decisions, and users must be notified. Assess repeat offenders using “three-strike rules.” Retaining user data and violative content may be necessary, even for anonymous complainants, risking privacy and personal data. Avoid disproportionate removals of specific groups due to inconsistent sanctions. Carefully evaluate human rights risks against potential harm. Consult experts on free expression, privacy, and affected rights. When automated tools lack certainty, involve human moderators. Additionally, allow for escalation to more experienced or specialist moderators when uncertainty arises. Cumulative Assessment \\t\\tSmaller services face substantial compliance requirements, potentially impacting freedom of expression. Given the implementation costs associated with compliance, smaller services may be incentivised to take a conservative approach and implement automated forms of compliance that can result in negative impacts on freedom of expression. While necessary for risk mitigation, these provisions can be expensive and discouraging. To address this, Ofcom should offer hands-on guidance and support to small businesses during the transition. Cumulative Assessment \\t\\tSmaller services face substantial compliance requirements, potentially impacting freedom of expression. Given the implementation costs associated with compliance, smaller services may be incentivised to take a conservative approach and implement automated forms of compliance that can result in negative impacts on freedom of expression. While necessary for risk mitigation, these provisions can be expensive and discouraging. To address this, Ofcom should offer hands-on guidance and support to small businesses during the transition. Governance and accountability\\t\\t\"Concerned about taking a personalised approach to the accountability mechanisms set for OSA enforcement. \"\\t\"Adopting a personalised approach creates incentives for the companies (and the human beings responsible for decision-making) to take an overly risk-averse approach to their responsibilities on illegal content and related government demands. Only used as a last resort. \"\\nApproach to the Codes\\t\\tEncouraged by Ofcom\\'s approach but with some reservations \\tEncouraged to see that Ofcom’s approach to developing its illegal content Codes of Conduct aligns with industry best practices although there is a risk that services will struggle with the absence of proper legal benchmarks in the guidance, meaning that they may take an overly risk-averse approach to compliance. Approach to the Codes\\t\\tEncouraged by Ofcom\\'s approach but with some reservations \\tEncouraged to see that Ofcom’s approach to developing its illegal content Codes of Conduct aligns with industry best practices although there is a risk that services will struggle with the absence of proper legal benchmarks in the guidance, meaning that they may take an overly risk-averse approach to compliance. Approach to the Codes\\t\\tEncouraged by Ofcom\\'s approach but with some reservations \\tEncouraged to see that Ofcom’s approach to developing its illegal content Codes of Conduct aligns with industry best practices although there is a risk that services will struggle with the absence of proper legal benchmarks in the guidance, meaning that they may take an overly risk-averse approach to compliance. Approach to the Codes\\t\\tEncouraged by Ofcom\\'s approach but with some reservations \\tEncouraged to see that Ofcom’s approach to developing its illegal content Codes of Conduct aligns with industry best practices although there is a risk that services will struggle with the absence of proper legal benchmarks in the guidance, meaning that they may take an overly risk-averse approach to compliance. Approach to the Codes\\t\\tEncouraged by Ofcom\\'s approach but with some reservations \\tEncouraged to see that Ofcom’s approach to developing its illegal content Codes of Conduct aligns with industry best practices although there is a risk that services will struggle with the absence of proper legal benchmarks in the guidance, meaning that they may take an overly risk-averse approach to compliance. Approach to the Codes\\t\\tGuidance and support should be offered to smaller services \\tRecommends that Ofcom works with smaller services to assist them in coming to grips with the guidance and developing and implementing the illegal content codes of practice guidance.may create an additional compliance burden fro smallservices \\nApproach to the Codes\\t\\tGuidance and support should be offered to smaller services \\tRecommends that Ofcom works with smaller services to assist them in coming to grips with the guidance and developing and implementing the illegal content codes of practice guidance.may create an additional compliance burden fro smallservices \\nApproach to the Codes\\t\\twelcome alignment with the EU regulations when determining the size of large platforms but notes that some large services such an non-profit services  may not have same level of resource \\tAs well as considering the numbers of users when determining the regulatory burden placed on a particular service, it is also important to consider how the type of business model may impact the level of risks - namely the difference between public interest platforms and non-profit services to commercial ad-revenue-driven platforms. By failing to acknowledge the different types of platforms when setting regulatory standards, there is a risk that not-for-profit and public interest platforms, which may not have the same level of resources as other large platforms, will struggle to ensure compliance and therefore be forced to exit the UK market\\nApproach to the Codes\\t\\twelcome alignment with the EU regulations when determining the size of large platforms but notes that some large services such an non-profit services  may not have same level of resource \\tAs well as considering the numbers of users when determining the regulatory burden placed on a particular service, it is also important to consider how the type of business model may impact the level of risks - namely the difference between public interest platforms and non-profit services to commercial ad-revenue-driven platforms. By failing to acknowledge the different types of platforms when setting regulatory standards, there is a risk that not-for-profit and public interest platforms, which may not have the same level of resources as other large platforms, will struggle to ensure compliance and therefore be forced to exit the UK market\\nApproach to the Codes\\t\\twelcome alignment with the EU regulations when determining the size of large platforms but notes that some large services such an non-profit services  may not have same level of resource \\tAs well as considering the numbers of users when determining the regulatory burden placed on a particular service, it is also important to consider how the type of business model may impact the level of risks - namely the difference between public interest platforms and non-profit services to commercial ad-revenue-driven platforms. By failing to acknowledge the different types of platforms when setting regulatory standards, there is a risk that not-for-profit and public interest platforms, which may not have the same level of resources as other large platforms, will struggle to ensure compliance and therefore be forced to exit the UK market\\nApproach to the Codes\\t\\twelcome alignment with the EU regulations when determining the size of large platforms but notes that some large services such an non-profit services  may not have same level of resource \\tAs well as considering the numbers of users when determining the regulatory burden placed on a particular service, it is also important to consider how the type of business model may impact the level of risks - namely the difference between public interest platforms and non-profit services to commercial ad-revenue-driven platforms. By failing to acknowledge the different types of platforms when setting regulatory standards, there is a risk that not-for-profit and public interest platforms, which may not have the same level of resources as other large platforms, will struggle to ensure compliance and therefore be forced to exit the UK market\\nApproach to the Codes\\t\\twelcome alignment with the EU regulations when determining the size of large platforms but notes that some large services such an non-profit services  may not have same level of resource \\tAs well as considering the numbers of users when determining the regulatory burden placed on a particular service, it is also important to consider how the type of business model may impact the level of risks - namely the difference between public interest platforms and non-profit services to commercial ad-revenue-driven platforms. By failing to acknowledge the different types of platforms when setting regulatory standards, there is a risk that not-for-profit and public interest platforms, which may not have the same level of resources as other large platforms, will struggle to ensure compliance and therefore be forced to exit the UK market\\nGovernance and accountability  \\t\\tSupports Ofcom\\'s proposals thought there are potential challenges and areas for refinement\\t\"Enhanced Safety: the proposals aim to make digital platforms safer for users, reducing the prevalence and impact of illegal content. Increased Transparency: The requirement for transparency reports and independent audits increases public and regulatory oversight, builds trust. Accountability at All Levels: Assigning responsibility to senior management ensures that online safety is prioritised at the highest levels within organisations. User Empowerment: Improved reporting mechanisms empower users \\nPotential challenges and areas for refinement\\nSmall and emerging platforms may face significant challenges in implementing these measures due to resource constraints. Measures to combat illegal content must be balanced with rights to Privacy and Freedom of Expression.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9919053316116333,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Clear guidelines and oversight mechanisms, as well as inclusive representation, can help. Aligning Ofcom\\'s governance and accountability measures with international standards and practices is crucial for global consistency .\"\\nGovernance and accountability  \\t\\tBroadly agrees and sets out reasons and factors to consider \\t\"Both user-to-user and search services have a vast reach and influence on the online ecosystem. Applying governance and accountability measures to these services can significantly impact reducing the spread of illegal content. Focusing on these services ensures that significant risks are addressed through comprehensive risk assessments, senior management accountability, and transparent reporting. Platforms have the resources and technical capabilities to implement such measures. While the agreement is founded on the rationale above, it\\'s crucial to consider the measures should be proportionate to the size and capacity of the service provider. Flexibility in how governance and accountability measures are applied and updated is essential \\nCollaboration with international regulators and adherence to global standards can enhance effectiveness \"\\nGovernance and accountability  \\t\\tAdditions to consider include:  Efficacy of independent verification; Costs of audits and resource allocation when preparing audits; and risks such as privacy, security and complaince focus \\t\"Third-party audits can provide an independent verification of the effectiveness to enhance trust. Audits could help identify gaps in current practices and encourage the adoption of industry best practices. Conducting third-party audits could impose additional financial burdens on providers, particularly smaller platforms with limited resources. Other costs could include dedicating staff time and resources to ensure compliance. There could be risks related to the handling of sensitive information during audits. Risk that services might focus on compliance for the sake of passing audits rather than fostering onlince safety. \"\\nApproach to the Codes\\t\\tagrees and provides additional comments \\t\"Risk-based approach aligns with best practice and encourages providers to proactively identify, assess, and mitigate risks of illegal content. Propsal to engagebroad range of  stakeholders is commendable as it ensures that the Codes are informed by diverse perspectives, effective and applicable. Updating Codes in line with technological advancements and emerging challenges allows for flexibility. Imposing obligations on service providers  in line with with the size of the service and the nature and severity of the risks involved ensure small platforms are not unduly burdened. Clarity for providers ensures that they can effectively implement the meaures \\npublicly available and easily understandable Codes can help raise awareness among users about the standards they should expect. Technology-neutral stance, aims not to stifle innovation among service providers.\"\\nApproach to the Codes\\tDefinition of multi-risk services\\tAgree with the definition but makes additional comments\\t\"Ofcom should provide clear guidelines and examples to help services. Support for Compliance particularly for smaller or emerging services.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9971935153007507,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Definition and categorization criteria should be regularly reviewed and updated. \"\\nApproach to the Codes\\t\\tMakes comments on flexibility; clarity and assessiblilty; balance between safety and rights;  support for compliance, engagment and transparency and enforcement and oversight. \"Recommends that Codes are adaptable to technological advancements and emerging threats. Alos recommends providing support, resources, and potentially a grace period for compliance, especially for small and emerging platforms. To be effective, Codes must be clearly written and easily accessible -Practical examples, case studies, and clear guidelines would be beneficial. The Codes must strike a balance between online safety and protecting users\\' rights to privacy and freedom of expression. Commends consultation process and breath of stakeholder enegagment. Clarity, fairess and consistency of enforcement and appelas processes will impact eefectiveness of the Codes.\"\\nApproach to the Codes\\tCosts\\tThis topic would require further study under a specific outline for us to have a better understanding.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9717265367507935,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This topic would require further study under a specific outline for us to have a better understanding. Content moderation (User to User) \\t\\tBroadly agrees with proposal for content mod (U2U) because of effectiveness and balance of user rights. Provides considerations and recomendations to support smaller platforms, oversight and appeals process and recommends that the Codes should be subject to continuous review and adaptation.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7282922863960266,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Emphasis on a risk-based approach allows platforms to allocate resources more effectively, targeting areas of highest risk. Proposed measures more feasinle for large platforms than smaller ones. The requirement for transparency and user reporting mechanisms supports  balance safety vs user rights - providing users with avenues to challenge decisions. The proposal is comprehensive and aligned with the goals of OSA but additional support for smaller platforms, possibly in the form of technology-sharing agreements, guidance, or financial assistance would ensure wider compliance. Recommends that proposal should clearly outline processes for oversight and appeals and Codes be subject to regular review and adaptation. Content moderation (Search)\\t\\tBroadly agrees with proposal and provides reasons, considerations and recommendations\\t\"Agrees with  proposal to reduce the visibility of illegal content on search services. Believes major search engines have the resources and technology to implement these measures effectively and proactively. However,acknowledges the challenges and costs that smaller search engines may face in complying. Emphasizes the importance of respecting and protecting the freedom of expression of users, and the need for transparency and accountability in content filtering processes. Suggest that to improve the proposals smaller search engines should receive technological support and guidelines to help them meet the content moderation standards. Content assessment should follow clear and consistent guidelines that distinguish between illegal and harmful content, and respect the freedom of expression of users. Content moderation practices should be transparent and accountable, and allow users to appeal against unjust decisions and should be regularly reviewed and adapted to reflect the changing online environment and search technologies.\"\\nAutomated content moderation (User to User) \\tPublic/private\\tsets out few key considerations for ACM proposals  \\tAutomated moderation can be fast and scalable, but also complex and error-prone. It needs transparency, accountability, ethics, and bias reduction.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.7180685997009277,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Automated content moderation (User to User) \\tCosts\\tSets out several consideration regarding accuracy. Adding thqat perceptual hash matching is a fast and scalable way to detect known CSAM, but it has limitations in identifying new or altered CSAM, and it can produce errors that need verification and  considers  costs and barriers for smaller services. \"Perceptual hash matching is widely used because it can scan and moderate large volumes of content quickly and efficiently. However, it also has some accuracy challenges that need to be addressed. For example, it cannot detect new CSAM that has not been reported or hashed before, and it can be fooled by modified versions of known CSAM. It can also make mistakes by flagging or missing some content that should or should not be moderated. To improve the accuracy and reliability of perceptual hash matching, proposals should consider the use of advanced cryptographic protocols and technologies that can reduce errors and allow public verification. Additionally, proposals should ensure that the use of perceptual hash matching is transparent and accountable, and that users have the right to appeal moderation decisions. Smaller services face significant costs and barriers  which can affect their service quality and competitiveness. They can overcome this by seeking partnerships and support from larger platforms or industry coalitions, to gain access to shared hash databases and technical assistance and leverage available resources and tools from organisations that are dedicated to fighting online child exploitation, such as Thorn or the Tech Coalition. Furthermore, smaller services can advocate for scalable and cost-effective solutions for CSAM detection that are accessible to platforms of all sizes. \"\\nAutomated content moderation (User to User) \\t\\tAccessing hash databases for CSAM detection involves collaboration, compliance, capability, cost, and ethics, and poses challenges for smaller services. \"Set out the following considerations and challenges for online services to access and use hash databases for detecting CSAM. Access to hash databases is often enabled by collaborating with child safety organisations and law enforcement agencies, which requires meeting certain security and operational standards e.g. includes data protection measures, secure access protocols, or  security audits.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9921209812164307,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Using hash databases also requires technical capability, financial resources, and legal and ethical responsibility to handle the sensitive data and balance content moderation with user privacy. Smaller services might face more difficulties in accessing and using hash databases due to their limited resources, technical integration, and security and privacy challenges.\"\\nAutomated content moderation (User to User) \\tCosts\\tOnline services can choose from different CSAM URL detection providers and estimate their service load and costs, while also considering Spectra as an option\\tThere are various organisations offering these services ranging from Cloudflare to Amazon. An approximate service load would need to be determined to assess pricing with the many potential vendors. One of the many CSAM URL detection solutions could be contracted in parallel with a Spectra agreement if desired. Automated content moderation (User to User) \\tCosts\\t Further study and understanding would be necessary. The specific hashing service would need to be further evaluated for scaling capacity and cost impacts at scale. Further study and understanding would be necessary. Content moderation (Search)\\t\\tSets out general principles and proposals for using automated systems to moderate illegal content on search services, while considering the technological, ethical, and user rights aspects. \"Outlines four general principles for automated content moderation in search services: effectiveness, balancing accuracy and minimiasation of overblocking, transparency about use of ACM, and adaptability and continuous improvments. It also expressed agreement with the proposals noting that it offers a framework that is flexible and supportive of small services, while highlighting some key considerations such as technological feasibility, ethical use, support for diverse platforms, and protection of user rights. \"\\nUser reporting and complaints (U2U and search) \\t\\tAgrees with proposals for user reporting and complaints mechanisms, while highlighting their strengths, critiques, and recommendations for improving user safety and trust online\\t\"Set out strenghts and weaknesses of user reporting. User reporting and complaints mechanisms can empower users, increase responsiveness to community concerns, and enhance transparency and trust between users and platforms. However, these mechanisms also face challenges such as clarity and accessibility of R&C tools, response times and feedback loops, over-reliance on automated systems, potential for abuse, and consistency across platforms. To address these challenges, platforms should invest in user education and awareness, ongoing evaluation and improvement, and a balanced approach to automation.\"\\nTerms of service and Publicly Available Statements\\t\\tExpressed broad agreement with the proposal for child online safety, highlighting its strengths and suggesting some areas for improvement. \"Agrees with the proposal and highlights its key strengths, such as proactive protection - notes that emphasis on safe default settings for child users is commendable aligning with\\' safety by design\\' principle; user support and education - essential for fostering safety online; and age-appropriate experiences. Raised some considerations and recommendations for improving the proposal, such as addressing age verification challenges, balancing safety with autonomy, engaging with parents and guardians, supporting smaller U2U platforms with regards to technical and financial challenges , and continuously evaluating and adapting the measures.\"\\nRecommender system testing (U2U) \\t\\tAgrees with the proposals for recommender system testing, highlighting their benefits for online safety, transparency, and innovation, and suggests some considerations and recommendations for improving their feasibility, methodology, user control, and ethics. \"Sets out key elements of recommender system testing proposals, which include transparency, accountability of services, incorporating risk assessments specific to recommender systems, regular testing and evaluation to minimise risk of harm. Respondent agrees with the proposals and highlights the benefits of a proactive approach to online safety; the necessity for transpareny - understanding how decisions are made by these systems is foundational for assessing risks and ensuring that content recommendations do not undermine user safety; and the balance between innovation and safety. The response also provides some considerations and recommendations, such as feasibility for smaller platforms, methodological transparency, user control and preferences, and ethical considerations and bias.\"\\nRecommender system testing (U2U) \\t\\t\" Suggests various methods for smaller services to test and improve their recommender systems for safety, such as off-platform simulation, third-party audits, user feedback, crowdsourcing, Spectra, and lightweight tools. \"\\t\"Sets out some methods for smaller services to test and improve their recommender systems for safety, such as:\\n-Simulating real-user interactions to help understand how certain types of content are promoted or demoted without affecting the live platform. -Partnering with third-party experts or industry collaboratives\\n-Gathering feedback from users about their experiences with recommended content or using  crowdsourced evaluators based on specific safety criteria\\n-Using tools like Spectra for analysis and comparison\\n-Integrating lightweight analytical tools\"\\nEnhanced user control (U2U) \\t\\tAgrees with the proposals and sets out the key elements and considerations of enhanced user control proposals for online safety and experience\\t\"Sets out the key elements of enhanced user control proposals for online platforms, such as: Customisable content filters to help users limit exposure to certain types of content; \\nPrivacy settings management, allowing users to control who can see their posts, interact with them; and Tools for managing interactions such as blocking, muting, or reporting. There is agreement with these proposals, as they empower users and prevent harm by giving them more control over their online experience. Provides suggestion of some considerations and recommendations to ensure the effectiveness of these proposals, such as user-friendly and assessible design (includes clear language, intuitive design, and easily navigable settings), education and awareness, balancing providing control to users with maintaining a positive user experience, and ongoing evaluation and feedback. \"\\nEnhanced user control (U2U) \\t\\tExplains the importance of making controls known to users such as user awareness, digital literacy, promoting safety features and makes recomendatiion for implementaion \\t\"Explains that users need to be aware of the enhanced user controls that platforms offer; digital literacy can bridge the gap, ensuring all users, regardless of their technical proficiency, can benefitfrom clear and accessible info about ehnanced  user controls. Suggests some ways to make these controls more user-friendly and accessible, such as intuitive design and prominent placement within the user interface, making them easy to find and adjust according to the user\\'s preference; Educational resources, Onboarding process, and Regular updates and reminders.\"\\nUser access to services (U2U) \\t\\tAgrees with proposals in relation to protection of vunerable users and balancing safety with access. Sets out key elements of User Access to services such as age verfification  and parental Controls and user privacy and data protection. It also sets out considerations and  recommendations. \"Sets out the key elements of the user access to service proposals as:\\nImplement age verification and parental controls for young users. Ensure accessibility and inclusivity in digital services. Balance user privacy and data protection with protective measures. Agrees with proposals that protecting vulnerable users, especially children, is crucial for a safe online environment. Also agrees that the aim is to balance user safety with maintaining open access to digital services. Considerations and Recommendations:\\nTechnological and Operational Feasibility:  Support mechanisms could help address challenges smaller platforms may face in implementing access controls. Risk of Overblocking: Restrictive measures could overblock legitimate content or restrict access for certain user groups. Continuous monitoring and appeal mechanisms are important.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9824490547180176,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Transparency and User Education: Platforms should be transparent about access controls and invest in user education about online risks. Collaboration and Standardisation: Collaboration to develop standardised approaches to age verification and access controls could enhance their effectiveness across the digital ecosystem.\"\\nUser access to services (U2U) \\t\\tLists various user blocking methods, their workings and limitations. It sets out recommendations for effective implementation which includes a layered approach, user appeal process, privacy, ethics, and continuous monitoring and adaptation.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9967371821403503,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"Sets out explanation of Username and Email Blocking, IP Address blocking, Device Behavioral Analysis and Machine Learning Fingerprinting, Social Authentication, Biometric Verification and how they work as well as their limitations.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9907869696617126,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Sets out recomendations of:\\nCcombining multiple methods in a layered approach to enhanced effectivness; \\nPutting in place continuous montioring of technological advancments and and continuously adapting and refining strategies  to remain effective against circumvention attempts. \"\\nUser access to services (U2U) \\t\\tSets out how to implement a strikes system for user blocking, considering the aspects of graduated response, transparency, communication, temporary limitations, contextual and behavioral analysis, appeals process, and user education. \"Sets out the follwoing : \\nA strikes system introduces a graduated response to violations, escalating consequences with repeated offences. This system allows users to correct their behaviour before facing severe penalties. Transparency and communication about the strikes system and the possibility of temporary limitations on account functionalities can help manage risks. services can impose temporary limitations on account functionalities before resorting to full blocking. Contextual and Behavioral Analysis is crutial when assessing actions that may lead to strikes or blocks. Services can employ behavioural analysis techniques to identify patterns consistent with previously blocked users, allowing for early intervention. A clear and accessible process for users to appeal strikes or blocks can help ensure fair treatment. User education initiatives can inform users about safe online conduct and the potential consequences of policy violations. Leveraging analytical tools (like Spectra) can provide insights into user behaviour and content spread patterns, aiding in the identification of potentially harmful behaviour and the effectiveness of strikes or blocks. \"\\nService design and user support (Search) \\t\\tappears to agree with the proposals  setting out how it views the out specific proposed measures  and the benefits based on risk-based approach, service-specific measures, risk assessment, user reporting, transparency, technological solutions, collaboration, and flexibilityof the measures\\t\"Sets out that the codes emphasis Ofcom\\'s commitment to a risk based and proportionate approach  and provides guidance on the expectations for regulated services in managing and mitigating illegal content. It delineates measures tailored to different types of services with emphasis on systems and processes for identifying and removing illegal content and for search services, on preventing the retrieval of illegal content. Sets out that Ofcom\\'s proposals highlights the importance of these assessments in informing the measures a service takes. Highlights the clarity and accessibility of the proposed 5\\tUser reporting and complaint handling measures. Also highlights the need for services to be transparent about their policies and practices related to illegal content management , balancing Auto content Mod with Human oversight. Encourages collaboration and sharing of best practices; and recognising the flexibility and evolution of measures. \"\\nAutomated content moderation (User to User)\\tPerceptual hashing\\tPerceptual hashes can be fooled. It has been observed by professionals that perceptual hashes can be fooled. False negatives are the main problem related to perceptual hashes. Experts suggest that this actually happens quite a lot as it isnt hard to manipulate an image so that its perceptual hash appears to be different from the image containing harmful content. Moreover, a high percentage of CSAM images and videos cannot be recognised by perceptual hashing techniques as there is no prior known image. It can also be easy to fool the algorithm to evade detection and frame somebody innocent. Automated content moderation (User to User)\\t\\tIP.rec suggest to; detect and block URLs already related to terrorist content; favour a service culture that encourages user reports; invest more into human moderation. Three recommendations: 1. Detecting, blocking and reporting URLs already identified as related to terrorist content is effective for combating terrorist organisations that operate through multiple platforms while presenting less risk of errors that could threaten freedom of expression. 2.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9881380200386047,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Providers should favour a design and service culture that encourages reports made by users. 3. Human moderation activities should recieve greater attention an investment so that; no measure that could impact users freedom of expression and privacy is taken without human supervision; measures are taken quickly, especially for live broadcasts of terrorist attacks. Approach to the Codes\\tMulti-risk\\tConcerned the definition of multi-risk is too large.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9201889634132385,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Concerned the definition of multi-risk is too large. Approach to the Codes\\t\\tAgrees with them as a whole\\tAgrees with them as a whole\\nApproach to the Codes\\tSegmentation\\tAgrees with the segmentation of the codes\\tAgrees with the segmentation of the codes\\nApproach to the Codes\\tLarge Services\\tAgrees with the definition\\tAgrees with the definition\\nApproach to the Codes\\tMulti-risk\\tDoesn\\'t agree with the definition\\tDoesn\\'t agree with the definition\\nApproach to the Codes\\t\\tAgrees with U2U but thinks search codes are too wide ranging. Agrees with U2U but thinks search codes are too wide ranging. Content moderation (User to User) \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nContent moderation (Search)\\t\\tDoesn\\'t agree with the proposals, sees them as too broad\\tDoesn\\'t agree with the proposals, sees them as too broad\\nAutomated content moderation (Search) \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nUser reporting and complaints (U2U and search) \\t\\tAgrees with the proposals if they are practial to implement\\tAgrees with the proposals if they are practial to implement\\nTerms of service and Publicly Available Statements\\t\\tAgrees with the proposals\\tAgrees with the proposals\\n\" Default settings and user support (U2U)\"\\t\\tAgrees with the proposals\\tAgrees with the proposals\\nRecommender system testing (U2U) \\t\\tDoesn\\'t agree to the proposals on the ground they\\'re too complex\\tDoesn\\'t agree to the proposals on the ground they\\'re too complex\\nEnhanced user control (U2U) \\t\\tDoesn\\'t agree on the grounds that the functionalities of blocking/disabling comments are not appropriate for all services\\tDoesn\\'t agree on the grounds that the functionalities of blocking/disabling comments are not appropriate for all services\\nUser access to services (U2U) \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nService design and user support (Search) \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nCumulative Assessment  \\t\\tDoesn\\'t agree with the proposals, sees them as excessive\\tDoesn\\'t agree with the proposals, sees them as excessive\\nCumulative Assessment  \\t\\tDoesn\\'t find it to be proportionate as too many businesses will be covered\\tDoesn\\'t find it to be proportionate as too many businesses will be covered\\nCumulative Assessment  \\t\\tAgrees the burden on large services is proportionate\\tAgrees the burden on large services is proportionate\\nStatutory Tests \\t\\tDoesn\\'t agree as the measures are not proportionate to the risk of harm presented by small services\\tDoesn\\'t agree as the measures are not proportionate to the risk of harm presented by small services\\nApproach to the Codes\\tSegmentation\\tAgrees that only the most onerous measures should be for large/high-risk services\\tAgrees that only the most onerous measures should be for large/high-risk services\\nApproach to the Codes\\tLarge services\\tAgrees with the definition of large service but doesn\\'t find it in line with small services\\tAgrees with the definition of large services but not those for small services. Adds that there should be a category below small for those that shouldn\\'t be considered under the regime due to their size. Content moderation (User to User) \\t\\tQuestions how formal the moderation needs to be. Speaks from personal experience of moderation on smaller services\\tSpeaking from person experience, they haven\\'t seen illegal content on very small, low-risk services and that the services moderation works. For example, every first post of any new member must be reviewed.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9911363124847412,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Whilst there may be a few who try to scam people, they are often easily spotted due to the moderation already working. User reporting and complaints (U2U and search) \\t\\tDoesn\\'t agree or disagree but states what they are currently doing\\tReport links are already built into the service they moderate for which are flagged to moderators with this system being built into the forum software. Terms of service and Publicly Available Statements\\t\\tRecognises the importance of formal ToS\\tRaises that it will be necessary to customise forum software to implement formal ToS. User access to services (U2U) \\t\\tDoesn\\'t agree or disagree but states what they are currently doing\\tThe service they run is willing to block users and has in the past. They have no commercial incentive to keep users. Users who post harmful cotnent have their account deleted and IP address and email blocked. Cumulative Assessment  \\t\\tDoesn\\'t agree with the proposals as they don\\'t see their service as a \\'businesses\\'\\tDoesn\\'t agree with the burden on micro businesses because there is no commcerical or profit incentive. They are concerned that if they are treated as a business, volunteers cannot sustain the resources necessary for the service to function\\nGovernance and accountability\\t\\tAsks for how senior management can be held accountable and how this process is managed\\tMost platforms have T&S responsibilities outside UK juridsdiction. Asks how can senior management be held accountable if they\\'re based outside of the UK? Asks how relevant training would be carried out with each platform taking their own approach - how would this be suitably managed? Asks how would carry out independent assurance checks and who would be allowed to carry this out? Questions if we can rely on internal audit mechanisms for platforms to check themselves\\nGovernance and accountability\\t\\tSays it should cover every kind of service\\tEvery \"internet\" enabled service should be covered, including gaming, dating, shopping etc. Although some will be smaller, it is imperative that all areas are covered, otherwise the gaps will encourage online harms to still occur\\nGovernance and accountability\\t\\tHash data and implementation is costly and not necessarily effective\\tHaving access to a CSEA hash set is a paid for service and certain criteria has to be established bfore access is granted. Many companies may not have the funds available to purchase the has data on top of the costs to incorporate it into their internal systems. Hash data is still only what is known to the authorities. There is a lack of appropriate, relevant, and effective sharing of information and intelligence across all sectors. Independent third parties can incur costs to train and have the relevant knowledge and expertise in each relevant online harm topic\\nGovernance and accountability\\t\\tAsks who would take overall responsibility when there are power struggles between different teams\\tWho would take the overall relevant responsibility when within platforms there is a struggle of power between Product, Policy, Operations, and Legal teams. Who would rightly have the final say and how would a failure be managed. Approach to the Codes\\t\\tQuestions what is meant by \"existing good practice\"\\tA phrase is stated \"existing good practice\" - however if there was currently good practice, there wouldn\\'t be so many illegal harms. Who has deemed a practice \\'good\\' and how was this identified? Approach to the Codes\\t\\tDoesn\\'t agree the most onerous measures should be on the larger/most risky services\\tThis should be fully across the whole spectrum of platforms and sites\\nApproach to the Codes\\t\\tAgrees with the definition but disagrees with the segmentation\\tAs a definition this would be acceptable, however, many illegal harms happen across a multitude of small to large platforms, so this would mean users may move from a large to a small platform, which could then in turn have a larger user base \\nApproach to the Codes\\t\\tDisagrees with the definition and it seems arbitrary\\tAn illegal harm should be viewed as it is, whether in conjunction with other harms or not. Any platform can have various risks associated for all sorts of reasons, even those that may seem \\'innocent\\'\\nApproach to the Codes\\t\\tSuggested it should be made easier to search the codes and demonstrate how they interact with the OSA\\tAs this is a highly comple and new set of codes, it should be clearer and easier to search the codes ands ee how they all interact with the OSA\\nApproach to the Codes\\t\\tSome of the figures for CSAM hash costs might be incorrect\\tWith regards to CSAM hash costs, some of the figures appear incorrect (taken from personal experience) and don\\'t appear to show \\'membership\\' fees to organisations that can supply the hash data. It also refers to \\'known\\' CSAM however, we also need to know about \\'unknown\\' and \\'new\\' CSAM. The other factor to consider is the detection of CSEM which is not currently being hashed. The figures for actionable content from NCMEC referrals isn\\'t clear to make a full judgement of costs about safeguarding. The figures are from 2022 and should be updated accoringdly if available. Content moderation (User to User) \\t\\tPerformance targets are not good for content moderators and instead there should be a focus on training\\tPerformance targets\\' is a bad idea for content moderators. These people are already under huge pressure with regards to timescales and content view. To add a \\'target\\' will enhance this pressure and could lead to damaging welfare and mental health implications. There should be a focus on training, rather than targets. Policies and processes are already in place on many platforms and they are clearly not as effective as they should and could be\\nContent moderation (Search)\\t\\tPerformance targets are not good for content moderators and instead there should be a focus on training\\tPerformance targets\\' is a bad idea for content moderators. These people are already under huge pressure with regards to timescales and content view. To add a \\'target\\' will enhance this pressure and could lead to damaging welfare and mental health implications. There should be a focus on training, rather than targets. If content is illegal, why would it be downranked - if it\\'s illegal it should be removed and reported. Automated content moderation (User to User) \\t\\tThere is a large cost and unexplained decisions around hash-matching. Critical of URL detection as well\\tHash-matching only works on \\'known\\' content, not \\'unknown\\'. Thre is a huge gap in how quickly an image can be hashed and shared around LEAs and platforms, therefore on one platform it could be hashed, and on another it isn\\'t and still in circulation. Purchasing and installing the technology for hash-matching is costly. There are also multiple hash datasets so who then decides which are the best to use? URL detection is time consuming for platforms, LEAs, and other agencies. Offenders are aware of detection capabilities so they will edit and alter URLs to disguise it. This further adds costs and resources to identifying illegal URLs\\nContent moderation (User to User) \\tEvidence on hash-matching and URL detection\\tHash-matching is useful however there are costs and uncertainties. Emphasis is placed on other ways to detect CSEA\\tThere is an issue with hash-matching around the \\'known\\' and \\'unknown\\' content. The figures used aren\\'t fully representative of reported content either as reports canbe received from multiple sources, not just NCMEC. It would be useful to know the percentage of the reports that are being investiagted and leading to successful outcomes. The cost of using hash-matching and the training required isn\\'t viable for smaller services. Enhancing knowledge and training to smaller services would be the cheaper and a still somewhat effective option. It would be worth considering supplying hash data for free, potentially funded by the government. Hash-matching is only a single part of CSEA detection as URL and keyword detection can also be used. A more centralised, information sharing approach would be more advantageous as it will be cheaper and resource effective, especially for smaller services.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9922417402267456,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Automated content moderation (Search) \\t\\tAgrees with the measures but thinks they won't work well in practice. A coordinated approach would be needed to ensure the measures work well\\tThe proposals in theory are good, the reality is different. How many sources would be supplying identified URLs, how quikcly, how time intensive will it be keeping it updated. Who is checking the URLs, who are the experts supplying the lists?\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923781156539917,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Unless there is a coordinated approach, then this is an impossible request for companies to achieve. User reporting and complaints (U2U and search) \\t\\tStresses that there should be consistency and ease of reporting and making complaints, asking what options do users have if they disgaree with a decision made by the service\\tThere needs to be consistency and ease of reporting and making complaints. Users want a response in timely manner and what happens if users disagree with a services decision? Who and how can a user contact the platform about this? If a paltform doesn\\'t overturn an action and a user requires a super complaint, how long will this process take? Terms of service and Publicly Available Statements\\t\\tAgrees with the proposals but this doesn\\'t stop users posting and sharing illegal content. Most platforms have ToS policies easily available and they do state what is/isn\\'t allowed on their platforms. However, users still post, share, and distribute illegal content. Certain platforms now are blocking users if they search for certain banned words. Terms of service and Publicly Available Statements\\t\\tSuggests banning keywords and adding direct links to policies in home pages \\tBanning those that search keywords is a good start. Platforms should have direct links to policies from their \\'home\\' pages to make them more accessible. \" Default settings and user support (U2U)\"\\t\\tHaving default settings for under 18s may have adverse affects and push them onto less regulated services\\tEvidence suggests that those aged 15-17 dislike being seen or treated as children, and will therefore find ways around any age verification/assurance. There is a need to understand how these young people use certain social media - for those in a school environment these actions and would be detrimental, and potentially push young users onto other platforms that aren\\'t following Ofcom\\'s regulations. Young people can search ways around certain setting as well. \" Default settings and user support (U2U)\"\\t\\tIt should be clear about what action will be taken is a user is under 18 and searching for illegal content\\tUnder 18s are more than likely to be aware of what is illegal content and potentially are intent in finding it (for example vapes and drugs). It should be made clear about what action will be taken (such as warning notice or a pop-up) is an under 18 user is searching for illegal content. Recommender system testing (U2U) \\t\\tAgrees with the proposals but raises services need to be in agreement with decisions\\tYes however, all paltforms would have to share relevant information and agree on content to be removed or blocked. If one platform allows one type of content, but another disallows, then the content is still available. Recommender system testing (U2U) \\t\\tAppropriate training would be suitable for smaller companies, followed by an audit check\\tRelevant and appropriate training by approved professionals would be suitable for smaller companies, with an audit check for compliance. Recommender system testing (U2U) \\t\\tSuggested the use of IP blocking, chat bots, and blur technology alongside information sharing \\tIP blocking has been used by platforms whena  user has violated its policies. Chat bots can be used to pop-up if a user is searching for illegal content and advise them accordingly. Blurring technology can be used for 18+ content until a user confirms their age. Information sharing across platforms with up-to-date information on trends and patterns of illegal harms can help change how design parameters are introduced. Enhanced user control (U2U) \\t\\tBlocking is a good functionality however it doesn\\'t stop offending users to create a new account \\tBlocking offending users is a good initial safety feature however, it is known that some offending users will just set up new accounts and continue the abuse in whatever form. It will also depend on the type of harm being caused and the reason why it is hapening. Need to consider how to stop unwanted attention after initial blocking has been activated. Enhanced user control (U2U) \\t\\tThe suggest \\'block\\' option should be made clear and obvious\\tIn theory yes, but there are already instructions in safety help centres on how to activate \\'blocking\\' and other options. The suggest \\'block\\' option should be made clearer and more obvious. Enhanced user control (U2U) \\t\\tSome users may try and become verified to avoid consequences or services may be inclined to take less action against verified users\\tVerification has bene proven to have an element of risk, as some users will attempt to become \\'verified\\' to avoid certain repercussions. Also, certain platforms will not take action on verified accounts even thought they might be offending in some way \\nUser access to services (U2U) \\t\\tBlocking a user is possible but not enforceable on a large scale. There should be considerations of where offenders will go after losing access to one platform\\tIt is highly possible to block a user using various options such as IP, phone number, e-mail, and on some platforms this is already undertaken, but not on large scale enforcement. It is also possible to identify patterns of re-occuring offenders using different style of user names. Consideration also needs to be given to pushing offenders onto other platforms – could certain information be shared across platforms? If offending users are only be looked at for the most serious harms, then any impact should be seen as positive. User access to services (U2U) \\t\\tSuggests there should be an option for a time length of a block depending on the circumstances, \\tThis is a valuable recommendation, and 1 with serious merit. I’d highly suggest there should be an option for a time length of a block depending on the circumstances. User access to services (U2U) \\t\\tSuggests other options such as leaving accounts open, given warnings for users to remove content, and having clear and easy appeals systems in place\\tIf, as Ofcom state, hash data is accurate, then no content should be erroneously identified. The specific point is what is being classed as CSAM, as in some cases, legal content can be deemed as CSAM (anime, innocent family images). Certain platforms already remove content that might be deemed CSEM, but leave the account ‘open’, this is one possible solution. Warning notes are also passed to user requesting removal of content, and if they don’t oblige, then a block is added until they do. The other consideration is how to stop users just setting up new accounts if they have been blocked.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.992432713508606,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"This is sometimes easier than waiting for an appeal against a violation or if they have been suspended. Service design and user support (Search) \\t\\tFurther clarification is needed when it comes to CSAM or CSEA related content. Once users realise certain words are 'flagged', they will turn to different methods\\tSome platforms already utilise such tooling. However, it has to be carefully considered what type of words would and could be deemed as harmful or those used to search for CSAM content. Clarification needs to be sought if only for CSAM or CSEA related content. Once users realise certain wording is ‘flagged’, they will use different methods, so it would be imperative to keep this information shared and updated. Cumulative Assessment  \\t\\tLow risk and small services should have adapted costs or providing cheaper alternatives\\tCosts for low risk small and micro business can be adapted by different approaches then those stated. For instance, improved training for staff on certain topics, simple and easy reporting and better understanding of the OSA and Ofcom reg’s. These would be cheaper alternatives for certain businesses. Cumulative Assessment  \\t\\tGood training can be a cheaper and potentially more effective measure alongside a pre-audit check and re-audit within a year\\tWith enhanced training and knowledge, which could be cheaper and more effective, any business that is at significant risk could minimise any potential risks occurring. This could also include a pre audit check to allow them to review current measures and improve, with a re-audit within a year before Ofcom take any further action. Cumulative Assessment  \\t\\tBurden on large services is proportionate but it's worth considering that larger services may have more challenging and complex issues to deal with \\tYes, however, although large services have more users and more financial backing, this does also come with more complex handling of these issues. Due to the amount of users and content, and even with use of technology, it can be more challenging.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.988871157169342,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Have access to resources to one thing, have access to the right resources and information is another. Statutory Tests \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nApproach to the Codes\\t\\tCats Protection agrees with the use of 'good practice' but emphasises that specialist knowledge should also be considered. Good practice in industry is useful to set clear expections on raising standards. However, service providers may require access to expert knowledge (such as veterinary in this case). Looking at practices of other types of online selling could lack relevance to animal welfare, potentially leaving users at risk of fraud if there is no specialist knowledge a service's 'good practice' is based on. Content moderation (User to User) \\t\\tCats Protection agrees with the U2U content moderation codes although doesn't think ACM measures should be used  for pet fraud. Cats Protection agrees with the U2U content moderation codes. In terms of training, it would require materials about breeding, age of cats, health checks etc. ACM on the other hand cannot verify the indentity of breeders. Given the complexity of idenityfing pet fraud or mis-selling, automated moderation cannot be fully relied upon - there will need to be human intervention for identifying the welfare and consumer risks. User reporting and complaints (U2U and search) \\t\\tCats Protection agrees with the user reporting and complaints measures, stressing their importance in the work they currently undertake. User reporting is an important part of the work Cats Protection and other members of PAAG conducts. It's been vital in helping services remove the content from their websites.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9891340136528015,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Having an accessible and responsive user reporting and complaints process is important in the types of fraud that occur through pet sales as other methods (such as keyword filters) can be harder to apply. Terms of service and Publicly Available Statements\\t\\tCats Protection agree that ToS need to be easy to find as they may ban certain types of sales\\tCats Protection agree that ToS need to be easy to find as they may ban certain types of sales\\n\\tGovernance and accountability\\tCELE want to ensure that human rights law and freedom of expression are considered when making proportionate codes\\tSuggests that enforcement of the 'risk centred-approach' takes human rights law and freedom of expression seriously. Risk mitigation measures of harmful content by platforms are legal, necessary, and proportionate.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9528880715370178,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Governance and accountability\\tRisk and size are good indicators but it would be good to see other factors such as the operations of a service. Size and risk are generally good indicators but the types of duties could be tailored better to the capacity of companies that comply with them. For example, whether a company is a start-up, non-profit, or part of a bigger holding should be considered as it will limit a services' access to resources\\n\\tGovernance and accountability\\tIf Ofcom were to introduce measures requiring third-party audits, there is requirements set out in the DSA that should be adopted. If Ofcom were to require third-party audits in the future, there are requirements for this already under the DSA for Very Large Online Platforms and Very Large Search Engines. CELE suggest adopting the requirements from the DSA and providing guidance to make sure the methodology followed is uniform and achieves the goals of the Act if they were to be introduced. Governance and accountability\\tSupports not proposing a remuneration measure due to concerns around incentivising and measuring 'effectiveness' in online safety. Supportive of not proposing a remuneration measure as CELE don't agree with incentivising senior managers to achieve better outcomes for online safety. Rather, senior managers would be encouraged to execessively remove/deindex/downrank borderline and permitted content as it will be the metric to assess their effectiveness. A system tying remuneration to results could lead managers to misrepresent their achievements, therefore distorting the general picture of online safety and in turn misleading future policy decisions. Approach to the Codes\\t\\tAgrees the size of a service is important to determine risk however when carrying out measures, the ability of a service to implement them should be taken into account \\tAgrees the size of a service is one among many factors that can make a service more risky than others. CELE is positive towards risk being defined against risk and a risk matrix. There is potential that size being measured against its user base is not necessarily a good indicator of a services ability/resources to comply with measures (e.g. non-profit services). More tailored measures could be used in these situations\\nApproach to the Codes\\t\\tAgrees with the definition of a large service\\tAgrees with the definition of a large service\\nApproach to the Codes\\t\\tAgrees with the definition of multi-risk\\tAgrees with the definition of multi-risk\\nApproach to the Codes\\t\\tAgrees with the codes however CELE believe there is a significant risk to freedom of expression. Companies should not be entrusted to make judgements on what content is/isn't illegal\\tPraises the codes of practice for explicitly stating that Ofcom must carry out its functions compatiably with the Human Rights Act 1998. However, CELE believe the guidance set out in Annex 7 and 8 poses risks to freedom of expression. Companies should not be entrusted with making an illegal content judgement and to take action if they believe it to be illegal. Except for the case of hash-matching, decisions on illegality should be made by authorities and not outsourced to platforms who are more likely to err on the side of caution. Performance targets should only be restrained to automated hash-matching. The metric for the time illegal content is left online is not good as it can create incesntive for paid removal of content without services thinking first. Content moderation (User to User) \\t\\tServices shouldn't be the ones to classify content as illegal or not. This should be done by authorities \\tThe decision on whether content is illegal or legal should fall under the correct authority. Services should not be entrusted to make decisions on the legality of content as they are more likely to err on the side of caution and therefore classify legal content as illegal and have impacts on freedom of expression. Content moderation (Search)\\t\\tServices shouldn't be the ones to classify content as illegal or not. This should be done by authorities \\tThe decision on whether content is illegal or legal should fall under the correct authority. Services should not be entrusted to make decisions on the legality of content as they are more likely to err on the side of caution and therefore classify legal content as illegal and have impacts on freedom of expression. Automated content moderation (User to User) \\t\\tAgree with most ACM measures but suggests some level of human intervention for fraud keyword detection\\tCELE agree with the measures for hash-matching and URL detection. As for keyword detection, automated systems can be used as long as there are safeguards against removal of legal content such as the intervention of human moderators to confirm the accuracy of machine-made decisions. Automated content moderation (Search) \\t\\tAgrees with the proposals.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9966989159584045,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agrees with the proposals. User reporting and complaints (U2U and search) \\t\\tAgrees with both accessible and easy to use complaints procedures and the creation of DRC\\tAgrees with both accessible and easy to use complaints procedures and the creation of DRC\\nTerms of service and Publicly Available Statements\\t\\tAgrees with the proposals\\tAgrees with the proposals\\n\" Default settings and user support (U2U)\"\\t\\tAgrees with the proposals\\tAgrees with the proposals\\n\" Default settings and user support (U2U)\"\\t\\tComments on the language to better suit children\\tThe language used to inform children on the measures they can take could be tailored to meet the needs of differently aged groups. Enhanced user control (U2U) \\t\\tAgrees with the proposals but suggests blocking from official government accounts may lead to restriction of access to information of some users\\tAgrees with the proposals to block users however, allowing government/public officers accounts to block personal accounts could pose difficulties in acess to information\\nEnhanced user control (U2U) \\t\\tVerification systems should be made transparent with users having access to reasons for verification decisions and distinction between verification and paid subscriptions \\tVerification systems should be as transparent as possible with users having acess to the reasons why a certain account is verified. Also advises that a sign of verification (badge etc.) is different to the sign of someone who holds a \\'premium\\' membership through a paid scheme \\nUser access to services (U2U) \\t\\tDoesn\\'t agree with the proposals \\tDoesn\\'t agree with the proposals \\nService design and user support (Search) \\t\\tAgrees with the proposals\\tAgrees with the proposals\\nAutomated content moderation (User to User) \\t\\tICO aren\\'t opposed to the measures but have concerns around data protection, mostly with alignment with Article 22 of UK GDPR and how services can minimise incorrect reports of CSAM to the NCA. ICO aren\\'t opposed to the measures but raise points for alignment with data protection law. ICO disagree the privacy impact of automated scanning is minimal. The privacy safeguards should be expanded to cover Article 22 of UK GDPR, including reference to data protection requirements such as transparency, purpose limitation, data minimisation, and accuracy. Article 22 of the UK GDPR places restrictions on solely automated decision-making based on personal information. Services should be required to take into account the importance of minimising incorrect reports of CSAM to the NCA when confirguring technical accuracy and deciding on the proportion of material approriate for human review. The privacy impact assessments for ACM, the requirement to report UK-linked detected and unreported CSEA content to the NCA have not been comprehensively set out. ICO don\\'t agree that the potential privacy impact of automated scanning is slight. Using automated means of content moderation will still have data protection implications for service users whose content is being scanned. Automated content moderation (User to User) \\t\\tICO would like to see the guidance worked on by providing examples, clear instructions on public/private, and clarity in some places. ICO consider the guidance does not provide sufficient certainty to enable services to make confident assessments on whether content is communicated publicy or privately. The lack of clarity may incentivise services to inappropriately assess content and risk diluting important privacy safeguards in the OSA. ICO suggest adding in worked examples and to make the guidance more definitive, particularly with clarifications to paragraph A9.23. Where a service still lacks confidence in their assessment, the default should be that content is communicated privately. ICO encourage Ofcom to explain why services are required to make their own assessments as it\\'s not a requirement in the OSA. Further clarity is needed on the definition of \"a substantial section of the public\" and on if there are no access restrictions, the content should be considered accessible to the whole of the UK. Governance and accountability\\t\\tICO are broadly supportive of governance measures but would like emphasis on data minimisation where applicable (such as measure 3E)\\tICO supports the measures for written statements of responsibilities as it complements the accountability requirements under data protection law. For measure 3E (tracking evidence of new and increasing illegal harms), there is an assumption it is likely to involve processing of personal data. Where personal data is processed, data minimisation should be a key principle for services as it will support adherence to data protection law. Content moderation (User to User) \\t\\tICO published guidance for U2U services, setting out their data protection expectations for content moderation. ICO have concerns that the privacy impact assessments aren\\'t robust enough. Content moderation systems involve the processing of a lot of user\\'s personal data. User generated content is likely to contain personal information as well as being linked to a user\\'s account with further personal information. ICO have published guidance for U2U services setting out their data protection expectations for content moderation, although this was not available when the consultation was published. For measures 4A-F, 4G, 4H, and 4I there are privacy impact assessments but ICO think they don\\'t take sufficient account of the impact of the proposed measures on information rights. A lack of inclusion of data protection may lead that the privacy safeguards set out in the body of the measures do not fully mitigate the potential impacts. ICO are pleased to see the privacy impact assessments referencing the importance of complying with data protection laws but it is unclear how this has been integrated into the measures. ICO suggest Ofcom provies more robust assessments. Content moderation (User to User) \\t\\tPrivacy safeguards should be included  in some content moderation measures. There are no privacy safeguards included in the body of the A4 content moderation measures. This will help bolster a more comprehensive privacy impact assessment. ICO is pleased to see privacy safeguards included in measures 4G-I but still view them as incomplete. Some privacy safeguards are already provided by recommended measures but aren\\'t specificied as being privacy safeguards. Doing so will provide more certainty to services that privacy safeguards are incorporated into the measures. Some safeguards provided but aren\\'t referenced as such for content moderation are 4B, 4C, 4F, 6A, 5A (this is laid out in more detail in the response). Some areas to consider are transparency, purpose limitation, data minimisation, accuracy, retention of personal data, data protection rights, and rights related to automated deicison making under UK GDPR Article 22. Content moderation (User to User) \\t\\tICO is concered with the impact of false positives when reporting CSEA to the NCA and what services can do to minimise this occurrence. The privacy impact assessment considers some content will be incorrectly reported as illegal to reporting bodies and organisations as a result of moderation measures. ICO agree that an incorrect report that contains personal data would be a significant intrusion into an individual\\'s privacy. From a data protection perspective, as soon as a report is made to the NCA an individual\\'s rights have been significantly impacted regardless if any action is taken as  a result of it. Some reports will involve personal data of children which requires specific protection under data protection laws. ICO is concerned that the safeguards in measure 4G do not differentiate between the level of accuracy that is appropriate for reports to the NCA and other significant but potentially less harmful actions such as takedowns. ICO recognise not all false positives of content moderation can be eliminated, but it would be good to see a margin of error factored into a service\\'s systems and processes. Services should be explicitly required to take into account the importance of minimising false positives being reported to the NCA. Content moderation (User to User) \\t\\tSuggests adding references to data protection law to performance target measures. ICO suggests that paragraph A4.12 (relating to performance targets) includes a reference to data protection law so services can take all reasonable steps to ensure users personal information that is generated by content moderation processes is accurate. This is particularly importantfor detecting CSEA material due to the risk of incorrectly reporting it to the NCA. Content moderation (Search)\\t\\tWould like further clarification on how personal data is being processed when it\\'s assumed content moderation will mostly be dealing with web page content rather than users of a service and their personal data. ICO would like further clarification on the privacy assessment for measure 4A as it is unclear why there is a possible outcome of services users being reported to reporting bodies as a privacy impact. It\\'s expected on determining the legality of search content, it would primiarly involve moderation of web page content and personal data processing would be limited to third party personal data contained on web pages. If services users may be reported to reporting bodies, ICO have the same concerns expressed to similar measures for U2U content moderation. User reporting and complaints (U2U and search) \\t\\tIt should be made clear across all measures what information Ofcom cosniders necessary for services to retain and use to comply with their online safety duties. Paragraphs 16.26-27 state Ofcom decided not to include a recommendation for services to keep complaints data to facilitate appeals. However, other measures require or recommend the further use of complaints data (e.g. RAG or 3E). It\\'s important that the overall package of measures make it clear what information Ofcom considers necessary for services to retain and use to comply with their online safety duties. This will help services feel confident about complying with data protection obligations. User reporting and complaints (U2U and search) \\t\\tICO are happy to see accessibility recommended for complaints systems \\tICO are pleased to see Ofcom recommends that all providers have easy to find, easy to access, and easy to use complaints systems. It complements ICO\\'s  guidance on transparency under data protection law. User reporting and complaints (U2U and search) \\t\\tMeasures that concern themselves with timelines should also include a reminder to services that other timelines may be applicable and may differ when it comes to complaints that cover different areas of law. Measures concerning with timelines for deciding complaints and appeals (5C and 5E(i), (ii)) can be influenced by data protection law as online safety complaints may also constitute complaints or requests under data protection legislation. Services will need to ensure they are able to identify where an individual is also exercising their data protection rights and comply with the timeframes set out by relevant data protection law. The need to comply with data protection timeframes will only apply to parts of the complaint that fall within data protection law. ICO recommends that the measures remind services of the need to comply with timelines that are set out by other areas of law. Terms of service and Publicly Available Statements\\t\\tICO support the measures. The recommendations support and complement the transparency provisions of data protection law in informing individuals about how their personal information may be used by services fulfilling their online safety obligations. \" Default settings and user support (U2U)\"\\t\\tICO is supportive of this measure and it aligns well with ICO\\'s Children\\'s code for services. ICO\\'s Children\\'s code takes a similar approach to the measures set out. Transparency standards and clear appropriate language from the ICO\\'s Children\\'s codes align with the recommendations. ICO is supportive of the approach to this measure as it complements ICO\\'s Children\\'s code. Recommender system testing (U2U) \\t\\tDisagrees with the basis of consent for on-platform testing and raises an assumption this consent can be part of terms of service. The privacy assessment refers to the need for services to obtain consent for processing personal data for on-platform testing. Consent is one of six lawful bases for processing personal data under Article 6 of UK GDPR. As consent can be refused or withdrawn at any time, it may prevent effective testing. The assessment also suggests services could obtain consent as part of consent to overall terms of service. Consent for data processing must be specific and freely given - it cannot be bundled with consent to terms of service. ICO suggest removing this and instead redirecting services to ICO\\'s guidance for information and consent. Service design and user support (Search) \\t\\tSome privacy implications may have been overlooked in the privacy assessment for measures 7B and 7C - recommends to review these and put in appropriate references to data protection law. The privacy assessment for measures 7B and 7C does not consider there to be any impact on the right to privacy as there\\'s no requirement for services to retain information about searches that trigger warnings of suicide and CSAM. Services may still process personal data in order to trigger these warning depending on the process services use. Analysing searches to provide crisis prevention may also require services to process special category data relating to the health of users. ICO recommend that Ofcom reviews its privacy assessment of these measures to take into account the data protection rights of users and for the measures to refer to the need of services to familiarise themselves with data protection if they are going to process this data. Governance and accountability\\tAgainst renumeration for senior managers as it may have a \\'chilling effect\\' and misunderstands how trust and safety programs operate. Tying renumeration for senior managers to positive online safety outcomes could have a chilling effect on the willingness of individuals to take on such roles. Online safety outcomes aren\\'t determined by a single person but a myriad of employees, contractors, and vendors all working towards a common goal as well as by the users of the service and how they choose to interact with one another. Tying renumeration to positive online safety outcomes misunderstands how trust and safety compliance programs operate and disproportionately assigns responsibility for negative outcomes. Approach to the Codes\\t\\tMost gaming platforms take steps to mitigate risks already but introducing regulation must ensure consistency globally \\tMost platforms take steps already to mitigate and prohibit illegal content as defined in the OSA on their platforms. Developing regulation that acknowledges the nature of global businesses and is consistent with the expectations or regulations of other countries is essential. Approach to the Codes\\t\\tAgrees that the burden should be proportionate in regard to size, resources, and level of risk but there is concerns how services are designated risk\\tAs a general matter, UKIE members support the principle that compliance burden should be proportionate to both service\\'s size and resource levels, and the risks they pose. However, there are concerns regarding how services are designated as medium, high, or multi-risk (see Q7-9). Approach to the Codes\\t\\tUser base is not the best indicator alongside an arbitrary threshold, with confusion over what a \\'user\\' is and how to measure it. UKIE understands the comparison to the DSA for setting a threshold for large services. However, UKIE has concerns that the threshold is rather arbitrary and focus should be placed on the functionalities of a service. Clarity is also required on what is meant by user numbers - is it active or registered users and if the former, how should this be measured? Is access only needed or would engagement be required too? Mirroring the DSA in a UK context is not adequate to reflect the diverse risks and responsibilities associated with different types of platforms in the UK landscape. Additionally, user base size is not necessarily determiniative of, or the most appropriate proxy to, whether it is justified to impose more onerous measures for some services. Effectively assessing online safety risks and responsibilities should embrace multi-faceted frameworks offered by the DSA to ensure fairer regulations. Approach to the Codes\\t\\tUKIE doesn\\'t agree as the definition is too restrctive and disproportionately impacts gaming companies with burdensome obligations. UKIE also raise the arbitrary definition of multi-risk and suggest other ways to assess this. UKIE doesn\\'t agree with the definition as it is too restrictive and disproportionately impacts gaming companies due to the nature of the paltforms. UKIE believes any game company whose service includes a chat functionality would be deemed either high-risk or multi-risk due to having a chat functionality. This would warrant more burdensome obligations for those found to be multi-risk despite not presenting the same level of risk as other services. A more proportional approach to multi-risk is needed, including viewing historical data, how the service is used by users, the types of user generated content shared, and any current mitigation efforts. It is not clear either why only two identified medium risks should warrant being deemed multi-risk and the line drawn is arbitrary. It would be more proportionate to take into account how significant the risk of each of the types of harm is in light of the funtionalities used by the service and users. The current approach also treats all illegal harm as being equal, with no consideration for different levels of severity and the harm they cause. The OSA addresses this by asking services to prioritise based on potential harm in Section 9 and 10. Proportionality is key when considering appropriate measures and responses due to the diversity of the online games industry and tech industry as a whole. Content moderation (User to User) \\t\\tUKIE agrees with the proposals, detailing what gaming companies currently do for content moderation\\tUKIE agrees with the proposals - content moderation is key to the gaming industry. Proposals that prioritise content for review are not fit for purpose for most platforms however, as in-game communications do not lend themselves to the time-sensitive issues of virality or public engagement on pieces of content. Platforms often engage with safety organisations and other platforms such as IW, UK Safer Internet Centre, Fair Play Alliance amongst others. Information sharing plays a role in games companies, sharing development efforts with the industry and wider technology sector so all can learn from best practice. UKIE note details on content moderation methods used by large game companies, notably: filtering, reporting, moderation, and enforcement actions. An important aspect of content moderation is the role of community manager and teams. It serves as a way to relay perceptions, expectations, trends, and other information about the community to the company. Some larger comanies are starting to introduce semantic analysis toolsto assist in identifying warning signs earlier in game play. Automated content moderation (User to User) \\t\\tAutomated systems are used already by gaming companies with emphasis placed on machine learning becoming increasingly used. Currently, most systems used to moderation in-game communication and user generated content are proprietary tech with a mixture of automated and manual processes. Some systems also use analytics team (ADSE) workflows which may use external software and data storage. Companies use different methods that are best suited to their games rather than one overarching generic solution. Machine learning is increasingly being used to imprive moderation of in-game communication and is now able to cover images, text, and audio. As this develops, enhanced monitoring and support to human moderators can be provided and help identify new and emerging harms. ACM is mostly used for filtering out banned words, phrases, URL addresses, filter through abuse reports for key words (which are then manually reviewed), scan chats for child exploitation, harassment, risks to life, and review images. Automated systems also play a key role in detecting cheating or highly toxic users. Companies still offer a complaints process for users to challenge and appeal findings of themselves or submit reports against another user. All moderation services and approaches are agreed with content moderation teams which are regularly sampled to sense check effectiveness and identify false positives so the processes can be improved if required. Automated content moderation (User to User) \\t\\tThe guidelines are useful, many gaming companies also remind users that they shouldn\\'t consider communications as \"private\" or confidential. The guidelines aim to recognise the variety of services users may engage with. Many gaming companies also remind players through Codes of Conduct, Community Guidelines etc. that users should not consider messages to other players as \"private\" or confidential. Automated content moderation (User to User) \\t\\tThe accuracy can vary on hash distance settings and additional filters. Costs are also of a concern\\tThe accuracy can depend on hash distance setting and additional filters. On costs, they can include start-up engineering, training and labour costs, as well as applying human review to all flagged reports. Automated content moderation (User to User) \\t\\tMain challenge for companies is the cost\\tThe main challenge for companies when implementing measures are engineering and labour costs to adapt procedures, technology, and teams to meet external databases\\nAutomated content moderation (User to User) \\t\\tMain costs are associated with engineering and labour\\tThe main costs for UKIE members are associated with engineering and labour costs. Many members experience low prevalence of CSAM URLs which ultimately limits the value of fuzzy matching for CSAM URL detection\\nAutomated content moderation (User to User) \\t\\tLarge cost for engineering and labour\\tThere is significant cost in engineering and labour for UKIE members. User reporting and complaints (U2U and search) \\t\\tAgrees with the proposals except that due to the nature of many gaming services, filtered content cannot be restotred upon successful appeal. UKIE mostly agrees with the propsals apart from specifically that filtered content should be restored upon successful appeal as this is not pratically possible in many circumstances due to the ephemeral nature of in-game communication. Another concern is on the proposal of providing indicative timelines for deciding complaints.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.985174834728241,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"It isn't the most efficient with barriers creating inflexibility or reduced decision-making powers for complaint handlers and instead propose to allow users to access a tracked status of the report as it will empower users with more relevant information. Terms of service and Publicly Available Statements\\t\\tThe requirements for Terms of Service are too specific, making services haivng to create long ToS and preventing them from being accessible. The OSA's requirements for terms of service are incredibly specific (e.g. requiring seperately addressing each form of primary priority content that is harmful to children) leading to long and detailed sections of terms of service. Not only will this have an overall impact in making terms of service but is also contrary to the aim to ensure terms of service should be drafted that minors can understand the content and are easily accessible to all users. Minors and the majority of users will have difficuly understanding a long terms of service that will necessarily include other rights (e.g. boilerplate clauses, IP clauses, liability clauses etc.). A better solution would be to allow greater flexibility in including necessary information outside of the terms of service in another accessible format. For example, safety pages, articles etc. allow for varied mediums for presenting information which can lead to greater accessibility.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9929187893867493,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It could be that URLs to these pieces of information are linked in the terms of service to meet the requirements. \" Default settings and user support (U2U)\"\\t\\tUKIE wants to ensure that the proposals don\\'t hamper the ability for real life friends to seek each other and connect online. UKIE agree with the need to innovate and provide greater protection and safety for child users however, UKIE want to ensure that the proposals don\\'t hamper the ability for real life friends to seek each other and connect online. Many games allow online match making to either build teams or partners for specific games. Greater clarity is needed on the definition of \"prompts to expand their network or friends\" to ensure it does not include multiplayer games. If it is applied broadly, it would make it impossible to have multiplayer online games with other players. Not including players in publicly visible lists makes any interaction difficult or impossible even if they are part of a team. Chat and communication controls help ensure the same outcome without impacting overall experiences that a group of friends may be looking for. \" Default settings and user support (U2U)\"\\t\\tThe proposals aren\\'t applicable to gaming services. For game companies, the limited prevalence of illegal content does not warrant it. Recommender system testing (U2U) \\t\\tUKIE points towards the PEGI system and other standards that are currently used in the UK/globally for gaming services for establishing good practice. The PEGI system is successful in establishing good practice. By joining PEGI, a publisher has to sign a Code of Conduct committing them to provide users and parents with objective, intelligible and reliable information regarding the suitability of a game\\'s content among undertaking other responsibilities. The industry also established the IARC (International Age Rating Coalition) to provide a solution for the globalised market of apps and has been adopted by Google Play Store, Microsoft Windows Store, Nintendo, and others. Some game companies purposefully use safety by design to appeal to these standards and meet their agreements. It may also include carrying out impact assessments, done at the design stage to identify and reduce risks. Through methods like age verification, manual intervention into account reclassification, and survey-based research, companies can assess factors on how people access their service and specifically the number of child accounts created. User access to services (U2U) \\t\\tUKIE agrees with the proposals. UKIE agrees that services should block accounts of users that share CSAM. In previous instances, companies have made a report to relevant law enforcement authorities. User access to services (U2U) \\t\\tDetails ways some gaming services block users\\tSome examples of practices of UKIE members to block users are: banning the username, email address, IP address, and sometimes possible to ban at the device level. Another method is suspending the offender\\'s account by permanently prevent the device used to connect to the network. It has been found by UKIE members that IP addresses are generally ineffective as they can have unintended consequences of punishing innocent players. User access to services (U2U) \\t\\tUKIE members impose permanent bans for any account holder sharing any CSAM with no exceptions. UKIE members impose permanent bans for any account holder sharing any CSAM with zero exceptions. This is typically followed by a report to relevant law enforcement authorities\\nUser access to services (U2U) \\t\\tContent already classed as CSAM is rejected from being uploaded - all suspected CSAM content caught is sent for human review. Content that is classed as CSAM by an automated tool will already be rejected from upload and the user would be told to try another image. All suspected CSAM content caught by the automated tool is sent for human review. The user is not sanctioned if the human reviewer decides it is not CSAM. If it is decided to be CSAM, the appropriate actions are taken. Human review significantly mitigates the risk of impacting a user\\'s rights. Cumulative Assessment  \\t\\tDoesn\\'t agree that the burden of measures is proportionate\\tDoesn\\'t agree with the burden of measures is proportionate (see answer to Q15). It would be helpful to obtain further guidance on what conditions services can meet to move into the low-risk category. Cumulative Assessment  \\t\\tDoesn\\'t agree (see Q15)\\tDoes not agree - see answer to Q15\\nCumulative Assessment  \\t\\tFocus should be on the functionalities of a service and nature of communications, not just the number of users on a service. The focus should be on the functionality of a service and the nature of communication and content-sharing it enables.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910188913345337,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This helps develop on the overall risk profile for users, more than just raw number of users on a service. Governance and accountability\\t\\tThe focus for strengthening platform governance should be instead on platform design\\t\"The framing of The responsibilities puts The focus on J15 and destroying content, not systemic approaches that could have an impact on reducing The production of, and/or incentives to create, harmful content. Content tracking also misses huge swaths of harms like fraud and some ephemeral interactions like in gaming or VR/AR. This also codifies unweighted content prevalence as The metric of success, whereas perspectives change on The most effective way to deal with harms, and prevalence of harmful content on platforms is not indicative of The platform’s impact on dealing with root causes of harms or The scale of harms caused. The focus for strengthening platform governance should be instead on platform design, and building transparency and accountability for The ways in which it contributes to The dissemination of illegal content, and incentivizing better design choices. platform governance includes what metrics The company uses to measure success, how it tests its systems, and how decisions are made about how different values (safety, growth, etc) are weighed. To mimic Ofcom’s stance on bookkeeping, this section should include: Keeping record of when responsibilities for senior members is revised; Keeping record of when a code of conduct is revised; If the staffing and service’s approach on compliance is revised\"\\nGovernance and accountability\\t\\tMetrics are a key way in which integrity and trust and safety can be better embedded into platform governance processes\\tMetrics are a key way in which integrity and trust and safety can be better embedded into platform governance processes. If the decision making process is dominated by platform growth and business interest metrics, and integrity and trust and safety metrics are not integrated until later, it will be difficult for risk mitigation measures to be properly incorporated. Outlined various challenges with integrity metrics, including the clash with engagement metrics. Platforms should not use engagement metrics to measure success - other options include quality focused metrics, user surveys of how much value they are getting from the platform, and/or only counting clearly good forms of engagement (e.g. Google Search)\\nGovernance and accountability\\t\\tAB tests should include integrity and safety focused metrics, such as the prevalence of harmful content, exposures to predicted harmful content, or user reports of harmful experiences. AB tests should include integrity and safety focused metrics, such as the prevalence of harmful content, exposures to predicted harmful content, or user reports of harmful experiences. Important to understand how they make trade-offs between the business interests and driving up user engagement, and the integrity and safety of the platform. Automated content moderation (User to User) \\t\\tOn the use of keyword detection for fraud: Keyword detection is less effective than a more tailored approach. Keyword detection is less effective than a more tailored approach. To improve the robustness of the keyword approach, other signals such as the account history including content toxicity, coordinated link-sharing behavior, sharing unreliable media, engagement patterns, \"disbelief\" responses to their posts, account reporting history, creation date, posting frequency, etc. should all be considered in the review of a flagged post or account. This would also serve a meta-analysis of trends in future flagging efforts.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9954219460487366,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This pre-supposes the availability of keywords in multiple languages that the platform might serve content in. If there is not a reliable, prescribed way to source these keywords, it is likely that this will lead to unmeasured biases in the sourcing of actionable content, with downstream impacts on certain user groups\\nAutomated content moderation (User to User) \\t\\tNumerous issues with hash matching - additionally encourage companies to do more cross platform hash sharing \\tNumerous issues with hash matching including: only works for still images; images must be an exact match including cropping; false positive matches; smaller services will struggle when dealing with reporting and legal aspects; applying to other harms still has the same issues. Potential positive of hash matching is that platforms have the challenge of coming up with definitions of what constitutes various harms such as suicide and self injury or eating disorder content, and would encourage platform alignment of these issues. Would additionally encourage companies to do more cross platform hash sharing. \" Default settings and user support (U2U)\"\\t\\tStrongly caution against ID-based age verification requirements, as there are major trade offs presented with age verification and privacy. Strongly caution against ID-based age verification requirements, as there are major trade offs presented with age verification and privacy. There are other approaches, including setting all accounts to the safest defaults, or using device-level verification. \" Default settings and user support (U2U)\"\\t\\tDefaults should be recommended for all users to improve user awareness of tools and encourage safety for all users\\tDefaults are a good tool for improving safety for all users, not just child users - these defaults should be recommended for all users to improve user awareness of tools and encourage safety for all users. Any reasoning as to why adult users should be given less informative default settings should be researched and thoroughly justified. There should be thresholds when discussing child users as there is a stark difference between child users age 3-5 and any child user under 13. \" Default settings and user support (U2U)\"\\t\\tWording of these proposals seems overly prescriptive and fails to be future proof.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934139847755432,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Wording of these proposals seems overly prescriptive and fails to be future proof. “Receiving a direct message” fails to encapsulate first time interactions in VR or interactions in a public game chat for example. This assumes that direct messaging will be the sole communication style in x years. We argue that instead of prescribing to the type of interaction, to prescribe towards the principle of ‘this is the first time a child user is being contacted’ instead\\n\" Default settings and user support (U2U)\"\\t\\tUrge platforms to consider alternative approaches, such as implementing penalties for harmful actors rather than implementing more controls on child users. Urge platforms to consider alternative approaches, such as implementing penalties for harmful actors rather than implementing more controls on child users. Reducing child freedoms on any platform will not be a permanent solution. Stricter thresholds around those that are promoting the harms are encouraged to target the source of the risk. Recommender system testing (U2U) \\t\\tThere are good practices when testing recommender systems to see if they are lifting harmful content, and evaluating changes. Best practices in evaluating changes to algorithmic systems would begin with “offline” evaluation of the change, using various simulation scenarios (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934294819831848,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Google Search).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9557453989982605,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Response outlines the first and second stage of evaluations (simulated to human). If a change looks good according to both these stages, then a live A/B test on users can be run. This would involve assigning a small subset of users, 1% for example, into a test or control group, anywhere from a few hours to a few weeks. At the end, metrics are computed for the test and control groups and compared to assess the impact the change would have. Ideally, platforms would have lots of integrity and trust and safety metrics in their tests, including: Metrics to track negative experiences (exposures to content that are likely violating; Prevalence of violating content: number of user report/flags; number of exposures to low quality content or accounts) and measures to track good experiences (impressions on high quality content; user responses on surveys about experience; clearly good engagement signals). Recommender system testing (U2U) \\t\\tThe primary metrics used by platforms are typically motivated by business interests, and will include things like user engagement, time spent on the platform, retention, and impressions on advertisements and revenue. This is poor practice that does not account for any risks. \"The primary metrics used by platforms are typically motivated by business interests, and will include things like user engagement, time spent on the platform, retention, and impressions on advertisements and revenue e.g. “increase time spent on the platform by 5% over the next 6 months”. Another poor practice would be to immediately begin experimenting on users in live A/B tests. After the A/B test concludes (hours to weeks), then it will be assessed. A poor assessment may be as simple as looking at a few engagement metrics, such as overall time spent on the platform and daily active users, and then launching. These types of narrowly focused decision making processes based on bad testing practices do not account for risk, and may pose risks themselves. We note: There is no accountability enforced for platforms to use the results of recommender system testing to actually make changes. Is this requirement simply that the service document its understanding of the impact of proposed changes on safety metrics, not guidance on whether or not the service can ship those changes even if the tests show negative impacts? It is also worth noting that testing for prevalence of illegal content has its limitations - is often so low that it may be difficult for even the largest platforms to find statistically significant metrics in test results. Therefore hardhard to see effectiveness of results; will rarely see illegal content go down X% where is X looks like a meaningful number. We recommend the results of these tests and definitions of the metrics used be available to third party auditors. Platforms should also be transparent about the most important features and machine learning models used in their recommender systems, and test how harmful content performs in these models. This can be tested internally and released publicly. Platforms should also study how their recommendation systems respond to illegal content. Gives examples of how this can be done\"\\nRecommender system testing (U2U) \\t\\tChoices that favor engagement based ranking will be risky. \"Other design parameters and choices that are proven to improve user safety:\\nInventory (what Ofcom refers to as “content pool”): The more opportunities there are for users to be shown lower quality content from accounts they don’t follow, the larger the risk is that violating and risky content will be broadly distributed. Gives evidence from Facebook 2020 series of research. In our study, we saw the highest levels of amplification of misinformation on Twitter, TikTok, and Facebook Video - platforms that rely heavily on engagement based ranking\\nFeatures (what Ofcom refers to as “content signals”): Features based on the historical engagement of users will pose more risk than other types of features that machine learning models can use. Machine learning models and final ranking score (“tuning prediction weights”): Machine learning models that predict the likelihood of a user engaging with content will pose more risk than other types of machine learning models, such as those predicting the safety of the content or the quality of the content. The more the ranking score is dependent upon engagement based factors, the higher risk it poses. \"\\nRecommender system testing (U2U) \\t\\tWhat is missing from Ofcom’s proposal is a discussion of other features of platform design that can improve user safety\\tWhat is missing from Ofcom’s proposal is a discussion of other features of platform design that can improve user safety. Companies can implement platform features that guard against gaming the systems e.g. Friction: (requiring more steps before resharing a post); Slowing the spread of highly viral content; Aging in (requiring accounts to exist for a certain period of time before they gain access to certain riskier features); Limiting the size of chat rooms, groups, and account followers. Companies can impose limits around risky behaviors e.g. No recommended content (only content from people a user has elected to follow); No recommended follows of unknown accounts; No contact (DMs, comments) allowed from unknown\\u202f or unconnected accounts \\nUser access to services (U2U) \\t\\tWhile IP and username blocking along with tracking social graphs are best practice measures for addressing CSAM content, as it stands, human review is by far the most applicable to date. While Ip addresses can be useful in following up on accounts, it isn’t a useful tool in handling CSAM recidivist accounts as can be changed. IP blocks and username checks are easy enough for any size platform to incorporate as an early step. Encourage platforms (especially ones with high risk of CSAM content) to track social graphs. Users that recidivate typically join similar social graphs and groups immediately after returning to the platform. Regarding punishment for sharing CSAM content: If someone posts something that needs to be reported to NCMEC, their account should be locked no matter the level of offense; malicious or high impact CSAM sharing should be on a zero strike policy. While IP and username blocking along with tracking social graphs are best practice measures for addressing CSAM content, as it stands, human review is by far the most applicable and necessary (and necessary for legal considerations when reporting to NCMEC)\\nApproach to the Codes\\t\\tThe group is concerned that the illegal harms Code of Practice fails to adequately bring small, single-risk services into scope by giving them specific exemptions from the regime – and from mitigation measures. The response says that Ofcom is aware of the risk that smaller services can have, yet the stakeholders feel this is not reflected in the Code of Practice due to Ofcom’s interpretation of proportionality and size. The response highlights an amendment to the Bill/Act on the last day of report stage that changed language of the Act from \"simply size\" to \"size or risk\". The stakeholders feel this change does not seem to be reflected in the Code as currently written.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9925069212913513,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They say \"We are also horrified to see that some companies with millions of users have not been considered large – surely this is an error. Ofcom must ensure that small services cannot bypass regulation that absolves them of their duties.\" \\nUser reporting and complaints (U2U and search) \\t\\tThe group says they are not satisfied that Ofcom\\'s approach to both reporting systems and content moderation address the needs of children and parents using online services. The response says that children and parents need clear and distinct reporting mechanisms that offer a right to appeal, are transparent, involves a human early in the process if a child is involved. Services do not provide enough transparency about moderation decisions. The stakeholder wants to be able to report issues with handling of user reports/content moderation to Ofcom. Content moderation (User to User) \\t\\tThe group says they are not satisfied that Ofcom\\'s approach to both content moderation and reporting systems address the needs of children and parents using online services. The response says that services do not provide enough transparency about moderation decisions. It makes comments about content moderation in connection with user reports (see that row as well). Terms of service and Publicly Available Statements\\t\\tThe group says internet companies should not be allowed to bypass their duties by saying that content doesn\\'t violate their community guidelines. The response says that in the experience of these bereaved familes, the reporting mechanisms of online services are ineffective when the outcome is that harmful content or activity does not violate services\\' community guidelines. It says that in relation to primary priority illegal content, the risk of harm should be made evident in the terms of service and there should be full alignment with reporting mechanisms. Recommender system testing (U2U) \\t\\tThe group says they are disappointed that Ofcom has set a high bar for self-harm offences within the Code of Practice, and not considered cumulative impact, especially for vulnerable users. The response says that in the experience of these bereaved familes, their children were put at a greater risk of harm because an algorithm showed them content, and because of the context in which they viewed the content. The stakeholders believe that Ofcom must have regard to the contextual judgement of recommender system recommendations that are made to users who may be seeking harm or more vulnerable to harm. This comment is made in reference to self harm offences. Approach to the Codes\\tMulti-risk\\tSpotify is particularly concerned by the proposal that any service which is at risk of 2 out of 15 illegal harms, will be considered a “multi-risk service.” \\tSpotify thinks the proposal is too broad and is not evidence based and is an arbritary threshold. It asks how a service will be considered at risk of an illegal harm - for example if it would relate to a qualified measure such as the number of \\'notices\\' a service receives from a relevant UK authority in a given period. Approach to the Codes\\tDSA/OSA harmonisation\\tWants expectations and implementation of OSA to be harmonised as much as possible with EU\\'s DSA. Spotify says it has made a range of investments to comply with the DSA. It says harmonisation between the DSA and the UK’s OSA would help avoid conflicting standards and excessive compliance costs. Governance and accountability  \\t\\tConcerned that some proposals are too prescriptive\\tSpotify says it wants Ofcom to be proportionate and allow services to prioritise certain harms over others. It cites part 3, vol 2 as an example of an overly prescriptive approach and \"excesive transparency\" - and says this is contradictory to creating an open and transparent environment to manage risk. Governance and accountability  \\t\\tConsider scalability and practicality in relation to lower risk services\\t\"Spotify says \"\"We suggest further consideration of the scalability and practicality of proposed\\nmeasures, ensuring they align with the operational capacities of lower risk tech companies.\"\" This relates to other comments that Spotify\\'s business model is not primarily a U2U service and it therefore sees its model as low risk.\"\\nGovernance and accountability  \\t\\tSpotify urge against mandatory external audits because they could introduce generic frameworks which don\\'t relate to a specific business model or risk profile. Spotify says it has robust internal audit functions that are designed with relevance to its service. It includes a dedicated team, the Global Platform policy team. It updates its \"Platform Rules\" based on abuse trends. It develops internal guidelines that inform its content moderation teams. The response highlights privacy concerns and risks for users\\' data that would be associated with external audits. Content moderation (User to User) \\t\\tSpotify thinks content moderation approaches need different timelines, and moderating artistic content may require a deliberative approach or can sometimes be done rapidly. \"Spotify\\'s response strongly recommend that Ofcom develops a flexible approach to enforcing\\nmoderation decisions, that takes into account the nature and format of the content, the risk profile of the service and the need to consult with experts. Also highlights that audio content may require more manual and deliberative, and less automated review. Also highlights the contrary perspective that fast decision-making should not be penalised if an appropriate risk-based approach is followed. \"\\nContent moderation (User to User) \\t\\tSpotify outlines which elements of the content moderation proposals it supports and also cautions against prescriptive measures. The response says that \"Spotify supports Ofcom’s acknowledgement that the appropriate use of moderation guidelines; the training and use of human staff; and the training and use of automated systems to remove harmful content all have a role to play in content moderation.\"  It adds that expectations for services to have policies on content for review shouldn\\'t be too prescriptive, so platforms can design policies according to the content on their services. Approach to the Codes\\t\\tRisk profile should be the predominant factor for applying measures to services, not userbase. Spotify\\'s response  says \"we believe obligations should be targeted and appropriately placed on the services that pose greater risk, and reasonable measures should apply to all U2U services in all risk categories, such as implementing a content moderation system or a process to take down illegal content swiftly.\" It reiterates their belief that their business model of sharing licenced music content is much lower risk than other music-sharing services that are 1. designed for social engagement or 2.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9924383759498596,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'have commercial incentives to reward sharing of content. It says that its content recommendations are based on a user\\'s on listening habits and that virality is not a feature of their service or audio services generally speaking. Approach to the Codes\\t\\tSpotify oppose the idea that size alone should influence the degree to which a service is regulated under the OSA\\t\"Spotify\\'s response says that a large service should not face more regulation than a small service, where they are both low risk. It also says a large service should not face unnecessary regulation just because it is assumed they are able to bear the burden. \"\\nApproach to the Codes\\t\\tSpotify think userbase should be defined in relation to the U2U parts of the service. Spotify\\'s response says it is unclear how Ofcom is defining user numbers for these purposes. Spotify says it believes it is essential for proportionality that users are defined based on their actual engagement with user generated content / the U2U portion of the service in question. Content moderation (User to User) \\t\\t\"Spotify supports allowing platforms to set out the factors that services should have regard\\nfor when designing their content moderation systems.\"\\tThe response says Spotify supports ‘Approach 3’ set out in Volume 4 (as per one-line summary). It also says the company applies content policies and T&S measures across all types of content, and invests in expanding global coverage of its content moderation teams - including coverage of the UK. An example provided is \"Spotify’s recent acquisition of Kinzen, an Ireland-based, global leader in protecting online communities from harmful content that is particularly specialised in podcast and audio content\". User reporting and complaints (U2U and search) \\t\\t\"Spotify supports the specific proposals that call for an easy to find and use complaints process, that\\nacknowledges receipt of complaints and that responds to successful complaint appeals, by\\nreinstating the person’s content/account.\"\\tSpotify says it \"recommends giving services discretion to design systems in line with their content offerings\". User access to services (U2U) \\t\\tSpotify deploys measures in relation to CSAM content. Spotify says it suspends user accounts to block sharers of CSAM content. It says it deploys this option to prevent repeat offenders of policy, IP and fraud violations from accessing the service.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9917214512825012,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It adds \"There is a risk that lawful content is erroneously classified as CSAM by automated systems, which may impact on the rights of law-abiding users.\"\\nApproach to the Codes\\t\\tWelcome the intention that the Codes be implemented in a manner that is proportional. Pinterest welcomes Ofcom’s intention that the Codes be implemented in a manner that is proportional to platform’s differing risk profiles and operational capacities. Codes should allow flexibility for platforms to implement only the safety measures that are practical and effective in light of their unique designs and risk profiles, and do so in a manner suited to the features and functionality of platforms. Approach to the Codes\\t\\tRecommend a nuanced approach to avoid undue burden on platforms. For small and medium sized platforms to continue to innovate and grow, it is important to avoid taking steps that are not necessary to promote users’ safety, considering substantial costs of compliance with the codes’ measures. Even platforms that may be considered “large”, due to their user base, may have vastly different resources and capabilities than other “large” platforms due to differences in business models/stages of development. Pinterest recommend a nuanced approach that accounts for platforms’ respective features, risk profiles, and resources, to ensure Ofcom’s regulatory framework is tailored to relevant circumstances and avoids undue burden on platforms. Approach to the Codes\\t\\tAppreciate Ofcom\\'s decision to account for a platform\\'s risk profile, as well as its size, in determining whether additional requirements will apply. Appreciate Ofcom\\'s decision to account for a platform\\'s risk profile, as well as its size, in determining whether additional requirements will apply. Helps ensure proportionality in the measures applied, and is an improvement from other regulatory frameworks such as the DSA. Approach to the Codes\\t\\tRisk thresholds are set at a level that will place a disproportiate burden on many platforms, particularly multi-risk. The definition of \"multi risk\" platform would likely lead to the most onerous measures being applied to platforms whose risk profiles do not merit such treatment. Do not agree that being medium risk of at least two harms entails a significant overall risk, particularly when considering design, use cases and existing mitigation measures. Approach to the Codes\\t\\tThe presence of any functionality should not necessarily lead to the conclusion that it poses harm. The draft codes\\' approach to risk assessment is largely premised that certain platform functionalities carry increased risk. The presence of any functionality should not necessarily lead to the conclusion that it poses harm - this approach would likely lead to most platforms being categorised as \"multi-risk\". Approach to the Codes\\t\\tThe level of risk that functionalities actually pose to users is dependent on a number of platform specific factors. The level of risk that functionalities actually pose to users is dependent on a number of platform specific factors ie design, existing controls and safeguards in place to mitigate risk, as these will result in smaller, residual risk for each functionality. Approach to the Codes\\t\\tIf platforms are deemed higher-risk based on their functionalities, we are concerned this approach would eliminate risk profile as a factor in determining obligations. If platforms are deemed higher-risk based on their functionalities, we are concerned this approach would eliminate risk profile as a factor in determining obligations, as the outcome of the OSA framework would be similar to the DSA in that risk is assessed purely on platform size, particularly considering that the codes identify a platform\\'s user base as a contributing factor to high risk. It would also make the codes more prescriptive, as more platform will need to meet highest compliance burdens due to functionalities, instead of adapting and responding appropriately to actual risks posed by their particular service. Approach to the Codes\\t\\tWe would welcome a more holistic, nuanced approach that accounts for the role of platfroms\\' design and risk mitigation measures. We would weclome a more holistic, nuanced approach that accounts for the role of platforms\\' design and risk mitigation measures. This would more accurately reflect the true risk profiles of regulated platforms, help foster a safe and positive online experience, while avoiding undue burden on platforms that already provide a low-risk service to users. Approach to the Codes\\t\\tSome measures are highly prescriptive\\tRecognise that Ofcom has emphasised that the codes consist of recommendations and that platforms may comply by implementing alternative measures. However, some measures throughout the Codes appear highly prescriptive, imposing particular measures or technologies rather than setting an outcome to be achieved and allow service flexibility to implement proportionate solutions tailored to platforms. Approach to the Codes\\t\\tUnclear how platforms can demonstrate alternative measures, or how Ofcom would assess these. Not clear how platforms can demonstrate that their alternative measures are achieving a similar or better outcome, or how Ofcom would assess whether those alternative measures are sufficient to comply with the Codes. Ofcom’s recognition of alternative approaches is welcome, guidance on the criteria to be applied to alternative measures would help platforms confidently plan for compliance with the Codes. This would also prevent the codes recommended measures from effectively becoming required, as platforms feel pressure to default to them to avoid risk of enforcement. Approach to the Codes\\t\\tfeedback on proposed measure 4A \\tAgree harmful content should be actioned expeditiously, but do not feel turnaround time should be treated as a determinative factor in whether platforms’ content moderation systems are effective. Pinterest place more emphasis on how many impressions an item of content received before it was actioned ie how many users viewed the content. As speed is not the sole measure of whether content moderation is effective, we believe this measure should be applied flexibly. Approach to the Codes\\t\\tfeedback on proposed measure 4C\\tOfcom proposes that platforms set performance targets for how quickly illegal content is removed and for the accuracy of content moderation decisions. Pinterest do not object overall to setting performance targets, but do not feel they should be used as a determinative measure of effective content moderation. Encourage Ofcom to allow companies to set performance targets that make sense in the context of their own platforms and content moderation systems, and performance should be measured with an \"eye towards overall success of content moderation efforts\", rather than reductive consideration of whether targets were met. Approach to the Codes\\t\\tfeedback on propsoed measure 4H\\tThis measure states that platforms should employ automated tools to detect URLs which have been identifed as hosting CSAM. Pinterest believe this measure is straightforward and technically feasible, but would welcome Ofcom providing platforms with a list of known CSAM domains. Approach to the Codes\\t\\tfeedback on proposed measure 4I\\tPinterest\\'s Community Guidelines prohibit harmful or deceptive practices such as exploitative financial practices, counterfeiting documents, and impersonation; as well as any content that reveals personal identification, private contact informatin, online login information, or financial information. This content can be reported by users. Approach to the Codes\\t\\tfeedback on proposed measure 4I\\tPinterest is concerned this measure is overly prescriptive and will overburden platforms. It would likely require a vast number of moderators to comply with the measure, and will likely only be practicable for the largest, well-resourced platforms. Proacticely searching the platform for fraud related items could result in queueing an overwhelming number of Pins which would need to be reviewed by human moderators to determine whether they are violative, and even then would not necessarily capture the full specture of potentially fraudulent content. Would like Ofcom to reconsider the measure, or work with platforms to set a restrictive set of keywords that are closely associated with known scams. Approach to the Codes\\t\\tfeedback on proposed measure 5C\\tPinterest would like Ofcom to consider aligning this measure with the DSA, whereby there is not a requirement for complainants to be provided with a timeframe for deciding their complaint, but does contain requirements concerning notifications to users on status and outcome of complaints. Approach to the Codes\\t\\tfeedback on proposed measure 5I\\tPinterest would like Ofcom to consider aligning this measure with the DSA, whereby trusted flaggers use the same channel as users for reporting illegal content and the only requirement to platforms is that these reports be given priority. This would reduce additional burden on platforms to maintain additional communication channels. Approach to the Codes\\t\\tfeedback on proposed measure 9A\\tPinterest feel the measure is overly prescriptive and the objectives of the measure are already met by their existing safety measures. Pinterest feel it would be disproportionate to have to develop a “mute” function, taking into consideration the functionality of the platform. Approach to the Codes\\t\\tfeedback on proposed measure 10A\\tWhilst Pinterest are committed to tackling content that supports proscribed organisations, they are concerned that this measure imposes proactive monitoring obligation on platforms. Proactive monitoring of content across an entire platform would be extremely burdensome and disproportionate. If Ofcom is proposing proactive monitoring, Pinterest recommend this measure be applied proportionately, based on a platform’s risk for this specific harm. \" Default settings and user support (U2U)\"\\t\\tAgree with many of the proposals, but believe some may not be additive to existing protections already in place, and in some cases may detract from teen safety. Agree with many of the proposals, but believe some may not be additive to existing protections already in place, and in some cases may detract from teen safety. Encourage that such measures only be applied where they would be effective in promoting teen safety overall in the context of a given platform. \" Default settings and user support (U2U)\"\\t\\tDefault settings - this feature could increase risk to teens. Ofcom recommends platforms do not display lists of users that teens are connected with. For Pinterest and other platforms that do not include a user\\'s age in their profile, this feature may actually increase risk to teens, as the absence of follower and following lists would identify them to other users as being under 18. \" Default settings and user support (U2U)\"\\t\\tNotices for first time message from other users would not be effective. Ofcom recommends that teens be provided with a notice every time they receive a message from another user for the first time, reminding them that this is the first direct communication with the other user and advising them of their options regarding receiving the message or blocking the user. This measure would not be effective on Pinterest, however, as a mutual following relationship is required for a teen to receive a direct message from another user, making such a notice redundant. Governance and accountability \\t\\tSupports Ofcom\\'s approach to G&A - flexible and scalable\\tOfcom’s proposed approach to governance and accountability measures in the Codes of Practice for illegal content are generally flexible and scalable (and aligned with leading international risk management standards noted above), permitting tailored approaches to consumer protection as necessitated by specific use cases. Therefore, we generally support Ofcom’s proposals that would ensure the use of a risk-based approach and proportionality in complying with governance and accountability requirements. We agree with Ofcom’s decision to not yet make any recommendations regarding external audit requirements, or regarding linking remuneration and bonuses to online safety outcomes due to limitations in currently available evidence. Governance and accountability \\t\\tDisagrees with potential future measure on remuneration\\tThe App Association questions whether regulating the renumeration of senior managers is an optimal means of accomplishing HM Government’s goals and is concerned with the precedent such an intervention might set. Approach to the Codes\\t\\tSupportive of the codes in promoting compliance\\tGenerally, we expect the Codes of Practice to foster a culture of compliance by providing an easily understood roadmap that may be used for self-assessments.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922873973846436,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We appreciate alignment with widely relied upon scaled risk management practices captured in international standards (referenced above). Approach to the Codes\\tSegmentation\\tAgrees with proposal to apply onerus measures to large/medium/high risk\\tHM Government’s approach to addressing online harms should be flexible and scalable, permitting tailored approaches to consumer protection as necessitated by specific use cases. Therefore, we support Ofcom’s proposal to ensure the use of a risk-based approach and proportionality in regulatory practice. App Association members have limited resources and are unable to spend the large amounts of money on outside counsel and consultants that larger companies can access. Should HM Government take an approach that is too rigid, it will suppress the UK’s digital economy startups and small businesses to the advantage of larger incumbents and unduly limit access to digital economy startups and small business innovations from abroad, ultimately damaging the public interest. Approach to the Codes\\tDefinition of large services\\tAgrees with definition of large services\\tAgrees with definition of large services\\nApproach to the Codes\\tDefinition of multi-risk\\tAgrees with definition of Multi-Risk\\tAgrees with definition of Multi-Risk\\nContent moderation (User to User) \\t\\tAgrees with proposals\\tWe are supportive of Ofcom’s U2U content moderation proposals that provide needed flexibility for such good faith moderation activities. Content moderation (Search)\\t\\tAgrees with proposals\\tSame answer as Question 13\\n\" Default settings and user support (U2U)\"\\tChild Users\\tPartially agrees with proposals\\t\"We urge Ofcom to ensure its efforts to protect minors online align with the following recommendations: Any requirements for age verification must be sufficiently flexible to ensure that they are applied in a manner proportional to the risk data collection poses for children. How rigorous age verification should be—and therefore the amount and kind of data needed to accomplish it —must be tailored to the degree of certainty necessary to protect children. Laws and regulations should provide for periodic review of age verification technology solutions to make sure they are applied only where necessary for compliance and to identify how they might be refined to scale certainty to risk. They should also provide incentives for developers to continue to develop new approaches that optimise privacy. Incentives should also promote development of solutions that minimise the data required to establish a child’s age or age range – and encourage its disposal after age verification occurs. Laws and regulations should place age verification in the context of privacy by design. While ensuring that children are of legal age to consent to data collection is important, building into the design of interfaces and technologies greater transparency and privacy-enhancing practices may mitigate the risk posed by the collection of children’s data and minimise the need for age verification that potentially compromises privacy. \"\\nRecommender system testing (U2U) \\t\\tSupports proposal\\tWe support HM Government’s development of easily understood codes of conduct that may be used for self-assessments. Furthermore, we reiterate our support for HM Government encouraging companies, particularly the digital economy’s small businesses that the App Association represents, to attest to and document adherence to this code of conduct, in return receiving a safe harbour from liability under related online harms laws and regulations. Enhanced user control (U2U) \\t\\tCalls for maximum flexibility around this proposal\\tResponse: We urge for maximum flexibility on how controls are made known to users to permit U2U services to develop optimal user interfaces (and to evolve them with user preferences that evolve over time). Governance and accountability\\t\\tOnline safety should be policed by the state/upheld by common law. Criminal use of the internet should be monitored, policed and enforced by government agents under UK common law and not through the OSA. If service\\'s are found to have knowingly or negligently facilitated criminal or tortious activity, then they would be subject to prosectution or civil liability. Governance and accountability\\t\\tAddressing illegal harms via the common law would be better for small platforms. BitChute state that the ultimate responsibility for illegal harms do not lie with the services, but with a small minority of users who abuse them. BitChute state that they believe that cost of reviewing the consultation as well as addressing \"hypothetical\" harms is very high and does not necessarily address \"actual\" harms rooted in UK common law. Approach to the Codes\\t\\tBitChute does not agree that the entire range of harms listed should be censored. BitChute agrees with US scholar, Nadine Strossen, that for most online harms, including hate speech and disinformation, that hosting open discussions and free speech are the appropriate way to address and mitigate their impact. BitChute believe that censoring this topics through the OSA is problematic.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9764947295188904,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Approach to the Codes\\t\\tBitChute states that smaller services, branded as multi-risk are likely to struggle. The impact of the multi-risk classification on smaller services could harm competition in the UK and drive smaller services out of business, this is due to the cumulative impact of the duties. BitChute notes that the EU\\'s Digital Services Act is more propitious for smaller services. Approach to the Codes\\t\\tLarger services do not care for a free and open internet, compliance with the OSA will reduce UK users freedoms. BitChute stated that compliance with the OSA codes for safe harbour is likely to negatively impact UK users as automated moderation and surveillance will become common practice. BitChute state they think \"broad information notices\" issued to platforms will impact the privacy and control of UK users. Governance and accountability\\t\\tOther methods and responsible parties should be considered to counter harms\\tElement write that harms can and should be countered with offline interventions, something that the guidance doesn\\'t touch on but could promote. Additionally, services such as ISP\\'s should share some of the responsibility for safeguarding users. Approach to the Codes\\t\\tE2EE can coexist with safety, security and privacy measures. Element write that they do not consider that online safety, security and privacy are mutually exclusive and that they promote (and use) measures with their Matrix end-to-end-encryption protocol that also allows protections for their users. They state that Volume 2 suggests that smaller services (such as Element) identify that organisations in their early stages of being higher risk, Element dispute this as a generalisation as these services could use measures similar to Element, who have invested in specific Legal, Compliance and Trust and Safety team members. Element ask that a more in-depth consideration of the role of business models is considered instead of just the perception of early-stage organisations not likely prioritising user safety. Approach to the Codes\\t\\tBenefits of E2EE are not considered holistically, including protections for children. Element state that they encryption benefits everyone, including children, such as those living in unsafe homes who are looking for support. Element state that there is a lack of accompanying data and evidence to support claims made throughout volume 2 that E2EE is a standout risk. Element state that a  reference made to a Tech Against Terrorism Paper does not fully also highlight the papers recommendations for both law enforcement and companies offering E2EE to address this risk without the use of backdoors or measures that undermine encryption. Approach to the Codes\\t\\tThe presentation of harms beyond CSEA and Terrorism in relation to E2EE go beyond the scope of the OSA. Element writes that section 121 of the OSA limits the scope of Technology Notices to screen for potential CSEA material; however the harms consultation promotes a link between 12 categories of illegal content. If this approach is taken, it would expand the scope of section 121 considerably. Additionally, Element write that there is a contradiction between stating that E2EE raises risk levels, whilst promoting technologies for analysing user generated content as set out in the consultation would undermine the security of users by introducing vulnerabilities/backdoors. Content moderation (User to User) \\t\\tThere are contradictions around the definitions of public vs private communications. Element state that there is a contradiction reflected in the guidance between statements in Volume 4 14 and Annex 9, relating to the definitions of encryption scanning content in the context of public vs. private communications. Element state that some proposals proposed in 14 for services with mixed public and private encryption are a hybrid approach contrary to the introduction of section 14 which states they only apply to public communications on U2U services. \" Default settings and user support (U2U)\"\\t\\tClarification requested for whether services defaulting to E2EE by default should be considered private over public. \"Element have asked for clarification on Annex 9, specifically how A9.12 would apply to services where end-to-end encryption is present by default. Element have stated that it is not clear if it is meant to be inferred that all communications that default to using E2EE should be considered defacto private and not public? Additionally, Element have asked for clarifications on the size of groups using E2EE messaging: \"\"In technical terms, even communications between large groups would be private due to the use of encryption. However, in social terms, the larger the group, the lower would be the expectation of privacy. Is this the same logic applied by Ofcom in this distinction? If so, how does Ofcom determine the threshold at which communications move from being private to public?\"\" AND: \"\"Does the fact that content communicated via encrypted services require an account to be accessed (as opposed to content available on the open Web)\\ncount as a blanket restriction on who may access content?\"\"\"\\nApproach to the Codes\\t\\tThe Act and the IH proposals are missing consideration of decentralised services. Element state that Annex 15.27 has a gap regarding details or considerations for decentralised services, where no single accountable server or service has access to, or, controls the communications. Approach to the Codes\\t\\tThe Act and guidance will disproportionately impact privacy-centric technology services. The scale of the measures highlighted in the Act and the IH guidance are likely to disproportionately impact companies that focus on privacy and security technology. Content moderation (User to User) \\t\\tSmaller services are not set up to investigate risks of criminal/illegal exploitation of E2EE services. \"Smaller services are unlikely to be able to adequately assess and consider the risk of criminal/illegal abuse of E2EE on their services as listed in s 9(5)(c) of the OSA. This is because, many of the illegal harms noted do not have established image/hash bases to detect and would rely on text based analysis from the messages, this would require investigative work that smaller services are not equipped to undertake. Mega state that non-image based analysis of the 130 priority offences will disproportionately impact smaller services and especially those based outside of the UK who have limited knowledge of these offences. Mega provides the example: \"\" prostitution is not a crime in New Zealand. Holding Mega liable because \\nits E2EE U2U services were used to facilitate prostitution in the United Kingdom or because \\nit failed to properly assess the risk of such “harm” under the OSA is a bridge too far in Mega’s \\nview. Treatment of controlled drugs also varies widely in different jurisdictions.\"\" \"\\nAutomated content moderation (User to User) \\t\\tE2EE services make automated review of content impossible. Mega write that E2EE messaging means that any automated moderation, or analysis of the content for recommendations systems is impossible, as they are not compatible and the contents of the E2EE messages cannot be analysed. Approach to the Codes\\t\\tIt is unreasonable to consider E2EE services as multi-risk based entirely on E2EE. Mega state that despite the fact they cannot determine the content of E2EE messages, they are likely to be treated as a multi-risk service and that this is unfair as they lack many of the other high risk features of multi and large scale services, such as recommender service. Approach to the Codes\\t\\tCategorisation between low, specific and multi risk services does not make sense. The categorisation between low, specific and multi risk services does not make sense to Mega. Mega state they cannot imagine a service that would be \"specific\" risk, as they cannot imagine a service scoring medium risk for only one single possible illegal harms concern. Mega also state that there is no flexibility to account for smaller multi-risk services, and they will be subject to the same measures that large scale companies may be able to deal with. Mega ask if the breakdown between service type can be reconsidered. Governance and accountability  \\t\\tMeasures for governance and accountability are too onerous on small business and will disrupt business operations. Mega state that whilst they agree Ofcom should \"flex our expectations depending on the type of service we are dealing with\", the imposing of a Code of Conduct for all staff would be overly burdenson and is not a realistics buiness operational behaviour. Additionally, some of the processes to document measures such as 3B, 3C, 3D, 3E, and 3G are overly beaurocratic and provide little practical benefit. Content moderation (User to User)\\t\\tThe proposed moderation prioritisation and timelines are difficult and counter-productive to enforce. Mega state that measures 4D (prioritisation based on illegality and severity), are not practical to set up as their business would need to view the content first to triage it anyway. So all pieces of content would need to be reviewed via the same approach making the suggested triage process impossible (see notes for possible confusion). Governance and accountability\\t\\tCARE is concerned that the balance of the codes of practice is more concerned with organisations and reputational risk than the needs of people - it is not further updates to codes of practice that are required, but rather root and branch change. The document suggests that large companies will merely have to prepare an annual paper to comply with the duties of the Act and have their board scrutinise that paper to ensure compliance. Given many of the large user-to-user pornography sites do not take their duty to remove illegal material seriously, governance and accountability based on current practice, with additional reporting and scrutiny is not going to lead to action to remove illegal material from these platforms. Outlined various examples of the failures of current systems by Pornhub, and a study by Durham. CARE is concerned that it will take more than the £16,000 to £36,000 per year cited at paragraph 8.31 of volume 3 to fix the problem. The governance and accountability measures currently utilised by large pornography websites are broken and it is clear that a victim/survivor centred approach is needed to ensure that material is removed, but also that processes are put in place to ensure the material does not appear on the platform in the first place. This will require a radical overhaul in how pornographic websites conduct their business. Currently there is limited moderation for onboarded content to the site, either by technology or through human screening of videos (outlined stats of moderation at Mindgeek). It is not further updates to codes of practice that are required, but rather root and branch change. Ofcom must bring forward proposals that are robust, the current proposals will not deal with illegal harm on platforms that onboard pornographic content. Governance and accountability\\t\\tConcerned with the proposal to treat large platforms that host user-to-user pornography differently to smaller platforms. Concerned with the proposal to treat large platforms that host user-to-user pornography differently to smaller platforms. The reality is that online pornography is largely dominated by a small number of operates that run multiple platforms (quote/evidence from APPG on Commercial Sexual Exploitation). Concerned that large pornographic websites may be incentivised not to accurately report their users or to open multiple websites to drive traffic away from a main site. Given the unusual structuring of the pornography industry, a platform that appears to be small or medium in size, can in fact be part of a much wider corporate entity. The highest standards and duties should apply to all pornography websites and corporate entities should be required to report owned and linked entities as part of the reporting process. Approach to the Codes\\t\\tSafe harbour approach is particularly concerning when applied to pornographic websites. User-to-user sites and social media that host pornographic content have failed to deal with illegal content and harm on their platforms – i.e. the outcome for users. The codes appear to be an input based ‘tick-box’ exercise. It is concerning that a platform may merely show willing and that is enough to potentially exempt them from enforcement. The harm from the publication of CSAM and illegal/extreme pornography is high, therefore content providers must be held to a higher standard than simply ticking a box. CARE is concerned that illegal material and CSAM on content providers hosting pornography will not be removed. This approach disincentivises content providers to do anything innovative/other than implement the minimum standard. Content moderation (User to User) \\t\\tA safety by design approach holds the services responsible for the safety of users by assessing, addressing, and mitigating potential harms before they occur. This is a shift in the focus from the proposed codes of practice which focus on mitigation after the harm has been onboarded, rather than prevention. Content moderation is focused on content take down/removal, rather than prevention. There is no obligation to ensure that the service implements measures in the design of the platform that mitigate risk. A safety by design approach holds the services responsible for the safety of users by assessing, addressing, and mitigating potential harms before they occur. This is a shift in the focus from the proposed codes of practice which focus on mitigation after the harm has been onboarded, rather than prevention. Approach to the Codes\\t\\tWants more focus on safety by design and Children’s Design Code. We echo the response of OSTIA re the need for a greater focus on Safety by Design and the legal obligations in the Children’s Design Code. Content moderation (User to User) \\t\\tNotes that a platform that indicates compliance with hash matching might have a completely unresolved problem with live streamed CSAM which is a risk - Majority of harm to vulnerable children is on live streaming \\tThe majority of the CSAM harm the children we represent experience is via live streaming. These children and young people need the greatest possible emphasis on how platforms respond to this medium of harm. A platform that indicates compliance with hash matching might have a completely unresolved problem with live streamed CSAM and therefore publishing apparent compliance with CSAM hashing measures might indicate a level of safety that the platform cannot afford users. Content moderation (Search)\\t\\tIn their experience, CSAM harm children  via live streaming. A platform that indicates compliance with hash matching might have a completely unresolved problem with live streamed CSAM \\tAs above, the majority of the CSAM harm the children we represent experience is via live streaming. They need the greatest possible emphasis on how platforms respond to this medium of harm. A platform that indicates compliance with hash matching might have a completely unresolved problem with live streamed CSAM and therefore publishing apparent compliance with CSAM hashing measures might indicate a level of safety that the platform cannot afford users. User reporting and complaints (U2U and search) \\tAppeals \\tReporting and complaints must be very easy to understand for vunerable children\\tFor these proposals to be effective for the children we represent they must be able to access any appeals process to record their complaint directly with OFCOM directly from the app they are using (“if you are not happy with our response click here”). This must be clear and explicit. Anything else will be too complicated for it to be accessible to them. User reporting and complaints (U2U and search) \\tAppeals \\tCurrent reporting process ineffective ;chilren expereince no response or an inadequate response and no way tp challenge these. Also choldren need offline support to deal with trauma. \"The children we work with regularly report to us that current reporting processes are ineffective and that they feel they have no way to challenge these.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9678506255149841,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'They have experienced making multiple reports in relation to serious incidents and receiving no response or an inadequate response. They need an immediate opportunity to register this (whilst acknowledging that OFCOM will not be in a position to respond to individual complaints) that provides them with independent support outside of the platform. Receiving all advice on the platform will not adequately protect them or their rights. They have asked us to communicate that they are likely to need to receive support away from the direct digital environment where the trauma occurred.\"\\nGovernance and accountability  \\tAlso covers Reporting\\tHighlights that effectiveness of expedient information sharing of child safeguarding is cruital. Questions how this will be shared effectively and in a timely amnner with law efforcement. Questions how the proposals will monitor the specific process of the platforms directly informing law enforcement of illegal content they have tracked or monitor whether this has been done in a timely manner. Reporting should be tailored for the different countries in UK to allow service in each country to respond effectively.Highlights that effectiveness of expedient information sharing of child safeguarding is cruital. Approach to the Codes\\t\\tAdvocate for a dynamic and flexible approach when it comes to prescribing the specific techniques to use for moderation and content removal\\t\"[go through that they agree with Ofcom\\'s overall approach]. However, Stop Scams UK, in line with the views of our members affected by the proposals, advocate for a dynamic and flexible approach when it comes to prescribing the specific systems and techniques organisations should use for moderation and content removal. While techniques such as keyword search and hash matching are useful tools, many of the largest ‘category 1’ services will already use these techniques to a degree, and in some instances have already developed more efficient and effective technologies.\"\\nApproach to the Codes\\t\\tAsk for adequate flexibility in duties that prescribe specific technologies or systems\\tFurthermore, scammers are dynamic and sophisticated criminals, and are often able to find ways to  bypass or beat detection and moderation systems through ingenuity and innovation. We would therefore suggest Ofcom ensures that there is adequate flexibility in duties that prescribe specific technologies or systems in order to enable organisations who are on the front line to keep pace with the sophisticated and evolving nature of online scams and fraud in the UK. For this reason, Stop Scams UK underscores the need for continuous collaboration, monitoring, and adaptation throughout the process of the duties implementation, to take account for the dynamic and evolving nature of scams and other types of online harm. We also recognise that the regulatory approach is untested so urge caution and a proportionate approach to implementation. Approach to the Codes\\t\\tAppreciates our approach\\t\"Stop Scams UK acknowledges and appreciates Ofcom\\'s structured and systematic \\napproach to developing the Illegal Content Codes of Practice, starting with the most egregious harm where most regulatory gain is to be had. It is evident that the proposed recommendations aim to address the diverse range of illegal harms prevalent in online spaces, promoting a safer digital environment for users.\"\\nApproach to the Codes\\tSegmentation\\tAgrees with segmentation of measures\\t\"Stop Scams UK largely agrees with the proposal to apply the most onerous measures in \\nthe Codes to services that are large and/or medium or high risk. This approach ensures that regulatory interventions are proportionate to the potential scale of the harm found on the services. This approach prioritises user safety while considering the operational capacities of different platforms. We believe that this approach strikes the right balance between the needs of business and the urgent need to ensure that consumers are better protected, and the gateways available to fraudsters are closed\"\\nApproach to the Codes\\tDefinition of large\\tAgrees with definition\\t\"Stop Scams UK supports the definition of large services proposed by Ofcom, which \\naligns with international standards such as those outlined in the Digital Services Act by the EU. This alignment facilitates consistency across regulatory frameworks and minimises the compliance burden on services operating in multiple jurisdictions. We believe this is crucial to help facilitate and enable common and shared approaches to tackling scams internationally.\"\\nApproach to the Codes\\tDefinition of multi-risk\\tAgrees with definition\\t\" Stop Scams UK agrees with the definition of multi-risk services proposed by Ofcom. Identifying services as multi-risk based on their susceptibility to multiple kinds of illegal harms demonstrates that Ofcom recognises certain services may pose diverse risks to users, that are not solely based on their size, requiring different mitigation strategies.\"\\nContent moderation (User to User) \\t\\tThink we need to provide more flexibility\\t\"In line with our members who run category 1 U2U services, Stop Scams UK supports a \\ndynamic and flexible approach to content moderation, ensuring that platforms remain equipped to combat forms of online harm that are likely to evolve quickly and, potentially, in unanticipated ways, effectively. The methods set out by Ofcom include hash matching and keyword search. We note the in the case of many large search services, these methods are either already in use or have been replaced by more complex and sophisticated moderation techniques. Ofcom will need to ensure that it takes account of such developments. As we note elsewhere in response to this consultation, fraudsters and scammers are dynamic and sophisticated criminals. They use innovative techniques and are often able to find ways to bypass or beat detection and moderation systems. We therefore suggest Ofcom make sure there is adequate flexibility in duties relating to search moderation, to make sure that the organisations who are on the front line, are able to keep pace with the sophisticated and evolving nature of scams and fraud. \"\\nContent moderation (Search)\\tFlexibility\\tAdvocate for more flexibility\\t\"In line with our members who run category 1 Search services, Stop Scams UK advocates \\nfor a dynamic and flexible approach to content moderation, ensuring that platforms remain equipped to combat evolving forms of online harm effectively. We refer Ofcom to the responses of those members which include Amazon, Google and Meta\"\\nAutomated content moderation (User to User) \\tFlexibility\\tAdvocate for more flexibility\\t\" In line with our members who run category 1 U2U services, Stop Scams UK advocates \\nfor a dynamic and flexible approach to content moderation, ensuring that platforms remain equipped to combat evolving forms of online harm effectively.\"\\nContent moderation (Search)\\tFlexibility\\tAdvocate for more flexibility\\t\" In line with our members who run category 1 Search services, Stop Scams UK advocates \\nfor a dynamic and flexible approach to content moderation, ensuring that platforms remain equipped to combat evolving forms of online harm effectively.\"\\nRecommender system testing (U2U) \\tSupport\\tSupport the measure\\t\"We generally support Ofcom\\'s proposal for U2U services to collect safety metrics \\nduring on-platform tests of their recommender systems, especially for those identified as medium or high risk for specified harms. Stop Scams UK is supportive of these measures. These safety metrics will enable services to assess whether changes to their recommender systems increase user exposure to illegal content, thereby allowing them to make more informed design choices and mitigate online harm effectively.\"\\nEnhanced user control (U2U) \\tSupports\\tSupport the measure\\t\" Stop Scams UK is strongly in favour of initiatives that give consumers greater control of \\ntheir online experience. Empowering users with greater agency over what they see, who they can block and who gets to interact with their content, is a crucial part of rebuilding consumer trust in the UK’s digital environment\"\\nGovernance and accountability  \\tGovernance measures\\tAgree with our governance measures\\t\" Stop Scams UK supports Ofcom\\'s proposals regarding governance and accountability \\nmeasures as set out in the illegal content Codes of Practice. Robust governance processes are crucial for effectively identifying and managing online safety risks, particularly in the context of combating scams. By ensuring services are accountable to senior governance bodies and mandating the implementation of measures such as written statements of responsibilities for staff, tracking evidence of illegal content, and establishing Codes of Conduct for employees, these proposals provide a structured framework for embedding accountability, that will help ensure services are proactive in managing the risks and protecting users from online fraud and scams. \"\\nGovernance and accountability  \\tGovernance measures\\tSpeak about our segmentation of governance measures\\t\"Stop Scams UK agrees with the types of services that Ofcom proposes the governance \\nand accountability measures should apply to. These measures are essential for ensuring that all services, regardless of size, play a role in mitigating online harms and combating scams. We recognise the importance of a proportional approach, which minimises the regulatory burden on smaller platforms, but are clear on the role that even smaller services can play in helping halt (or enable) the dissemination of fraudulent content. Therefore, the inclusion in the duties, of a requirement for a senior accountable officer for all services, including U2U and search services, helps promote accountability across the digital landscape. By implementing these measures universally, Ofcom is able to foster a collective responsibility among all service providers to uphold online safety standards and protect users from scams.\"\\nApproach to the Codes\\t\\tWhilst the approach is sensible, these services should report on key compliance costings to inform whether some measures could be applied to a broader suite of services in the future. The approach seems sensible. However, it would be beneficial for these services to report on key costings of their compliance with the Codes, so Ofcom can consider whether it would be viable and proportionate to widen the services included in the \\'most onerous\\' measures at a later date. This would be particularly helpful if a small or micro business is known to have repeated harmful content on its platform but is not considered to be a large / medium or hgih risk. Approach to the Codes\\t\\tSeveral measuresa re scoped too narrowly to CSEA or terrorism; other illegal harms should be brought into scope of these measures. There are multiple measures designed to mitigate the online harms, however several measures are scoped too narrowly to Child Sexual Exploitation and Abuse (CSEA) or terrorism only. Other illegal harms should be brought into scope including, as a minimum, indicators of these illegal harms, and should be treated as a significant risk factor to trigger these measure being undertaken. Approach to the Codes\\t\\tThis definition appears to be sensible and would bring a number of services in scope of various measures that might not be captured by the \\'large services category\\'. This definition appears to be sensible and would bring a number of services in scope of various measures that might not be captured by the \\'large services category\\'. Approach to the Codes\\t\\tOfcom should introduce a new \\'high volume\\' category or allow a flexible aoorach to user base definition in a specific area to allow for platforms who present a high volume risk of a single form of illegal content to be subject to more onerous measures. The Codes of Practice should have the flexibility to apply more onerous measures to platforms who present a high-volume risk of a single form of illegal content, either via a new ‘high volume’ category or by allowing for a flexible approach to user base definition in a specific area. Approach to the Codes\\t\\tA larger proportion of services should be included in the definition of \\'large services\\' e.g. to ensure that dating services, whee most romance scams are proliferated, are in scope. While Ofcom explains why it is restricting some measures to large services (e.g., cost) it would be beneficial for consumers if a larger proportion of services was included. For example, currently most dating platforms would not be in scope of the measures, however most romance scams are proliferated on dating services which can lead to other illegal activities. User reporting and complaints (U2U and search) \\t\\tLarger financial institutions should be included within the list of \\'trusted flaggers\\' and Ofcom should provide more clarity on what a flagger system looks like in practice\\t\"LBG supports the principle behind a trusted flagger system;  as U2U services do not have priority systems in place. To deliver a DRC in practice, trusted flaggers would require a near real-time anonymised information-sharing platform. LBG has published data on romance scams; these increased by more than 22% last year, with the average amount lost per victim being £6,937. If LBG were listed as a trusted flagger, the speed of reporting and take-down of harmful content / profiles would increase dramatically. LBG is also seeking further clarity on what a flagger system would look like in practice, specifically what data would be shared and how. Ofcom should also consider a maximum time limit to investigate / take-down fraudulent adverts or accounts once a report is received. \"\\nAutomated content moderation (User to User) \\t\\tReal-time use of AI or word pattern analysis would be more beneficial than the keyword detection measure. Real-time use of AI or word pattern analysis would be more beneficial than the keyword detection measure. Automated content moderation (User to User) \\t\\tOfcom should, where necessary and proportionate, should require technically feasible comprehensive action to tackle fraudulent content (noting the additional protections suggested for children in encrypyed environments). Keyword detection requirements are less likely to be effective for fraud; they cannot be used in private / E2EE encrypted environments and language used in more public U2U services is less obviously fraudulent. Whilst Ofcom cannot require online services to use proactive technology on private communications, there are powers in the OSA that enable ofcom to, where necessary and proportionate, require technology companies to make best enfeavours to develop our source technology to protect consumers from CSEA content. In addition to dealing with CSEA material, LBG encourages Ofcom ensure the Codes also require tech companies to take comprehensive action and introduce the most effective measures that are technically possible and feasible ot tackle fraudulent content. Automated content moderation (Search) \\t\\tSuggested including the free online scam website checker in audtomated search content moderation and de-indexing. An easy and cost-effective solution could include known scam websites in automated search content moderation and de-indexing by linking to the free online scam website checker data. Free Website Scam Checker - Check a website by Get Safe Online. Automated content moderation (User to User) \\t\\tAutomated keyword detection: This measure may have limitations given how quickly fraudsters adapt; Ofcom should consider how frequently the word lists are updated. Different platforms will have different keywords to input, which should be monitored carefully  along with relevant fraud trends.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9958564639091492,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Fraudsters will adapt their approach once certain words are identified, so ongoing monitoring will be necessary. It is not clear how successful the technology will be - this will need to be monitored on an ongoing basis. Furthermore, fraud isn't always obvious (most fraud does not advertise that it is a fraud or 'too good to be true.') So Ofcom should think about how frequently the word library should be updated - i.e. along with certain trends.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9934819340705872,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'U2U services could deliver this using AI in real-time, for instance to keep up with viral moments and understand new terms being used. A good example of this would be the trend of “Elon Musk” and “Martin Lewis” crypto scams, where seemingly ‘verified’ accounts have posed as an influential figure to encourage victims to invest in certain cryptocurrency scams. Automated content moderation (User to User) \\t\\tUser profile risk monitoring, combined with prompts to users who\\'ve interacted with a fraudulent profile, could be more effective than keyword detection. Larger U2U services have the capability to mark \\'false information\\' warnings on posts, which could be spreading \\'fake news\\' and use image matching to find the same information. These posts are often taken down and user profiles are either \\'shadow banned\\' or disabled. Ofcom could encourage larger tech companies to implement similar steps to alert users of potentially fraudulent adverts, either once they\\'ve been reported or if they fit a criteria (e.g. consistently mentioning cryptocurrency and Elon Musk in the same post). ofcom could also encourage larger technology companies to alert users of new content indicating someonone is selling something / promoting a service that\\'s from a new account and is \\'too good to be true.\\' E.g.alerting a Facebook marketplace user when they are communicating with a seller who is using a new profile without a profile picture or many friends. If profiles continued to post these ads consistently then they should be removed. Enhanced user control (U2U) \\t\\tGiven criminals\\' abuse of some monetised verification schemes, platforms should have more checks in place to confirm the true identity of the user. There is evidence that some monetised verification schemes are being abused by criminals to build their credibility. Specifically X/Twitter accounts have the capability to impersonate an influential figure and pay a subscription fee per month, with potential to post misleading information to trick victims into believing they are giving financial details or investing in something authentic. We strongly believe there should be more checks in place by social media platforms to confirm the true identity of the user. The Government’s digital ID Trust framework is a method that this could be implemented while avoiding regulation. User reporting and complaints (U2U and search) \\t\\tTier One banks / banks who provide business and personal current accounts should be listed as trusted flaggers and there should be further clarity on what this system looks like in practice. Dedicated reporting channels: Lloyds Banking Group and other Tier One regulated banks and/or banks who provide business and personal current accounts should be listed as Trusted Flaggers. To collaborate effectively and proactively target criminal networks and individuals, across industry, legal entities, and Government agencies, would require a near real-time anonymised information sharing platform. The existing model is constrained by regulation (Data Protection) and inhibits the ability to share across sectors for effective action. The information payment services providers could receive from technology companies and social media platforms, and reciprocally the information we could share, could improve the collective investigation, detection, and prevention of economic crime. New legislation to share information may also need to be considered to enable and mandate this. We would ask for further clarity on what a trusted flagger system could look like, what data would be shared, and how. We would be happy to work alongside Ofcom to find a suitable solution. Our approach to the Codes\\t\\tSupportive of approach, which is commensurate with international payment scheme requirements. Supportive of our overarchign approach, proportionality and definitions. Noted the commensurate nature of approach with international payment scheme requirements, need for controls to be commensurate to inherent risks and alignment with DSA (on large service definition) \\nOur approach to the Codes\\t\\tControls should be commensurate to inherence risks, be they impact or instance\\tControls should be commensurate to inherence risks, be they impact or instance\\nOur approach to the Codes\\t\\tAgreed - aligned with DSA\\tAgreed - aligned with DSA\\nOur approach to the Codes\\t\\tAgreed with definition \\tAgreed with definition - it will ensure appropriate controls are designed to protect from specific harms. Content moderation (User to User) and Content moderation (Search)\\t\\tSupportive of proposals, which are in line with existing practice\\tSupportive of proposals, which are in line with existing practice\\nContent moderation (User to User) and Content moderation (Search)\\t\\tSupportive of our proposals, which are a reasonable preventative control\\tSupportive of our proposals, which are a reasonable preventative control\\n\"Automated content moderation (User to User)\\n\"\\t\\tSupportive of our proposals, which are in line with industry best practice and readily available technologies. Supportive of our proposals, which are in line with industry best practice and readily available technologies. \"Automated content moderation (User to User)\\nAutomated content moderation (Search) \"\\t\\tSupportive of our proposals, which are in line with industry best practice and readily available technologies. Supportive of our proposals, which are in line with industry best practice and readily available technologies.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923529028892517,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'User reporting and complaints\\t\\tSupportive of our proposals and proportionate approach. It is proportionate for larger or higher risk service providers to use these channels as part of their overall detection control frameworks and as an additional check on other controls. Terms of service and publicly available statements\\t\\tSupportive of our approach, which reflects standard business practice\\tSupportive of our approach, which reflects standard business practice\\nDefault settings and user support for child users (U2U) \\t\\tSupportive of our approach \\tProposed measures seem reasnobale to help protect children and help them help themselves. Recommender system testing (U2U)\\t\\tSupportive of our approach\\tMeasures povide extra layer of control\\nEnhanced user control (U2U)\\t\\tSupportive of our approach. Should include information on how to report content\\tSupportive of our approach. Should include information on how to report content\\nEnhanced user control (U2U)\\t\\tAgreed that first two proposed measures should include requirements for how controls are made known to users. Agreed that first two proposed measures should include requirements for how controls are made known to users. User access to services (U2U)\\t\\tAgreed with our proposals. Measures seem proportionate and reasonable \\nUser access to services (U2U)\\t\\tVerified CSAM dissemination should lead to permanent disqualification; a more proportionate approach could be taken for mistakes / boderline cases\\tIf a categorisation of the infraction severity exists this may make sense in the case of ‘mistakes’ or borderline cases but in general my view is that verified CSAM dissemination by a user should disqualify that user permanently. User access to services (U2U)\\t\\tInstitute an appeals process\\tInstitute an appeals process\\nService design and user support (Search) \\t\\tAgreed with our proposals. Measures are effective preventive controls\\nCumulative assessment\\t\\tAgreed with our proposals.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9913288354873657,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Risk of harm must be addressed, rather than service provider's ability to design and implement controls. Cumulative assessment\\t\\tAgreed with our proposals. Risk of harm must be addressed, rather than service provider's ability to design and implement controls. Cumulative assessment\\t\\tAgreed with our proposals. Larger services providers are already deploying resources to provide controls and compliance assurance.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.7677866816520691,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Statutory tests\\t\\tAgreed with our proposals. Recommendations take account of the principles that the statue (sic) requires\\nGovernance and accountability  \\t\\t5Rights Foundation suggests Ofcom should recommend that the roles the Institute of Electrical and Electronics Engineers Standards Association have identified as essential to delivering safety by design services as best practice\\t5Rights Foundation state that whilst they recognise placing accountability on senior staff to ensure compliance, design descitions that impact the safety of children should not be the sole responsibility of top management. They then go on to recommend Ofcom should recommend that the roles the Institute of Electrical and Electronics Engineers Standards Association have identified as essential to delivering safety by design services as best practice. The argue that the research and expertise in this report relate to age appropriate design, it underpins and speaks to the objectives of the illegal harms duties. Governance and accountability  \\t\\t5Rights Foundation argue Ofcom should recommend that when new kinds of illegal content or increases in illegal content are identified, this should be taken to the board as well as reported to senior management\\t5Rights Foundation are concerned that many tech companies to date have harbored internal research and information regarding the harms on their services which they have not acted upon. They say they support the recommendation that services track and report evidence of new kinds of illegal content on services, and unusual increases in particular kinds of illegal content to senior management, but argue this recommendation could be strengthened by putting an obligation on services to also report this to the board. Approach to the Codes\\t\\t5Rights Foundation state they are critically concerned that Ofcom has interpreted 'measures' as tools rather than systems and processes\\t5Rights Foundation argue that by interpreting 'measures' as tools rather than systems and processes it means that services can be considered compliant even when the service is risky by design. They recommend Ofcom should put processes that are iterative and have outcomes so that the measure or mitigation is not deemed adequate until the outcome has been achieved to be in line with what the government assured would be a systems and processes regime. Approach to the Codes\\t\\t5Rights Foundation state they are concerned that Ofcom places a focus on the cost to the services complying with the measures, rather than focusing on the cost to the potential victims of offences and illegal content the service causes\\t5Rights Foundation argues that Ofcom places and undue focus on the cost of the services complying with the measures which they claim runs counter to the stated objectives of the Act. 5Rights Foundation argue that by Ofcom focusing too heavily on measures being 'proportionate' and not taking a victim centred approach, the regime may result in being regressive whereby companies that alrrady comply pull back their investments in safety and those that do nothing make minimum investments. They further argue that measures should be as far as possible expressed in processes that iterate until the goal has been achieved. Approach to the Codes\\t\\t5Rights Foundation argue that the current drafting of the Codes does not account for smaller high-risk services which host illegal content and facilitate illegal activity\\t5Rights Foundation state they are concerned that as the Codes are currently drafted, a number of small high-risk services that are dedicated to single issues themes included in the illegal harms and offences are not covered. They use a quote by Lord Minister said during the passage of the OSA in which they reassured parliamentarians that these types of services would be included. They then state that the Codes should follow that where there is risk of illegal harm or harm to children, online safety regulation applies to any and all online services, irrespective of the nature or size of the service. Approach to the Codes\\tThird-party auditing\\t5Rights Foundation state they are supportive of a future measure where services are audited by an independent third-party \\t55Rights Foundation state that third-party adititing measures should have been part of the first iteration of the Codes. They use quotes by Meta whilstleblowers such as Frances Haugen and Arturo Bejar arguing that Meta are often caught in conflicts between its own profits and users safety, and that by Meta doing 'their own homework' it means they can downplay the prevalence of harmful content. 5Rights Foundation state that the purpose of having a regulatory regime is to bring an end to self-regulation, and that due to the severity of the harms dealt with in this consultation, independently assessed auditing of illegal content risks would be appropriate. Approach to the Codes\\tRenumeration for senior managers\\t5Rights Foundation state that renumeration for senior managers is a corporate responsibility and not an individual one\\t5Rights Foundation state that renumeration for senior managers is a corporate responsibility and not an individual one and further argues that to drive safety by design, process meansures need to be set out that have clear outcomes and allow companies to iterate to those outcomes. User reporting and complaints (U2U and search) \\t\\t5Rights Foundation argue that many users online will not report it when they are victims of offences and include evidence\\t5Rights Foundation argue that many users online will not report it when they are victims of offences, particularly vulnerable groups such as children,  young people and women. They point to evidence by UCL and NSPCC that argues that users do not think reporting works or users feel ashamed to report. Approach to the Codes\\t\\t5Rights Foundation state they do not agree with the proposal that the most onerous measures in Codes shoud only apply to services that are large and/or medium or high risk\\t5Rights Foundation raise in their response that they are concerned Ofcom has placed too much onus on cost of services when complying with the regulation rather than the cost of the outcome of harm to victims. They therefore propose that who the most onerous measures in the Codes should first of all be based off the severity of risk, followed by the scale of risk and then how the risks intersects to create new risks. Approach to the Codes\\t\\t5Rights Foundation state they do not agree with Ofcom's definition of large services\\t5Rights Foundation state they were shocked by the definition as it copies the EU DSA in this regard but not in ways in which it offers a stronger regime. The further suggest that Ofcom carries out polling of the general public as they believe most of the public would believe services such as Roblox and Fortnite would be considered large services. They argue services with 2 million users should be considered large and state that if a service is a 'small service' it does not mean it is a safe service. Approach to the Codes\\t\\t5Rights Foundation state they would like to see Ofcom give further detail as to how regularly services will need to review their user-base to determine their size and when new measures need to be implemented \\t5Rights Foundation state they would like to see Ofcom give further detail as to how regularly services will need to review their user-base to determine their size and when new measures need to be implemented \\nApproach to the Codes\\t\\t5Rights Foundation argue that by Ofcom approaching risk by linking it solely to particular offences, it disincentivises services from thinking holistically about how systems, features and functionality can lead users to harm online and create risk\\t5Rights Foundation argue that by Ofcom approaching risk by linking it solely to particular offences, it disincentivises services from thinking holistically about how systems, features and functionality can lead users to harm online and create risk\\nApproach to the Codes\\t\\t5Rights Foundation raises the proliferation of small services that are decicated to one subject within the scope of the offences of the Act and how these services should be in scope, but as it stands are not\\t5Rights Foundation raises the proliferation of small services that are decicated to one subject within the scope of the offences of the Act, like for instance websites commercialising the production of gen AI CSAM (evidence from IWF included) or forums decidated to suicide and self-harm ideaton that have been linked to deaths (links to relevant research). They argue that as it stands, these types of services will not be in scope as Ofcom are associating risk with particular offences rather than the outcome of a risk assessment further undermines the possibility of understanding how a service could effectively mitigate that risk. Approach to the Codes\\t\\t5Rights state they are concerned with Ofcom's 'ex facto approach' and our interpretation of measures as tools rather than systems, claiming this could mean services are considered compliant even when the desired outcome has not been achieved\\t5Rights suggests that Ofcom should set out processes that are iterative and have outcomes so that the measure or mitigation is not deemed adequate until the outcome has been achieved. They believe this would be more true to the systems and processes approach and would eliminate the risk of services being considered compliant when they are not. They reference the Age Appropriate Design Code (AADC), which is a mixture of outcomes-based and prescriptive standards and has resulted in tech companies such as Google, TikTok and Meta having brought many design changes. Approach to the Codes\\t\\t5Rights state that proportionality should be based on severity of risk and not size or other variables\\t5Rights state they are concerned that many of the Codes will only be required to be implemented by the largest services with high/medium risk. They state that proportionality should be based on severity of risk and not size or other variables such as costs to companies etc. Approach to the Codes\\t\\t5Rights state they are concerned Ofcom do not propose to require many high risk services that the child user measures apply to have age assurance in place\\t5Rights are concerned a lot of high risk services will be out of scope when it comes to implementing age assurance. They reference the Age Appropriate Design Code (AADC), which sets out that services that are 'likely to be accessed' by children threshold test that services are already required to meet if they let children onto their service. 5Rights arguethat the services that Ofcom recommend child user measurs for should also be required to have age assurance in place. Approach to the Codes\\t\\t5Rights say they are confused as to why the child user measures  will only be required of services at risk of grooming when age is set out as a risk factor with regards to many other offences\\t5Rights say they are confused as to why the child user measures  will only be required of services at risk of grooming when age is set out as a risk factor with regards to many other offences. They argue Ofcom should require default privacy settings for all services, as required under the Age Appropriate Design Code (AADC) and also expand this list as per the additional functionality set out elsewhere in the response. Approach to the Codes\\t\\t5Rights state that the cost assumptions for human moderators do not seem to account for the fact that many services outsource moderator roles to countries where salaries can be considerably lower\\t5Right state that Ofcom should consider in its cost assumptions for human moderator roles the fact that many services outsource these roles to countries where salaries can be considerably lower. They link to a piece of academic research on this in their response. Content moderation (User to User) \\t\\t5Rights say they are unsure why Ofcom instructs services to only take down illegal content if if its terms and conditions expressly detailed that it is not allowed\\t5Right refer to measure 1 and para 12.80 (I belive it is rather measyre 4A and paras A4.1-A4.5) where Ofcom recommends that 'Where the provider is satisfied that its terms and conditions for the service prohibit the types of illegal content defined in the Act which it has reason to suspect exist, consider whether the content is in breach of those terms of service and, if it is, take the content down swiftly'. They state they are unsure why services are only told to remove illegal content if it is expressly detailed that it is not allowed on their service in their terms and conditions. They state that although services will beed to uphold their terms and conditions, the services are still subject to law so this should override the terms and conditions. Content moderation (User to User) \\t\\t5Rights state that performance targets are often skewed to play down the prevalence of harmful content so Ofcom should set outcomes for that the performance targets should achieve\\t5Rights argue that info revealed by whilstblowers have shown that at Meta for instance, they would set their own standards for harmful content to play down its prevalence. 5Rights therefore recommend Ofcom should set outcomes for that the performance targets should achieve so internal performace targets cannpt be skewed by services. Content moderation (User to User) \\t\\t5Rights welcome the recommendation that moderators should be trained, but want Ofcom to include that this training needs to identify risks to child safety\\t5Rights state that knowledge of risks to different groups of children and the full range of content and activity that is illegal or might be harmful to a child is something moderators need to be trained on. They urge Ofcom to include this in our recommendation on content moderation training. Content moderation (Search)\\t\\t5Rights argue that although content moderation is an important ex post facto measure, up stream measures should be prioritised to prevent illegal content and activity to take place on services\\t5Rights state they are concerned that the draft Codes does not make reference to minumum standards of use which may mean it is not consistently applied across services. Further to this they argue that content moderation is an ex post factor measure and this should not be prioritised over up stream measures that prevent illegal content and activity. They argue this tool should serve as once alongaide many other measures for services to be able to tackle illegal conten and activity. Automated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights state they work with the WeProtect Global Alliance which consists of governments, civil society, and industry. At WeProtect they work to develop policies and solutions to protect children from sexual explitation and abuse online and encourage Ofcom to look into the emerging research from the coalition on how to tackle CSAM and CSEA online. Automated content moderation (User to User) \\t\\t5Rights argue Ofcom are not clear enough about what constitutes 'privately' or 'publicly' communicated content\\t5Rights state that as Ofcom are not clear enough about that constitutes 'privately' and 'publicly' in the guidance, this will be left to determine by serivce themselves and lead to inconsistent application. They argue it is important that Ofcom provides clarity on this to ensure the codes of practice can be effectively complied with. They suggest to do so, the guidance can provide a threshold for how many users should be able to access content for it to be considered 'public'. Further they recommend the guidance would benefit from test case examples fo services and for services to be reminded of their duties under GDPR and the Age Appropriate Design Code. Automated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\nAutomated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\nAutomated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\nAutomated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\nAutomated content moderation (User to User) \\t\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\t5Rights recommend Ofcom look into the emerging research from the WeProtect Global Alliance on measures to tackle CSAM and CSEA online\\nAutomated content moderation (Search) \\t\\t5Rights argue that although content moderation is an important ex post facto measure, up stream measures should be prioritised to prevent illegal content and activity to take place on services\\t5Rights state they are concerned that the draft Codes does not make reference to minumum standards of use which may mean it is not consistently applied across services. Further to this they argue that content moderation is an ex post factor measure and this should not be prioritised over up stream measures that prevent illegal content and activity. They argue this tool should serve as once alongside many other measures for services to be able to tackle illegal conten and activity.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.996058464050293,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'FInally, they state they support NSPCC\\'s response with regards to these recommended measures. User reporting and complaints (U2U and search) \\t\\t5Rights state they broadly agree with Ofcom\\'s reporting and complaints proposals and otuline what they particularly welcome\\t\"5Rights state that they braodly agree with Ofcom\\'s proposals on reporting and complaints, in particular that the additional work on how to support child users e.g.:\\n- Reporting should allow for contextual info to be supplied - they argue this is particularly important for content harmful to children as in isolation it may not meet a threshold but in broader context is is harmful to children\\n- Reporting should be available to non-registered users - they state that through consultation with the Bereaved Families for Online Safety, the inability to raise a complaint as a non-registered user or when logged in was raised as a key issue. \"\\nUser reporting and complaints (U2U and search) \\t\\t5Rights suggest Ofcom require services to provide info as to why a descision has been made on content\\t5Rights state that in consultation with the Bereaved Families for Online Safety, it has been raised that services are often not transparent about how decisions are made regarding content, even when it is perceived to be in breach of ToS, the service sometimes disagrees it is. They therefore recommend Ofcom require services to provide info as to why a descision has been made on content. User reporting and complaints (U2U and search) \\t\\t5Rights welcome the measure for services to acknowledge the report and providing a timescale for resolution, however, state Ofcom should also recommend services should keep child users or users submitting reports/complaints regarding child users up to date or be given information as to where they are in a database of complaints\\t5Rights welcome the measure for services to acknowledge the report and providing a timescale for resolution, however, in consultation with the Bereaved Families for Online Safety, the group raised that a lack of update or further detail can cause additional anxiety where reporting or complaints are related to a harm or risk to children. They suggest Ofcom should also recommend services should keep child users or users submitting reports/complaints regarding child users up to date or be given information as to where they are in a database of complaints. User reporting and complaints (U2U and search) \\t\\t5Rights state that a reporting system should be able to find a person quickly when a child is involved\\t5Rights claim that much of the reporting and complaints process is automated and no humans are involved. They argue this makes it difficult for those concerned about the impact of content on vulnerable individuals, such as children, to raise concerns urgently. Tehy further state that automated systems fail to consider the nature by which the content has been displayed and to whom so contextual judgements cannot be made. They recommend that when content is reported by a child or reported as a concern for children, these reports/complaints should be assigned to a person quickly. User reporting and complaints (U2U and search) \\t\\t5Rights argue services need to be required to clearly provide guidance on how to report content for non-registered users\\t5Rights argue services need to be required to clearly provide guidance on how to report content for non-registered users\\nUser reporting and complaints (U2U and search) \\t\\t5Rights argue that users should have the chance to also appeal decisions made to not take reported content down if it concerns a child/child safety\\t5Rights said that through consultation with the Bereaved Families for Online Safety, it has been raised that users that report or complain about content that involves harm or risk to a child that a service decides not to take down, should have the ability to appeal this decision, just like users that have their content reported/complained about have the right to appeal against a decision by the service to take the content down. User reporting and complaints (U2U and search) \\t\\t\"5Rights suggest Ofcom encourage:\\n- more transparency of reporting mechanisms\\n- services to make it easier for children to report content\\n- services allow for more context when users report content\"\\t5Rights argue that requiring services to be transparent about the number and nature of the reports and complaints they get will be instrumental to driving change, and state Ofcom can encourage greater transparency. They also argue that it is important that services make the reporting mechanisms easier for child users to understand and use, and allow users to add more context to the nature of the report/complaint as still will also help Ofcom understand and know the level of harm. User reporting and complaints (U2U and search) \\t\\t5Rights argue that reporting of content is important, but should be considered as a complementary measure alongside other safety by degin measures, and raise that Ofcom need to be careful not to place the responsibility on users to report and flag harmful content as this responsibility should lie with the services themselves\\t5Rights noted that by itself, content reporting by users is not an efficient measure to ensure safety of users, particularly child users, on its own. They argue this is complimentary alongside other safety by design measures. They raise that it is important Ofcom do not shift the responsiblity to report harmful content onto users as evidence suggest that many child users do not see the point in reporting content, and this responsiblity ultimately lie with the services themselves. Terms of service and Publicly Available Statements\\t\\t5Rights broadly support the measure on terms of service and publicly available statements, however, recommend Ofcom considers the IEEE Standards for an Age Appropriate Digital Services Framework \\t\"5Rights state that accessible, easy to understand ToS and PAS\\' are central to giving users knowledge regarding the agreement they are entering into when they use a service, including children. They state they support Ofcom\\'s recommended measure and standards, particularly that it is outcomes based, but they note that although the reading age of the terms is recommended to be understandable to the youngest person on the service, it does not necesaarily mean every 13 year old will understand the contract they are entering in to. They recommend Ofcom considers the IEEE Standards for an Age Appropriate Digital Services Framework which includes how services can effectively produce age-appropriate published terms. This sets out tht info to children on a serivice must:\\n- age-appropriate\\n- designed so that the information they contain comprehensible\\n- an appropriate length, clearly presented\\n- easy to find, introduced at the right moments, and;\\n- understandable to all young people, no matter who they are, how old they are, or where they come from\"\\nTerms of service and Publicly Available Statements\\t\\t5Rights recommend Ofcom map how services have changed their ToS and PAS\\' since 2020 to ensure that companies are not downgrading their terms to comply \\t5Rights state that as it is a central requirement of the OS regime that services uphold their ToSand PAS\\', they recommend Ofcom map how services have changed their public terms since 2022 ensure that companies are not downgrading their terms to comply rather than upgrading their systems and processes to meet the spirit of the Act. Terms of service and Publicly Available Statements\\t\\t5Rights broadly support the measure on terms of service and publicly available statements, however, recommend Ofcom considers the IEEE Standards for an Age Appropriate Digital Services Framework \\t\"5Rights state that accessible, easy to understand ToS and PAS\\' are central to giving users knowledge regarding the agreement they are entering into when they use a service, including children. They state they support Ofcom\\'s recommended measure and standards, particularly that it is outcomes based, but they note that although the reading age of the terms is recommended to be understandable to the youngest person on the service, it does not necesaarily mean every 13 year old will understand the contract they are entering in to. They recommend Ofcom considers the IEEE Standards for an Age Appropriate Digital Services Framework which includes how services can effectively produce age-appropriate published terms. This sets out tht info to children on a serivice must:\\n- age-appropriate\\n- designed so that the information they contain comprehensible\\n- an appropriate length, clearly presented\\n- easy to find, introduced at the right moments, and;\\n- understandable to all young people, no matter who they are, how old they are, or where they come from\"\\nTerms of service and Publicly Available Statements\\t\\t5Rights recommend Ofcom map how services have changed their ToS and PAS\\' since 2020 to ensure that companies are not downgrading their terms to comply \\t5Rights state that as it is a central requirement of the OS regime that services uphold their ToSand PAS\\', they recommend Ofcom map how services have changed their public terms since 2022 ensure that companies are not downgrading their terms to comply rather than upgrading their systems and processes to meet the spirit of the Act. \" Default settings and user support (U2U)\"\\t\\t5Rights welcome the default settings and support for child users measures but recommend it be extended to all services, not just services at high risk of grooming \\t5Rights state that they recommend Ofcom extends the U2U default settings and support for child users measures to all services, not just those at high risk of grooming as this is a regulatory requirement under GDPR and the the Age Appropriate Design Code (AADC), recommended by the OECD. They state that Ofcom\\'s requirements relating to default settings must not undermine the default settings many services must already apply to child users in the AADC. \" Default settings and user support (U2U)\"\\t\\t5Rights state they strongly disagree that services that do not currently ask for or record the age of a user should can exempt from age assurance measures\\t5Rights state they strongly disagree that services that do not currently ask for or record the age of a user should can exempt from age assurance measures. They warn that the Code should not undermine the protection of child users already received under the AADC, and recommend alignment with the AADC \\'likely to be accessed\\' threshold as this is an assessment used by services currently, many of whom fall in scope of the Act. \" Default settings and user support (U2U)\"\\t\\t5Rights welcome the measure that services put in place default settings which protect child user accounts from being promoted to adult user accounts and argue it should be required of services that these settings cannot be turned off by default\\t5Rights welcome the measure that services put in place default settings which protect child user accounts from being promoted to adult user accounts. They arguethis is a key way to tackle grooming and harassment. Further they argue it should be required of services that these settings cannot be turned off by default and that there should be no option to turn them on for younger groups of children\\n\" Default settings and user support (U2U)\"\\t\\t5Rights note that the measure proposed that \\'Children using a service are not included in publicly visible lists of who users are connected to, and lists setting out who child users are connected to are not displayed to other users\\' is already a regulatory requirement for services under the Age Appropriate Design Code. 5Rights note that the measure proposed that \\'Children using a service are not included in publicly visible lists of who users are connected to, and lists setting out who child users are connected to are not displayed to other users\\' is already a regulatory requirement for services under the Age Appropriate Design Code. \" Default settings and user support (U2U)\"\\t\\t5Rights state they strongly support the measure that people cannot send direct messages to children without first establishing a connection e.g. \\'becoming friends\\', but state Ofcom should require that these settings are turned off entirely for younger groups of children, with no option to turn back on\\t5Rights state they strongly support the measure that people cannot send direct messages to children without first establishing a connection e.g. \\'becoming friends\\', but state Ofcom should require that these settings are turned off entirely for younger groups of children, with no option to turn back on. They link to their own research, Pathways, that on some services, child users were added to group chats by user accounts they did not follow in which pornographic content was shared. \" Default settings and user support (U2U)\"\\t\\t5Rights state they strongly agree with the measure in para. A7.3 b) of the Codes\\t5Rights state they strongly agree with the measure in para. A7.3 b) of the Codes\\n\" Default settings and user support (U2U)\"\\t\\t5Rights state that the measure in para. A7.4 in the Codes is already a regulatory requirement for services under the AADC\\t5Rights state that the measure in para. A7.4 in the Codes is already a regulatory requirement for services under the AADC\\n\" Default settings and user support (U2U)\"\\t\\t5Rights state the support Ofcom\\'s consideration of the timings of prompts to children and that it should be presented in a format children can understand, but raise concerns that research has found that sometimes services attempt to persuade users into accepting certain settings through specific wording and presentation of prompts and recommend Ofcom address this\\t5Rights state the importance of info on services being age-appropriate and designed so it is comprehensible, appropriate length etc., and say they welcome that Ofcom has included the importance of timings of prompts to children and that they have to be presented clearly in a format children can understand, and taken onboard 5Rights evidence with regards to this. They further state their concern that services sometimes attempt to persuade users into accepting certain settings through specific wording and presentation of prompts, as identified in research by the Norwegian Consumer Association. They therefore recommend Ofcom should make clear that the info in prompts should be empowering, relaying the risks and facts and not persuade children to lower protections offered to them by default settings. \" Default settings and user support (U2U)\"\\t\\t\"5Rights state that the agree with the functionalities listed, but that Ofcom should expand the list to cover a number of additional functionalities that should be turned “off” by default for children in order to keep them safe from illegal harm including:\\n- Public profiles\\n- In-game gifts/gifting in-app\\n- Livestreaming and video-sharing\\n- Quick add\\n- Targeted advertising\\n-  Autoplay\\n- Ephemeral content\"\\t\"5Rights state that the agree with the functionalities listed, but that Ofcom should expand the list to cover a number of additional functionalities that should be turned “off” by default for children in order to keep them safe from illegal harm including:\\n- Public profiles\\n5Rights argue anyone under 18 on all services should be private by default and never visible or searchable to all users of the service\\n- In-game gifts/gifting in-app\\n5Rights refer to their own research which shows groomers leverable children by offering in-game/in-app gifts or currency that can be used to coerce them into participating in criminal activity. They also refer to a news article revealing children as young as 12 are groomed into becoming \\'drug mules\\' by sending children Fortnite currency. - Livestreaming and video-sharing\\n5Rights refer to their research Risky by design which highlights that children are at risk of being groomed via livestreaming and video-sharing functionaltities \\n- Quick add\\n5Rights refer to their research that argues that the ability to quick add people on services alongside pressures many children face about being popular online means they are likely to add people they do not know via these functions, and bad actors then have a greater chance of reaching children quickly. - Targeted advertising\\n5Rights aruge that by the way in which targeted ads collect user data to build porfiles that can be used to target highly individualised ads, some products that are detrimental to children\\'s health and wellbeing may be promoted. They note that this is in breach of the AADC. -  Autoplay\\n5Rights argue that this functionality increases the risk of children being shown content without the initiation by a user and that what is autoplayed is informed by recommendation algorithms. Child users are therefore at risk of the auto play content to become more narrow in focus and amplify extremist content. - Ephemeral content\\n5Rights aruge that because posts can disappear it is difficult to fact-check them, and research has found children find it difficult to report\"\\n\" Default settings and user support (U2U)\"\\t\\t5Rights stress that support for child users and default settings should be required for the full range of harms and for all services in scope of the regulation, however, add that Ofcom should propose services send periodic reminders child users in cases where default safety settings have been turned off at the request of the child\\t5Rights welcome that Ofcom has taken onboard 5Rights evidence on prompts and warnings for child users. They believe these measures should be applied to all services and the full range of harms. Further, they suggest Ofcom should recommend that where children have turned off default safety settings at their own request, services should send periodic reminders to remind children that e.g. their prolfile has been put to \\'public\\' or whatever they have changed their default settings to. Recommender system testing (U2U) \\t\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure as it is an ex-ante approach and a key feature of safety by design. They state they have developed a four-step algorithmic oversigh model which is tech neutral and can be applied to a number of different measures, something they will provide more evidence on in the PoC consultation\\nRecommender system testing (U2U) \\t\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure as it is an ex-ante approach and a key feature of safety by design. They state they have developed a four-step algorithmic oversigh model which is tech neutral and can be applied to a number of different measures, something they will provide more evidence on in the PoC consultation\\nRecommender system testing (U2U) \\t\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure\\t5Rights argue that services should undertake risk assessments of recommender systems before they are applied to a service, and would welcome such a measure as it is an ex-ante approach and a key feature of safety by design. They state they have developed a four-step algorithmic oversigh model which is tech neutral and can be applied to a number of different measures, something they will provide more evidence on in the PoC consultation\\nEnhanced user control (U2U) \\t\\t5Rights support measures that give users, particularly children, the ability to have more control over their experiences online, but flag that the promotion of these controls and tools by itself is not an effective mean to ensure the safety of child users, and Ofcom should be careful not to shift the responsibility of safety onto users/children users themselves\\t5Rights support measures that give users, particularly children, the ability to have more control over their experiences online. They refer to evidence from the AADC, standard 15 of this Code which states that services must provide tools that help children exercise their data protection rights and report concerns. They then flag that the promotion of these controls and tools by itself is not an effective mean to ensure the safety of child users and should be considered a complimentary measure alongside other safety by design measures. Finally they stress that Ofcom should be careful not to shift the responsibility of safety onto users/children users themselves as this is ultimately the services\\' responsibility. Enhanced user control (U2U) \\t\\t5Rights provides evidence through other regulatory Codes that online tools and controls should be displayed prominently to children\\t5Rights state that the ICO\\'s Age Approproate Design Code shows how online tools and controls should be displayed prominently to child usersof different age groups, and that the IEEE Standard for age approriate digital services framework sets out how services can best apply this design strategy for children. Enhanced user control (U2U) \\t\\t5Rights argue that the guidance on this measure should make clear that transparency is needed so children can understand how how verification schemes are operated as evidence they refer to (including Ofcom research) states that children are more susceptible to risks of fraud and bad actors\\t5Rights agree with Ofcom\\'s assessment that services shouls provide transparency regarding how verification schemes are operated, but argue that because this measure should also highlight the unique vulnerability of children to fraud and bad actors online. They refer to research by Ofcom which has found that verification schemes can be used by bad actors to impersonate official sources and mislead users and that 23% of children could not correctly identify a fake social media profile when presented with one. They argue this proves that children are more susceptible to fraud and being targeted by bad actors online, so the measure should recommend that any info, context or prompts services put in place to create more transparency around how users can gain verified status must also must be age-appropriate. They refer to the IEEE Standard for an Age Appropriate Digital Services Framework which sets out how services can best and most comprehensively apply this design strategy for children. User access to services (U2U) \\t\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes and use this info to inform future iterations of the Codes\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes of users that have broken the law/ToS, and this should consider challenges such as the use of VPNs to create new user profiles/burner accounts. They  recommend this info should be used to inform fututre iterations of the Codes and also the proposals regarding how users who have shared CSAM should be dealt with. They further suggest servives should work with relevant LE authorities regarding individuals they suspect of having committed offences on their platforms. User access to services (U2U) \\t\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes and use this info to inform future iterations of the Codes\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes of users that have broken the law/ToS, and this should consider challenges such as the use of VPNs to create new user profiles/burner accounts. They  recommend this info should be used to inform fututre iterations of the Codes and also the proposals regarding how users who have shared CSAM should be dealt with. They further suggest servives should work with relevant LE authorities regarding individuals they suspect of having committed offences on their platforms. User access to services (U2U) \\t\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes and use this info to inform future iterations of the Codes\\t5Rights suggest Ofcom use information powers to understand how services are approaching blocking or strikes of users that have broken the law/ToS, and this should consider challenges such as the use of VPNs to create new user profiles/burner accounts. They  recommend this info should be used to inform fututre iterations of the Codes and also the proposals regarding how users who have shared CSAM should be dealt with. They further suggest servives should work with relevant LE authorities regarding individuals they suspect of having committed offences on their platforms. User access to services (U2U) \\t\\t5Rights suggest services should work with with the relevant enforcement authorities regarding individuals they suspect of having committed offences on their platforms\\t5Rights suggest services should work with with the relevant enforcement authorities regarding individuals they suspect of having committed offences on their platforms\\nService design and user support (Search) \\t\\t5Rights agree with the proposal on reporting for predictive search, but state that this on its own is not an effective mean to ensure the safety of users, particularly not children\\t5Rights agree with the proposal on reporting for predictive search, but state that this should be considered complimentary alongside other safety by degins oriented measures as on its own, it is inot going to be efficient in keeping users safe. They also argue that the responsiblity should not lie with the users ensuring their own safety, but rather that this responsbility lies with the services, and Ofcom should be careful not to shift responsbility onto users. Service design and user support (Search) \\t\\t5Rights recommend Ofcom consider NCPSS\\'s response with regards to warning messages for users searching suicide content or CSAM\\t5Rights recommend Ofcom consider NCPSS\\'s response with regards to warning messages for users searching suicide content or CSAM\\nCumulative Assessment  \\t\\t5Rights Foundation state they are critically concerned that Ofcom has interpreted \\'measures\\' as tools rather than systems and processes\\t5Rights Foundation argue that by interpreting \\'measures\\' as tools rather than systems and processes it means that services can be considered compliant even when the service is risky by design. They recommend Ofcom should put processes that are iterative and have outcomes so that the measure or mitigation is not deemed adequate until the outcome has been achieved to be in line with what the government assured would be a systems and processes regime. Cumulative Assessment  \\t\\t5Rights Foundation state they are concerned that Ofcom places a focus on the cost to the services complying with the measures, rather than focusing on the cost to the potential victims of offences and illegal content the service causes\\t5Rights Foundation argues that Ofcom places and undue focus on the cost of the services complying with the measures which they claim runs counter to the stated objectives of the Act. 5Rights Foundation argue that by Ofcom focusing too heavily on measures being \\'proportionate\\' and not taking a victim centred approach, the regime may result in being regressive whereby companies that alrrady comply pull back their investments in safety and those that do nothing make minimum investments. They further argue that measures should be as far as possible expressed in processes that iterate until the goal has been achieved. Cumulative Assessment  \\t\\t5Rights Foundation argue that the current drafting of the Codes does not account for smaller high-risk services which host illegal content and facilitate illegal activity\\t5Rights Foundation state they are concerned that as the Codes are currently drafted, a number of small high-risk services that are dedicated to single issues themes included in the illegal harms and offences are not covered. They use a quote by Lord Minister said during the passage of the OSA in which they reassured parliamentarians that these types of services would be included. They then state that the Codes should follow that where there is risk of illegal harm or harm to children, online safety regulation applies to any and all online services, irrespective of the nature or size of the service. Cumulative Assessment  \\t\\t5Rights Foundation state they are critically concerned that Ofcom has interpreted \\'measures\\' as tools rather than systems and processes\\t5Rights Foundation argue that by interpreting \\'measures\\' as tools rather than systems and processes it means that services can be considered compliant even when the service is risky by design. They recommend Ofcom should put processes that are iterative and have outcomes so that the measure or mitigation is not deemed adequate until the outcome has been achieved to be in line with what the government assured would be a systems and processes regime. Cumulative Assessment  \\t\\t5Rights Foundation state they are concerned that Ofcom places a focus on the cost to the services complying with the measures, rather than focusing on the cost to the potential victims of offences and illegal content the service causes\\t5Rights Foundation argues that Ofcom places and undue focus on the cost of the services complying with the measures which they claim runs counter to the stated objectives of the Act. 5Rights Foundation argue that by Ofcom focusing too heavily on measures being \\'proportionate\\' and not taking a victim centred approach, the regime may result in being regressive whereby companies that alrrady comply pull back their investments in safety and those that do nothing make minimum investments. They further argue that measures should be as far as possible expressed in processes that iterate until the goal has been achieved. Cumulative Assessment  \\t\\t5Rights Foundation argue that the current drafting of the Codes does not account for smaller high-risk services which host illegal content and facilitate illegal activity\\t5Rights Foundation state they are concerned that as the Codes are currently drafted, a number of small high-risk services that are dedicated to single issues themes included in the illegal harms and offences are not covered. They use a quote by Lord Minister said during the passage of the OSA in which they reassured parliamentarians that these types of services would be included. They then state that the Codes should follow that where there is risk of illegal harm or harm to children, online safety regulation applies to any and all online services, irrespective of the nature or size of the service. Cumulative Assessment  \\t\\t5Rights Foundation state they are critically concerned that Ofcom has interpreted \\'measures\\' as tools rather than systems and processes\\t5Rights Foundation argue that by interpreting \\'measures\\' as tools rather than systems and processes it means that services can be considered compliant even when the service is risky by design. They recommend Ofcom should put processes that are iterative and have outcomes so that the measure or mitigation is not deemed adequate until the outcome has been achieved to be in line with what the government assured would be a systems and processes regime. Cumulative Assessment  \\t\\t5Rights Foundation state they are concerned that Ofcom places a focus on the cost to the services complying with the measures, rather than focusing on the cost to the potential victims of offences and illegal content the service causes\\t5Rights Foundation argues that Ofcom places and undue focus on the cost of the services complying with the measures which they claim runs counter to the stated objectives of the Act. 5Rights Foundation argue that by Ofcom focusing too heavily on measures being \\'proportionate\\' and not taking a victim centred approach, the regime may result in being regressive whereby companies that alrrady comply pull back their investments in safety and those that do nothing make minimum investments. They further argue that measures should be as far as possible expressed in processes that iterate until the goal has been achieved. Cumulative Assessment  \\t\\t5Rights Foundation argue that the current drafting of the Codes does not account for smaller high-risk services which host illegal content and facilitate illegal activity\\t5Rights Foundation state they are concerned that as the Codes are currently drafted, a number of small high-risk services that are dedicated to single issues themes included in the illegal harms and offences are not covered. They use a quote by Lord Minister said during the passage of the OSA in which they reassured parliamentarians that these types of services would be included. They then state that the Codes should follow that where there is risk of illegal harm or harm to children, online safety regulation applies to any and all online services, irrespective of the nature or size of the service. Statutory Tests \\t\\t5Rights urge Ofcom to consider the statement of the Online Safety Act Network with regards to this, of which 5Rights is a signatory\\t5Rights urge Ofcom to consider the statement of the Online Safety Act Network with regards to this, of which 5Rights is a signatory\\nGovernance and accountability\\t\\tDo not sufficiently recongise potenial impact on measures on FoE, data misuse, non-illegal harms\\tn volume 3 there is a lack of recognition of the harm that inadequate governance and accountability measures can pose from undue restrictions on FoE, over�collection or misuse of personal data, and other harms that are not from exposure to illegal content\\nGovernance and accountability\\t\\twe support Ofcom’s decision not to include a remuneration related measure in this first Code of Practice\\tWe support measures on staff incenticves, but recongised costs on poor risk management is more nuanced in OS than financial setor. We think there will be a learning process so support decision not to include remuneration related measure in this first Code of Practice. Governance and accountability\\t\\tYes, governance measures important for large services\\tlure to effectively mitigate risk in these services can have significant impact on users and on the rest of the market. Parallels, albeit an imperfect ones, can be made with other sectors. The Arthur Andersen/Enron scandal (2001) and bank run on Silicon Valley Bank (2023) are just two examples that highlight the key importance of corporate governance, internal and external auditing and risk management procedures, and need to monitor and review periodically. Governance and accountability\\t\\tAgree with proposal not to implement independent audit requirements\\t\"A new market would need to develop for audit requirements to be effective. Ofcom should monitor DSA developments. We foresee two interrelated issues with independent audit of illegal content reporting: \\ni) Cost/Benefit Issue: Is there any evidence that external audits would add societal value beyond the existing governance mechanisms within the OSB? ii) Implementation Issue: How would the market for independent audit of illegal content reporting function? Is it possible for the firms conducting these audits to \\nhave the necessary capacities and independence to conduct them competently?': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9901425838470459,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Who would oversee the auditors? Would developing this new audit market possibly risk harm to existing audit markets?\"\\nApproach to the Codes\\t\\tAgree with overall approach to codes\\tWe commend Ofcom on its overall approach to developing the Code and the way in which the topics to be covered were broken down. We note the reservation with which Ofcom approaches areas where the body of evidence to support a particular measure or practice is not yet developed.': [{'label': 'POSITIVE',\n",
       "               'score': 0.996489942073822,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We can also see a precautionary approach being taken in relation to CSEA, which is appropriate given the nature of the harm. Approach to the Codes\\t\\tAgree not to require use of hash dateabases related to terrosim content\\tAs Ofcom notes, some of the same measures are used by services in relation to terrorist content, namely those relying on hash databases. We agree that Ofcom is right not to require such measures at this time and appreciate Ofcom’s recognition that an ecosystem of larger and smaller services, and sophisticated networks, can be involved in this offence. There are also greater risks to freedom of expression from misidentification of terrorist content. Approach to the Codes\\t\\tAgree that performance of conmod measures should consider both speed and accuracy\\tWherever performance targets are discussed in Volume 4 in relation to U2U and Search content moderation, Ofcom makes very clear that such targets should be based on both speed and accuracy and not speed alone. This approach seems to be aimed at mitigatingthe risk of over-removals or over-blocking, which can have a stifling effect on freedom of expression and other fundamental rights.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9931366443634033,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This is a welcome approach. As has been argued, there are commercial incentives to standardise, automate and centralise content moderation in larger services, which can remove it from context and result in being overly cautious in some grey areas. It might be helpful in this section to give some indications of how accuracy should be determined to ensure this is a useful measure. Approach to the Codes\\t\\tWelcomes recognition of the importance of language capabilities\\tWe also welcome Ofcom’s consistent recognition of the importance of language capabilities throughout its approach. Initial findings from research we are currently conducting has already flagged up this as critical to efforts to combat harmful content and to adequately protect freedom of expression. Ofcom has appropriately highlighted capacity in the diversity of languages used in the UK in the section on resources and capabilities for content moderation\\nApproach to the Codes\\t\\tGenerally agree with complaints measures but concerned with Ofcom’s attribution of the human right to freedom of expression rights to the services themselves\\tWe generally agree with Ofcom’s approach to measures related to complaints handling, which are generally in line with instruments in other jurisdictions. Ofcom rightly notes that rather than negatively impacting freedom of expression, these measures should help to ensure it is protected. We also believe that it is not useful at this stage to require complaint tracking or responses in a specific timeframe. However, we are somewhat concerned with Ofcom’s attribution of the human right to freedom of expression rights to the services themselves in its discussion of the impact on rights in paragraph 16.112. Requiring service providers to provide information that is relevant to consumer protection is normal across sectors and crucial to the exercise of regulation\\nApproach to the Codes\\t\\tAgree with limiting user access only to CSEA and terrorist content disemination\\tWe appreciate and agree with Ofcom’s decision to limit prescriptions in the Code on blocking users’ access to when CSEA has been disseminated and to terrorist groups proscribed by the UK government. Given the severity of the harms in question the precautionary principle should apply and these measures included in the Code despite the lack of definitive evidence on effectiveness. As long as the elements covered in the complaints handling part are also in place, there should be effective recourse for those who feel unjustly blocked. Approach to the Codes\\t\\tAgree with choice not to require identify verification as a measure\\tWe agree with Ofcom’s reasoning and approach on the use of schemes that purport to verify identity (notable user and monetised schemes) and its choice to not require identify verification as a measure for mitigating harms. Transparency and appropriate internal policies are especially important when services offer monetised labelling schemes that can be easily manipulated to deceive users. The option of anonymity, especially on major services that act as public spaces, remains important for preserving freedom of expression and the practice of journalism. We look forward to future consultation on age verification measures\\nAutomated content moderation (User to User)\\t\\tAgree not to require use of hash databases related to terrosim content\\tAs Ofcom notes, some of the same measures are used by services in relation to terrorist content, namely those relying on hash databases. We agree that Ofcom is right not to require such measures at this time and appreciate Ofcom’s recognition that an ecosystem of larger and smaller services, and sophisticated networks, can be involved in this offence. There are also greater risks to freedom of expression from misidentification of terrorist content. Content moderation (User to User)\\t\\tAgree that performance of conmod measures should consider both speed and accuracy\\tWherever performance targets are discussed in Volume 4 in relation to U2U and Search content moderation, Ofcom makes very clear that such targets should be based on both speed and accuracy and not speed alone. This approach seems to be aimed at mitigatingthe risk of over-removals or over-blocking, which can have a stifling effect on freedom of expression and other fundamental rights. This is a welcome approach. As has been argued, there are commercial incentives to standardise, automate and centralise content moderation in larger services, which can remove it from context and result in being overly cautious in some grey areas. It might be helpful in this section to give some indications of how accuracy should be determined to ensure this is a useful measure. User reporting and complaints (U2U and search) \\t\\tGenerally agree with complaints measures but concerned with Ofcom’s attribution of the human right to freedom of expression rights to the services themselves\\t\"We generally agree with Ofcom’s approach to measures related to complaints handling, which are generally in line with instruments in other jurisdictions. Ofcom rightly notes that rather than negatively impacting freedom of expression, these measures should help to ensure it is protected. We also believe that it is not useful at this stage to require complaint tracking or responses in a specific timeframe. However, we are somewhat concerned with Ofcom’s attribution of the human right to freedom of expression rights to the services themselves in its discussion of the impact on rights in paragraph 16.112. Requiring service providers to provide information that is relevant to consumer protection\\nis normal across sectors and crucial to the exercise of regulation\"\\nUser access to services (U2U) \\t\\tAgree with limiting user access only to CSEA and terrorist content disemination\\tWe appreciate and agree with Ofcom’s decision to limit prescriptions in the Code on blocking users’ access to when CSEA has been disseminated and to terrorist groups proscribed by the UK government. Given the severity of the harms in question the precautionary principle should apply and these measures included in the Code despite the lack of definitive evidence on effectiveness. As long as the elements covered in the complaints handling part are also in place, there should be effective recourse for those who feel unjustly blocked. Enhanced user control (U2U) \\t\\tAgree with choice not to require identify verification as a measure\\tWe agree with Ofcom’s reasoning and approach on the use of schemes that purport to verify identity (notable user and monetised schemes) and its choice to not require identify verification as a measure for mitigating harms. Transparency and appropriate internal policies are especially important when services offer monetised labelling schemes that can be easily manipulated to deceive users.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9914634823799133,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The option of anonymity, especially on major services that act as public spaces, remains important for preserving freedom of expression and the practice of journalism. We look forward to future consultation on age verification measures\\nApproach to the Codes\\t\\tAgree most onerous measures should apply to largest and riskiest services\\tYes. Illegal content offences can be perpetrated on smaller, niche services, and often these are used for some of the most egregious ones in order to avoid detection. However, size in the form of reach and market power do matter when it comes to harmful impact due to reach, the often public nature of the dissemination, and the potential for contagion.30 In addition, research that we are currently undertaking has already collected evidence from monitors on the dynamics of FIO and false communication offences that indicates that the large popular services can be gateways to the less-moderated smaller or private services, or used for fishing. For example, links shared on popular social media that do not meet the threshold for actionable content, and may be bot-generated, are used to entice users into Telegram groups, or other closed spaces. Mitigation measures taken at the level of the larger services can still help mitigate these risks. Approach to the Codes\\t\\t10% threshold appears suitable and should try to be consistent with DSA\\tit appears that the 10% threshold appears suitable as most of the platforms on the ‘very large’ list are comfortably over the threshold, a trend which is likely to be similar in the UK’s ‘large’ list. Therefore, most of the largest services should be captured by the 10% threshold. Equally, we support Ofcom\\'s suggestion that having the same methodology for designating a service as large under the OSA or very large under the DSA should reduce the burden on services. The notion that once as firm is designated as large that it cannot drop below the large threshold unless it registers average monthly users below the threshold for 6 months appears reasonable, as if a firm is very close to the threshold and goes over it/under it on a monthly basis then a prudent approach seems appropriate\\nAutomated content moderation (User to User)\\t\\tOfcom should place appropriate emphasis on large firms to introduce automated content moderation\\tIt is not seen as credible that large firms serving millions of users are able to manually review activity to detect crime. As such, Ofcom should place appropriate emphasis on large firms to introduce automated content moderation, in order for their Codes of Practice to deliver a significant reduction in illegal harm. The Financial Conduct Authority (FCA) has developed best practice with regard to how automated transaction monitoring systems should be developed, assessed and implemented. This includes outlining methodologies, the development and implementation of such models, effectiveness assessments and oversight mechanisms. Automated content moderation (User to User)\\t\\tCodes should require services that have a significant risk of fraud revealed in their risk assessments to develop an ACM programme that can detect high risk content reflective of the significant technological and financial resources they have access to - not just key word detection\\tWe are specifically suggesting that large firms, who conduct a risk assessment and subsequently identify their products and services to be at a high risk of user-generated content facilitating fraud, should be required to introduce automated content moderation controls. The controls in place should reflect the fraud risks associated with the specific platform, rather than blanket measures that do not reflect the harms associated with one service or another. It should take into account numerous other data points and the technology that large firms have at hand to design processes to detect and flag high risk content, as well as utilise the intelligence provided by trusted flaggers. Automated content moderation (User to User)\\t\\tConcerned about efficacy of key word detection\\tWe are concerned about the efficacy of fraud keyword detection given that its effectiveness hinges on fraudsters using very specific keywords, content of keyword lists, and the manner in which keyword detection is conducted. As Ofcom has conceded, fraudsters may circumvent keyword detection by adopting new keywords and combinations of keywords to conceal their activities. Keyword detection thus relies on firms modifying their systems and constantly updating their keyword lists according to the language that fraudulent actors use. Automated content moderation (User to User)\\t\\t\"focusing solely on articles for use in frauds is a weakness of the proposals - should also focus on purchase scams and investment scams \"\\t“Articles for use in frauds” is too narrow as it largely focuses on items designed or intended to facilitate fraudulent activities such as content which offers to supply individuals’ stolen personal or financial credentials. From the perspective of FinTech and financial services, this is not the only type of fraud that is perpetuated by criminal actors via large user-to-user services, such as social media. Various stats on purchase/investment scams orignating on Meta. Automated content moderation (User to User)\\t\\tShould consider compelling all in scope large U2U services to proactively update keyword lists\\tWe are concerned that relying on standard keyword detection in any case is outdated and prescriptive. Standard keyword detection should be the bare minimum technology that large firms should employ to tackle fraud at source. Nevertheless, given that Ofcom considers this the most proportionate approach, we propose that a code of practice that compels all in-scope large online user-to-user services to proactively identify and accordingly update their keyword lists with new keywords that are associated with articles for use in frauds online. Automated content moderation (User to User)\\t\\tOfcom should set out minimum ID and listing verification for large U2U services\\tSuch measures must be consistent and transparent across all user-to-user services, though in the principle of tech neutrality firms should be free to determine the tools by which they conduct verification checks. This will clamp down on anonymity, making it more difficult for fraudsters to list fake goods with impunity, thereby cutting down on purchase scams. This information will also make it easier for user-to-user services to assist law enforcement in retrieving the lost money and taking action against fraudsters. Automated content moderation (User to User)\\t\\t\"Online peer-to-peer marketplaces should integrate with secure payment services \"\\tOfcom should therefore make integration with secure payment services compulsory because this will require sellers to verify their identity with a regulated PSP. At the same time, these secure PSPs can prevent any payment from being released to fraudsters immediately through standard delays, or until goods are confirmed to have been received. Our members also consider this to be a solution that will enable buyers to easily dispute transactions and receive refunds as opposed to working in cash or via bank transfer. It will also incentivise online peer-to-peer marketplaces to ensure that all listings are genuine to keep the cost of their integrations low. Automated content moderation (User to User)\\t\\tour position is that Ofcom should prioritise the potential harms averted over the relative costs involved. Considering that over 60% of all APP fraud originate via Meta according to UK Finance data, we consider it fair and proportionate to compel large social media platforms to bear additional costs in order to improve their fraud prevention systems and upskill their staff.15 In fact, Ofcom’s estimated costs for large firms to introduce standard keyword detection pale in comparison to what the PSR is expecting PSPs reimburse each victim of APP fraud, regardless of their size. Automated content moderation (User to User)\\t\\tExtend to smaller services once more evidence is available\\tWe do not have evidence on the costs for applying standard keyword detection for smaller services. However, we also recognise Ofcom’s concerns about excluding smaller services from its fraud prevention measures. We would urge Ofcom to consider the merits of extending its proposals to smaller services while noting the proportionality of the potential costs, once it has received further evidence from industry. User reporting and complaints (U2U and search) \\t\\tWelcome trusted flaggers but should be extended\\tThe dedicated reporting channel for fraud for trusted flaggers is a welcome step and Ofcom highlights the fragmented ecosystem that has prevented more effective proactive action in preventing fraud by reporting suspected fraudulent content. That said, we would like to see Ofcom go further in its final proposals to cover a wider range of trusted flaggers, which we break down into two separate categories: public and regulated private entities. Enhanced user control (U2U) \\t\\tWelcome recongnition of impersonation fraud - wider requirements should be considered. We welcome the recognition and initial steps outlined by Ofcom on the harm caused by impersonation fraud. We also agree that the scale of the challenge posted by impersonation to commit fraud means that the costs associated with implementing measures to prevent it are justified. The consultation paper acknowledges the fact that “impersonation is a factor in a much broader range of harms such as romance fraud, [and] fraud on online marketplaces”. While we welcome the initial steps outlined we think it is important that wider requirements are considered and brought into scope to account for the breadth of impersonation scams online. Governance and accountability\\t\\tAdequate resources required for services and any NGOs/charities that help them to monitor trends\\tThere are some areas where we feel some additional factors will need to be considered around tracking new kinds of illegal content and increases in particular kinds of illegal content. This will require adequate resources and capacity for the services in scope to be able to continually monitor and provide meaningful data for change. There is also an opportunity here for NGOs and charities working in the space to provide relevant insight based on the users experiencing issues relating to illegal harm. This will be invaluable data and ensure Ofcom can make informed decisions about the type of accountability measures they take when illegal content codes are breached by services in scope. However, those NGOs and charities will need to, in turn, be adequately resourced to provide the insights required. Governance and accountability\\t\\tIt’s vital that as many services as possible are included in scope and consideration is given to the internet being borderless and how the regulator intends to manage this. Unfortunately, the very nature of regulating services is likely to drive some of the perpetrator behaviour currently being encountered on major platforms onto smaller, less visible services and into dark web spaces, as well as across borders to areas where this bill will not apply, and which understandably cannot be monitored. For this reason, it’s vital that as many services as possible are included in scope and consideration is given to the internet being borderless and how the regulator intends to manage this\\nGovernance and accountability\\t\\tOfcom should revist the independent appeals process campaign\\tt is right that an independent service would review the measures services put in place as they would in any other industry. Further, this right should be passed on to users of the services themselves, and we would call on Ofcom to revisit the Independent Appeals Process campaign led by SWGfL last year when the bill was being debated in the House of Lords and which we fully supported. It makes complete sense for there to be an impartial/ independent body that can mitigate in instances where services in scope have been unable to do so through their own internal reporting mechanisms. Governance and accountability\\t\\tRemuneration should be put into services helping service users. (such as the cyber helpline)\\tAt The Cyber Helpline, we would like to see such remuneration put back into the services that are currently supporting service users with the very issues those services have failed to address, such as helplines like The Cyber Helpline working to support people who have experienced these issues. Approach to the Codes\\t\\tReview approach once enforcement has begn to understand if appropriate\\tThe Cyber Helpline acknowledges that this area is likely to evoke differences of opinion across the sector and that this initial approach is a step in the right direction. We would suggest that the approach itself needs to be reviewed once enforcement has begun to better understand if it is still the right fit, regularly reviewing and amending as required. Approach to the Codes\\t\\tNo - risks on small sites and the biggest have already done the most\\tit’s often those relating to smaller or more relatively unknown services that can have the highest risk to the user. We have dealt with and responded to high-profile stalking cases where information has been shared on Twitch, Telegram, Snapchat and X, which wouldn’t automatically be considered large services in scope but also cases involved from boards and specialist subject fan fiction sites, which will almost always fall into the smaller service category. In our experience working in this space, it’s often the biggest players (e.g.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9927226901054382,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Meta and MindGeek) who have done the most already to try and mitigate risks of harm and the smaller sites go relatively unmonitored or regulated in comparison. Approach to the Codes\\t\\tFair starting assumption but should review\\t As this type of enforcement has never been carried out at this scale globally before it’s a bit of an unknown and is a fair starting assumption. As the bill becomes more established, we would recommend regularly reviewing and updating this definition as required. Approach to the Codes\\t\\tFair starting assumption but should review\\t As this type of enforcement has never been carried out at this scale globally before it’s a bit of an unknown and is a fair starting assumption. As the bill becomes more established, we would recommend regularly reviewing and updating this definition as required. Approach to the Codes\\t\\tDo not go far enough in terms of who is in scope\\t\"Response: In principle, The Cyber Helpline feels that these codes do not go far enough in terms of who is in scope. We would like to see more of them apply to all services where possible but recognise due to economies of scale that this might be impractical for very small services. Specifically, all governance and accountability measures should be applicable to all services, similarly, content moderation codes relating to CSAM should apply to all services in scope. It is encouraging to see the reporting codes largely being recommended to all services, but we would like to see the 5I dedicated reporting channels put out to all services for Fraud and DA-related illegal harms so that our response as a nation to these crimes and prevention can be better from the start. \"\\nUser reporting and complaints (U2U and search) \\t\\tNeed further info on where users go when internal reporting structures fail to address issues\\t It Is vital that an independent and impartial appeals process is set up for service users themselves to effectively seek redress for the issues they are facing where services in scope have failed to do this. Currently, under the Video Sharing Platforms regulation, users are afforded this option, and removing that right does not make sense. Further, currently, users who have experienced harm on a UK-based VSP can report this directly to Ofcom through their complaint’s mechanism, which won’t be the case once the Online Safety Bill is in full force. Where, then, do these users go? Content moderation (Search)\\t\\tdeindex/downrank measure should apply to all services in scope\\tWe welcome the requirement for search services in scope to have systems or processes to deindex/ downrank illegal content, but we believe this should be applied to all search services in scope, not just large and multi-risk services. Automated content moderation (User to User)\\t\\tURL detection for CSAM and fraud should apply to all services in scope\\tWe welcome the requirement for U2U services in scope to have URL detection capabilities for CSAM and Fraud content but believe this should be applied to all U2U services in scope, not just large and multi-risk services. In addition, the comments made in question 18 apply here too. Automated content moderation (User to User)\\t\\tAgrees with principles in Annex 9 and impact of hamr should be main consideration\\tThe Cyber Helpline agrees with the principles set out in Annex 9 and would just reiterate that regardless of whether a piece of content is deemed private or public in nature if it is shared beyond its initial audience/ outside of its initial purpose it will have the potential to cause harm and the impact of that harm on the individuals experiencing it should be what is being taken into consideration first and foremost here. Automated content moderation (User to User)\\t\\twant discussion with Ofcom on contribution\\tWe would welcome a further discussion with Ofcom about this as believe our threat intelligence and understanding of the landscape could prove invaluable here. Automated content moderation (Search)\\t\\tURL detection for search services should cover fraud as well\\tWe welcome the requirement for search services in scope to have URL detection capabilities for CSAM but believe this should be applied to other priority offences, too, such as Fraud. In addition, the comments made in question 18 apply here too. User reporting and complaints (U2U and search) \\t\\tPleased with detail, but other trusted flaggers, reporting processes should be for all users, and accessibility needs to be suitable for all users. The Cyber Helpline are particularly pleased with the detail and scope of the recommendations made in this section for all services in scope. We do believe that there are other trusted flaggers than those mentioned as dedicated reporting channels for Fraud that should also be considered for inclusion. Further, we believe that reporting processes need to be made available to all users, not just those with accounts.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9653359055519104,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Also, the accessibility of these needs to be suited to all users, considering different abilities and neurodivergence. Terms of service and Publicly Available Statements\\t\\tYes, accessibility needs to be suitable for all users\\twe would like to reiterate that the accessibility of these needs to be suited to all users, considering different abilities and neurodivergence. Terms of service and Publicly Available Statements\\t\\tWant to discuss chat bot\\tWe would welcome further discussion on the development of our unique Chatbot, which includes the use of natural language detection to help diagnose and offer self-help guides to those who have encountered a cybercrime as we feel that some of the same underlying principles will be present here. \" Default settings and user support (U2U)\"\\t\\tAgree with proposals but need to be mindful of age assurance technologies\\tThe accuracy and bias of age assurance technologies are still relatively unknown, with quite varied levels of response, meaning that this alone cannot be used as an effective way of gauging a user’s age. We would recommend that his measure needs to be coupled with others to help identify child users and ensure that all children are properly protected on the services in scope. \" Default settings and user support (U2U)\"\\t\\tExplore if default settings for push promts should be disabled\\tWe would recommend exploring if the default settings for push prompts encouraging users to engage with more content of a similar nature could be disabled for children to help with the management of their online experience\\nRecommender system testing (U2U) \\t\\tYes agree\\tToo often, services in scope have separate safety and development teams, meaning that product developments currently don’t always receive the robust signoff with due consideration to user safety that they should. We would recommend consulting with Roblox about their unique approach to product development sign-offs, ensuring that safety is at the heart of this as a potential best practice model for other services to replicate. Recommender system testing (U2U) \\t\\tYes proactively alert users when they sign up for illegal content and sign up to StopNCII\\t\" Yes – Mind Geek proactively alert users when they search for illegal content on their services alerting them why they cannot find this and giving them a prompt to seek help for this behaviour through Stop it Now in the UK. Also, they proactively remove content when it is reported, operating a remove first, then investigating and reinstating, if need be, meaning minimal risk of additional exposure to harmful content once it has been reported. This is a best practice example that other mainstream services should adopt for illegal content. In addition, signing up for StopNCII is a way to proactively prevent a service from sharing non-consensual intimate image abuse material. This should be a requirement for all platforms in scope to address image-based sexual abuse on their platforms. \"\\nEnhanced user control (U2U) \\t\\tAgree\\t designs on Meta and X services allow users to turn comments off on posts and to ban certain words from appearing in their feeds to prevent certain abuse from appearing. Enhanced user control (U2U) \\t\\tAgree\\tAgree\\nEnhanced user control (U2U) \\t\\tAgree, but shouldn\\'t be paid for\\tThe Cyber Helpline recognises that it would be useful to understand the credibility of the information being consumed by the user, and labelling accounts through voluntary verification schemes is an equally transparent way of doing this. We would recommend that this does not become a paid subscription-based verification, however, as this undermines the authenticity of the verification (e.g. X and Meta’s ticks subscription services).': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9899542331695557,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'User access to services (U2U) \\t\\tYes, but should be expanded to other harms as well. Yes – we feel this should be expanded to other illegal priority offences, too, and not just limited to CSAM. Due consideration needs to be given to how the content is shared, however, as it’s widely recognised that a proportion of this type of content is often circulated through inauthentic accounts, meaning that innocent users could be held responsible for actions in which they had no part. Service design and user support (Search) \\t\\tYes – We would recommend consulting with Roblox\\tYes – We would recommend consulting with Roblox about their unique approach to product development sign-offs, ensuring that safety is at the heart of this as a potential best practice model for other services to replicate. Cumulative assessment\\t\\tNo\\t As detailed earlier in this response, in our experience it is often the smaller services and relatively unknown players where the most risk of harm is, largely due to ineffective content moderation and minimal to no trust and safety mechanisms on the services. For this reason, it is our feeling that these services need to be helped to be held accountable, perhaps in a buddy style system with larger services in scope\\nAutomated content moderation (User to User) \\tKeyword detection \\tOfcom could go further and make it a requirement that services remoce content, not just leave it to the discretion of the individual service and its content moderation policies. Recommends taking down this content. Also notes that the consultation makes no reference to using technoloy to identify videos or images (GIFs and memes) that are commonly used on services by fraudsters to entice victims and avoid automated keyword detection technology. Services have the ability to automatically fact check images, so they should be equally able to adopt the same technology to identify fraudulent content in the form of images. User reporting and complaints (U2U and search) \\tDRC Dedicated Reporting Channels for fraud \\tAgree with the DRC proposal but want to add 2 organisations to the list of trusted flaggers. They agree that DRCs for fraud will improve detection of illegal content and reduce harm to users by reducing difficulties organisations with expertise in frayd have in reporting known scams to services. Strongly assert that botht he Insurance Fraud Enforcement Department (IFED) and the Insurance Fraud Bureau (IFB) should be specifically included/ names as \"trusted flaggers\". Noth organisations have knowledge and experience in detecting and investigating fraud.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9925317168235779,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'See para 11 - 13 for why these organisatiosn are relevant. Enhanced user control (U2U) \\tNotable User Verification Schemes \\tThe measure should cover all services, whether or not they \"currently\" have a scheme. \"Ofcom\\'s proposal is a further step in the right direction. Clear guidance from Ofcom to services on what features an internal policy and associated processes should include for verification schemes will help ensure more consistency across services. - Issue: There is no requirement for services that don\\'t currently operate verification schemes to begin doing this. Ofcom should require services to establish and maintain a notable user verification system that meers certain criteria (where they do not already operate one). - Verification should be free of charge and part of a service\\'s duty to protect their customers. - Identity verification of a commerical entity should be more than just a chec that the corporation exists and is registered on Companies House. Ofcom should consider a commitment from services to review their anti-money laundering and sanction screening of corporate customers, if not already covered by current legislation or regulation. \"\\nGovernance and accountability  \\t\\tAgree with our proposals.': [{'label': 'POSITIVE',\n",
       "               'score': 0.5521413087844849,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Agree with our proposals. Content moderation (Search)\\t\\tAgree with our proposals. Search moderation plays an important role in reducing illegal harms experienced online. A level of subjectivity and proportionality would be beneficial for search services. Automated content moderation (Search) \\t\\tAgree with our proposals as this would be useful for the reduction of harm. Agree with our proposals as this would be useful for the reduction of harm. User reporting and complaints (U2U and search) \\t\\tAgree with our proposals. Agree with our proposals. Terms of service and Publicly Available Statements\\t\\tAgree with our proposals. Agree with our proposals. \" Default settings and user support (U2U)\"\\t\\tAgree with our proposals. Agree with our proposals. Recommender system testing (U2U) \\t\\tAgree with our proposals. Collecting safety metrics that indicate whether there will be an increase of user exposure to illegal content is a sensible proposal.\\'\\nEnhanced user control (U2U) \\t\\tAgree with our proposals. providing users with the ability to block, disable, or mute hateful or dangerous comments would reduce certain type of harm. Public transparency regarding verified status’s would also be beneficial.\\'\\nService design and user support (Search) \\t\\tAgree with our proposals. These proposals would be useful in preventing harms.\\'\\nAutomated content moderation (User to User) \\t\\tThere appears to be discrepancies in how Ofcom links its ACM measures to E2EE content – Ofcom should clarify this as it could have a chilling effect on UK and global user privacy. ACM should only apply to genuinely public communications – Apple is very concerned that a requirement to scan messages intended to be private would have a chilling effect on UK and global user privacy. The ‘at a glance document’ and chapter 14 (volume 4) state that the ACM measures do not apply to private or E2EE communications, but this is not expressly stated in the Annex 7 draft Code of Practice. There is a similar gap in Annex 9 which does not state that E2EE content can never be exempted regarded as something communicated ‘publicly’ and be exempted from the ACM measures. Ofcom should clarify this because, as we state in paragraph 14.16, these measures would not be technically feasible. Automated content moderation (User to User) \\t\\tApple is concerned about the unintended consequences of Ofcom adopting an unduly narrow approach to defining content communicated privately. \"Ofcom’s unduly narrow approach to defining content communicated privately means that communications intended to be private will be treated as if they are communicated publicly. Restricting access to content shared on file-storage and file-sharing services does not necessarily make them ‘private communications’, and so they may be subject to ACM requirements. A natural and ordinary meaning of ‘able’ to access content is that a person has all the necessary means to access it, including the necessary information to locate it. Ofcom’s approach might mean that if a user uploads content on a U2U service that can, in theory, be accessed by a wide group of people (e.g. no password is required to access it), but in practice can only be accessed by a small group of people who are given a complex URL by means of which the content is accessed, then that content should be regarded as “public”. Apple believes this approach goes against the meaning set out in section 232(2)(a) of the OSA, and fails to take sufficient account of the reasonable expectations of users when communicating in this way, and in particular, their reasonable expectation of privacy. Referring to paragraph A9.33, Apple is unclear if there are specific or only theoretical circumstances in which treating content which can only be accessed by a limited group of people as private would not be considered appropriate by Ofcom. Ofcom’s guidance should make clear that service providers should by default assume that a user taking steps to restrict access to content to a limited group of people means the relevant content should be regarded as communicated privately, unless there are clear and exceptional factors indicating an intention to share the content with the wider public. This would align more appropriately with: 1) users’ reasonable expectation of privacy; 2) the duties in the OSA, and privacy law and rights; and 3) Ofcom’s duty under the Human Rights Act to regulate compatibly with the right to respect for private and family life. Apple has ‘serious concerns’ that any requirement to scan privately stored files, which are shared with a restricted group, could undermine the privacy and security of their users. This could lead to numerous unintended consequences [examples provided in response].': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9923769235610962,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'It could also impact on FoE with automated scanning leading to the removal of more legal content than necessary, and an opening of the door to bulk surveillance. Awareness of a government compelling a provider to carry out continuous monitoring risk chilling legitimate associational, expressive, political freedoms, and economic activity. UK standards risk being leveraged and expanded globally to countries with lower legal protections for citizens. Apple believes there are alternate, privacy-protecting means of achieving Ofcom’s safety goal.\"\\nApproach to the Codes\\t\\tThe CoP should explicitly include programmatic advertising services, have specific provisions on the transparency of ad-tech companie ad placements and ad-bids, and set out a role for civil society in informing disclosure guidelines. [Response links to a paper which argues that existing transparency tools do not provide regulators with enough information. It argues there should be mandated transparency for content decisions, and public databases.]\\nApproach to the Codes\\t\\tTransparency obligations should be applied to all programmatic advertising services, ieally all online monetisation services too, to set a minimum standard for all of industry. Differing transparency obligations between large and small programmatic advertising services has the potential to create a \\'whack-a-mole\\' problem where smaller companies take up the monetisation of bad actors displaced from regulated larger ones. Online advertising provisions should cover all companies offering programmatic advertising, regardless of size. The CoP should ideally extend to other online monetisation services to ensure they have policies against the funding of harmful content and disinformation. Governance and accountability  \\t\\tIt is unclear whether programmatic advertising services are in scope - excluding them presents the risk of them not releasing data on illegal content. \"It is unclear whether programmatic advertising services would be captured under the U2U or Search Codes of Practice. If excluded, there is a risk that they do not report data which could reveal the extent to which illegal content is being funded and artificially amplified. If included, measures should include applying comprehensive policies, and revealing enforcement data. A report on behalf of the European Commission found that a lack of transparency is a key challenge in digital advertising. A 2020 survey conducted by the World Federation of Advertisers found that 79% of large advertisers encountered a lack of data sharing when working with large platforms. Auditors must have access to transparency data to hold services accountable.\"\\nGovernance and accountability  \\t\\tProgrammatic advertising companies should be considered ‘smaller services with specific risk’ as they can fund and amplify illegal content – they should have more obligations to share data necessary to defund harmful content. There is a financial incentive for disinformation actors to create content which triggers strong emotions. More-enraging content is promoted by algorithms and ensures high ad revenues. Social media services also benefit financially from this. If programmatic advertising companies were subject to governance measures (they currently are not), they would be required to be more transparent and accountable for the ad monies they flow towards illegal content [academic paper linked]. Governance and accountability  \\t\\tOfcom should mandate independent third-party auditing to mitigate the risks of services using the information asymmetry between them and the regulator to skirt regulation. When digital services have power to dictate how they are assessed there has been misconduct – they can leverage the gaps in knowledge/information between themselves and regulators. In the European chemicals sector, the delegation of information reporting to industry led to methodological abuses to skirt regulation. Civil society organisations should input, advise and provide data for the audit process. Ofcom should mandate that the choice of auditor and compensation be provided through an independent third-party (e.g. coalition of civil society members) who do not receive platform funding. Approach to the Codes\\t\\tOfcom’s Codes of Practice are out of step with the Online Fraud Charter\\tOfcom’s Codes of Practice are out of step with the Online Fraud Charter. The Online Fraud Charter outlines two actions major firms should take, if they are operating a major peer-to-peer marketplace or e-commerce site in the UK, to prevent their platforms from being used for the facilitation of fraud. These actions would ‘effectively mitigate the risk of a U2U service being used for the facilitation of a priority offence’, i.e. fraud, which is a key requirement under the Online Safety Act. We therefore strongly believe that Ofcom should incorporate these requirements into the Codes of Practice for firms, to ensure that they are effectively acting to prevent users from encountering content that amounts to a priority offence. Below we unpack the specific requirement, and why we believe it is proportionate and necessary to protect UK consumers. Approach to the Codes\\t\\tRecommendation 1: Require major peer to peer marketplaces to integrate with secure payment services\\t\"UK consumers are more likely to fall victim to purchase scams than any\\nother type of Authorised Push Payment (APP) scam. Outlines some evidence. The regulator has the power to introduce two key changes, building on the voluntary commitments in the Online Fraud Charter:\\n1. Integrating with secure payment service providers: Large firms, e.g. Facebook must be required to integrate with secure payment service providers to offer their users a safe way to pay for goods and services online. Platforms with purchase protections in place drive fewer fraud losses for our\\ncustomers.': [{'label': 'POSITIVE',\n",
       "               'score': 0.9953538179397583,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Gives some stats. This simple, cost neutral solution would help tackle the most prevalent APP fraud type in the UK. This proposal has also already garnered broad support across the industry. This would: make it harder for scammers to list fake goods; keep money out of the hands of fraudsters; make it quicker and easier to report fraud, and get a refund; protect legitimate sellers; establish a commercial incentive on peer to peer marketplaces to crack down on fraud (response contains explanations of each of these). 2. Verification of sellers: peer-to-peer marketplaces should be required to verify sellers: Anonymity makes it easier for fraudsters to list fake items on marketplaces. Requiring verification would raise the bar for criminals to gain access to these online marketplaces. Sites such as eBay and Vinted require levels of seller verification. A combination of these two measures has been shown to be effective (gives evidence) through the dramatically lower fraud reports driven by these two firms, than with Facebook.\"\\nContent moderation (User to User) \\t\\tContent moderation, including automated content moderation, for the purposes of reducing the risk of users encountering content amounting to a priority offence will be crucial for Ofcom to effectively deliver a reduction in illegal harms, across the digital economy\\tContent moderation, including automated content moderation, for the purposes of reducing the risk of users encountering content amounting to a priority offence will be crucial for Ofcom to effectively deliver a reduction in illegal harms, across the digital economy\\nAutomated content moderation (User to User) \\t\\tOfcom should place appropriate emphasis on major firms to introduce automated content moderation, in order for their Codes of Practice to demonstrate a reduction in illegal harm. Automated content moderation is extremely similar to automated transaction monitoring financial services firms conduct to meet their obligations under POCA, and the Fraud Act. It is not seen as credible that major firms serving millions of users are able to manually review activity.As such, Ofcom should place appropriate emphasis on major firms to introduce automated content moderation, in order for their Codes of Practice to demonstrate a reduction in illegal harm. The FCA has developed best practice with regard to how automated transaction monitoring systems should be developed, assessed and implemented\\nAutomated content moderation (User to User) \\t\\t\"Recommendation 2: The regulator must consider expanding the requirement for\\nservices, specifically large firms with a high risk of fraud, to require that they develop automated content moderation controls designed to proactively identify and if necessary remove high risk content\"\\t\"Recommendation 2: The regulator must consider expanding the requirement for services, specifically large firms with a high risk of fraud, to require that they develop automated content moderation controls designed to proactively identify and if necessary remove high risk content. The regulator is currently prescribing a particular type of automated content moderation (ACM) - standard keyword detection - to combat a specific sub-offence, content promoting articles for use in fraud, rather than the priority offence of fraud as a whole, as described in Schedule 7 of the Act. Grateful that Ofcom\\nhas identified fraud as the first offence to trial the use of a specific type of automated\\ncontent moderation technology. Correct to focus on fraud, due to the scale of this particular harm. However, we do not believe that limiting the focus to a specific sub-category of fraud, and prescribing a basic form of automated content moderation is the right approach. Codes should set the right incentives and require firms that have a significant risk of fraud revealed in their risk assessments, to develop an ACM programme to detect and remove high risk content relating to that priority offence as a whole. Major firms who identify their products and services to be at a high risk of user generated content facilitating fraud, should be required to introduce content moderation controls, including automated controls, and to produce an evidence base to highlight that these controls are effective. These firms need to be responsible - not simply implementing one piece of prescribed technology and announcing that they are compliant. Controls in place should reflect the fraud risks associated with the specific platform, rather than blanket measures. Automated content moderation can include standardised keyword detection, but also take into account numerous other data points, use tech to detect and flag high risk content, and use intelligence provided by trusted flaggers. High risk content identified through automated content moderation rules could then be subject to human review, reducing the risk of adverse outcomes. Not against the use of a specific proactive technology, but there must\\nbe a more general requirement to develop an ACM approach that is reflective of and commensurate to the risk they are introducing into the system. Ofcom risks restricting innovation and competition and rubber stamping the use of a fairly blunt tool (standard keyword detection). The correct\\nincentives must be set to achieve the desired outcome: to reduce illegal harms. \"\\nAutomated content moderation (User to User) \\t\\tVital that Ofcom introduces incentives through outcomes based interventions, including the bare minimum of standard keyword detection (which we believe is insufficient), to tackle all types of fraud\\t\"“Articles for use of fraud” is too narrow as it largely focuses on items designed or intended to facilitate fraudulent activities such as content which offers to supply individuals’ stolen personal or financial credentials. From the perspective of FinTech and financial services, this is not the only type of fraud that is perpetuated by criminal actors via large user-to-user services, such as social media - indeed it is not even necessarily the most prevalent. Authorised Push Payment scams cost consumers £239.3 million in the first half of 2023, while unauthorised fraud cost £340.7 million. APP scams are significant cause of harm to UK consumers and UK Finance shows that 70% of APP scams originated via an online platform. It is sensible and proportionate for large firms to have a clear requirement to employ ACM for the purposes of the detection and prevention of scams, as well as to detect content for a particular sub-offence for “articles for use in fraud”. Vital that Ofcom introduces incentives through outcomes based interventions, including the bare minimum of standard keyword detection (which we believe is insufficient), to tackle all types of fraud. It cannot be that challenger banks,\\nelectronic money institutions (EMIs) and payment service providers (PSPs) are being left to tackle the scourge of fraud driven from these companies, while the large services are not subjected to any rules to prevent fraud from being promoted in the first place.\"\\nAutomated content moderation (User to User) \\t\\tThe limitations of standard keyword detection - Monzo would not use standard keyword detection in isolation to identify suspicious transactions, and is somewhat surprised that Ofcom needs to prescribe the basic intervention\\t\"Monzo would not use standard keyword detection in isolation to identify suspicious\\ntransactions, however we certainly use the technology as part of a suite of controls. Absolutely correct that the use of this technology is table stakes in the battle to reduce the risk of illegal harm, but somewhat surprised that Ofcom needs to prescribe the basic intervention - any requirement for firms at high risk of fraud to introduce ACM systems would likely include it. The regulator needs to take an outcomes based approach to content moderation, which can be underpinned with recommendations such as standard keyword detection. Standard keyword detection requires a regularly updated list, and tracking of emerging trends. The efficacy of the tool hinges on fraudsters using very specific keywords, content of keyword lists and the manner in which keyword detection is conducted. As Ofcom has conceded, fraudsters may circumvent keyword detection. It thus relies on firms modifying their systems and constantly updating their keyword lists according to the language that fraudulent actors use. We have not seen appropriate caveats that a regular review of the list,\\nbased on emerging technologies, would occur. \"\\nAutomated content moderation (User to User) \\t\\t\"Ofcom also decided against introducing fraud keyword detection in tackling illegal financial promotions and investment scams. Agree with this assessment - standard keyword detection is not the best automated content moderation tool for the\\ndetection of all fraud types\"\\t\"Ofcom also decided against introducing fraud keyword detection in tackling illegal financial promotions and investment scams. Agree with this assessment - standard keyword detection is not the best automated content moderation tool for the\\ndetection of all fraud types\"\\nAutomated content moderation (User to User) \\t\\tAn outcomes based approach to automated content moderation (ACM) would be far more effective to mitigate concerns around the limitations of one specific technology, rather than simply not requiring ACM for fraud as a whole. Monzo provides a summary of its points so far. An outcomes based approach to automated content moderation (ACM) would be far more effective to mitigate concerns around the limitations of one specific technology, rather than simply not requiring ACM for fraud as a whole. Instead, firms should be required to use the data points available to them, to develop automated content moderation tools. This can include standard keyword detection, but as this may generate false positives for certain fraud typologies, it cannot be limited to such measures. Mandating one specific action - standard keyword detection - effectively precludes firms from being incentivised to improve and  utilise the technology they have deployed in other areas (such as machine learning or AI tools, trained using multiple data points, to identify said content). Firms should be incentivised to discharge their regulatory requirements in a tech neutral manner. If false positives are a concern as a result of automated content moderation, high risk content that is flagged by the ACM tools can be escalated for human review, flagged publicly to other users on the platform as ‘under review’, and subsequently checked for further indications of illegal harm. Automated content moderation (User to User) \\t\\tOfcom should prioritise the potential harms averted over the relative costs involved. \"Given that the focus of our response is on large user-to-user services such as Big Tech and social media platforms and that standard keyword detection is only being applied to these large services, we think Ofcom should prioritise the potential harms averted over the relative costs involved. From October onwards, PSPs will be required to to mandatorily reimburse victims of\\nAPP fraud up to £415,000 per case. Considering that over 60% of all APP fraud originate via Meta according to UK Finance data , we consider it fair and proportionate to compel Big Tech and social\\nmedia giants’ to bear costs in order to buttress its fraud prevention systems and upskill their staff. Ofcom’s estimated costs for large firms to introduce standard keyword detection pale in comparison to what the PSR is expecting PSPs reimburse each victim of APP fraud, regardless of their size.\"\\nUser reporting and complaints (U2U and search) \\t\\t\"Recommendation 3: Trusted Flaggers list must be expanded to include UK\\nFinance, and a limited pool of financial services firms.\"\\t\"The proposal to establish a dedicated reporting channel for fraud for trusted flaggers is hugely welcome. Right now, we have no timely way to facilitate fraud reports to major user to user services to protect consumers. One piece of fraudulent content can attract multiple victims, thus timely removal and identification is vital. We believe that the dedicated reporting channel should be made available to a wider range of trusted flaggers to include regulated financial services institutions, who are currently required to report Authorised Push Payment scam data to the regulator. There are only 14 financial institutions required to conduct this reporting at present which make up 95% of the volume of payments sent in the UK. This is a relevant measure, because these are therefore the financial institutions who receive the majority of fraud reports, and the regulator has identified as players who have the critical view of Authorised Push Payment scam incidents in the UK. Monzo collects fraud origination data. If this is a user to user service, we collect URLs of the harmful content and screenshots, to help evidence our investigation. We stand ready to share this with user to user services, should Ofcom enable us to access a dedicated reporting channel. For a number of reasons, banks are typically the first place victims go to report the fraud (some further evidence/detail given). Despite banks having some of the best visibility over fraudulent content to alert social media firms of content hosted on their site that has been posted\\nby a fraudster, we are unable to do so in a timely manner. Our fraud investigators have even gone so far as to report fraudulent content from their own personal social media accounts. 14 regulated financial institutions is a far cry from the “many tens of thousands of entities” that Ofcom rightly states are regulated by the FCA. Includes a table  outlining the number of fraud reports relating to purchase scams, by far the most common scam type, that we can with confidence attribute to major social media firms operating in the UK in the last 6 months of 2023 (table follows). Other steps could be taken to reduce the volumes reported through the dedicated reporting channel, through cooperation with financial services industry, Ofcom and the major services to determine more detailed parameters that would trigger a report through the dedicated reporting channel. we believe it would be proportionate for Ofcom to expand the measure to a handful of regulated financial services providers given the scale of harm to ensure these firms are removing harmful content. Need to ensure a suitable direct means of flagging suspected fraudulent activity between financial institutions and online service providers\"\\nUser reporting and complaints (U2U and search) \\t\\t\"If the regulator still believes that it is disproportionate for large firms to have to review fraud reports relating to the content that they are hosting from a comprehensive section of the industry, an alternative approach could be to define trusted flaggers as any Payment\\nService Provider with over 7 million UK customers. \"\\t\"Without reporting, there is a significant risk that user to user services are not effectively able to build a picture of their fraud harms. If the regulator still believes that it is disproportionate for large firms to have to review fraud reports relating to the content that they are hosting from a comprehensive section of the industry, an alternative approach could be to define trusted flaggers as any Payment Service Provider with over 7 million UK customers. This would limit access to only large\\nfinancial institutions, and is consistent with the definition of large user to user services,\\nto whom they would be reporting to. Given the scale of harm emerging from large U2U services, it is not disproportionate for them to have to sift through reports from major Payment Service Providers. If their controls were better, the number of reports would be reduced - providing a healthy incentive to improve outcomes. Cannot allow large services to continue to ignore the intelligence provided by major banks.\"\\nUser reporting and complaints (U2U and search) \\t\\t\"We also encourage Ofcom to\\nconsider expanding this list to include the Payment Systems Regulator and Stop Scams UK in due course\"\\t\"Trusted flaggers covers several bodies - we also encourage Ofcom to consider expanding this list to include the Payment Systems Regulator and Stop Scams UK. The PSR are progressing work to standardise and collect “fraud origination” data from the 14 financial institutions within the scope of the APP scam reporting requirements. Stop Scams UK is also conducting similar\\nwork.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9906266927719116,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'This would allow services to understand how their fraud footprint is evolving and spot\\nwider trends. This data can then be leveraged to adapt risk assessments and content\\nmoderation systems to better prevent different these illegal harms\"\\nUser reporting and complaints (U2U and search) \\t\\tRegarding the recommendation that at least every two years the service should seek feedback from the trusted flagger, we suggest this is reduced to one year\\tRegarding the recommendation that at least every two years the service should seek feedback from the trusted flagger, we suggest this is reduced to one year. This is on the basis that the fraud ecosystem and threats evolves so fast, so as to ensure an up to date approach and way of working. This also reflects the cycle within which financial institutions are required to review their own risk assessments, to ensure they are adapting to emerging threats. Enhanced user control (U2U) \\t\\tE-commerce and major peer-to-peer marketplaces should be required to verify sellers on their platform, and the goods they are advertising\\tE-commerce and major peer-to-peer marketplaces should be required to verify sellers on their platform, and the goods they are advertising. Such processes can easily be automated, thereby removing anonymity to crackdown on the disproportionate amount of fraud driven from these sites. Ofcom notes that some types of services, like online marketplaces, operate forms of verification, but we highlight this is not standard practice, and as a result requires urgent attention. Platforms like Facebook Marketplace where identity verification is not required to make a sale have been well publicised in terms of how poor their controls are, with the potential to fraudulent listings and possible sales \\nEnhanced user control (U2U) \\t\\tThere should be a basic policy condition for accessing notable user verification, designed to prevent the ability of criminals from gaining this trustmark\\tWe strongly support Ofcom’s proposal to increase transparency and understanding of users surrounding notable user verification. Notable user verification is ultimately a trustmark, and consumers may believe that a verified user is a legitimate individual, regardless of the fact that many firms do not have robust policies designed to prevent impersonation/verify the legitimate identity of the user who receives such marks. Building on Ofcom’s proposals, there should be a basic policy condition for accessing notable user verification, designed to prevent the ability of criminals from gaining this trustmark. Ofcom should place policy restrictions under which these statuses cannot be acquired/maintained, such as being on the FCA watch list for promoting fake investments. These can be very basic policy restrictions, to prohibit prolific fraudsters from gaining access to these trustmarks, while also relying on transparency to educate consumers on the lack of verification surrounding these trustmarks. User access to services (U2U) \\t\\tThe scope of this control must be expanded to accounts violating the fraud, POCA and FSMA priority offences\\t\"Major firms should not simply have requirements to remove accounts related to terrorist offences. The scope of this control must be expanded to accounts violating the fraud, POCA and FSMA priority offences. Information from consumer complaints and trusted flaggers, as well as other legitimate\\ndata sources (e.g. the FCA Watchlist) must be considered. At the moment, firms will not be required to remove accounts on the basis that these profiles should sit outside of their risk appetite i.e. are at such a high risk of driving consumer harm. Ofcom should introduce a generic requirement for firms to remove account access to consumers outside their risk appetite, and the risk appetite should be informed by consumer complaints, information from trusted flaggers, internal analysis and publicly available data sources such as the FCA watchlist.\"\\nApproach to the Codes\\tDRC  Dedicated Reporting Channels \\tAgree with our proposals. They particularly welcome the proposal to implement a dedicated Fraud Reporting Channel for applicable services. Approach to the Codes\\tSegmentation \\tThey agree that Ofcom should apply the most onerous measures to large and / or medium or high risk services. They state that when considering criminal investigations supported by National Trading Standards where there is a clear link to online activity, a significant proportion of those investigations stem from online services that would be considered large and/or medium risk. Equally, when considering online disruption activity undertaken by National Trading Standards (domain suspensions, removal of harmful social media content etc.) a significant proportion of that activity relates to online services that would be considered large and/or medium risk. Approach to the Codes\\tDefinition of large services \\tThey agree but have some concerns. They note that it is somewhat difficult to provide empirical evidence to support the view as in many ways a “line must be drawn somewhere”. Therefore, they believe the definition proposed appears to be proportionate, particularly as it based on existing (albeit EU) legislation\\nApproach to the Codes\\tDefinition of multi-risk\\tAgree with our proposals. Services that tend to exhibit a high risk in relation to online fraud (for example search and social media) will, in our view, also pose a high risk in relation to other forms of illegal harm and would therefore meet the proposed definition of a multi-risk service. Approach to the Codes\\t\\tAgree with our proposals but they also want to be a trusted flagger. They broadly welcome the proposed Codes of Practice, in particular where it is proposed services should have due regard to information sourced from persons with expertise in the identification of content that might be considered fraudulent. We also welcome the proposals to develop dedicated fraud reporting channels and the notion of ‘trusted flaggers’. They would urge consideration to expand the proposed list of ‘trusted flaggers’ to include Trading Standards. National Trading Standards currently undertakes work to disrupt online harm directed at UK consumers. This includes, amongst other things, domain suspensions and the removal of harmful social media content. They believe the additional of ‘trusted flagger’ status will further enhance the protection of UK consumers. Content moderation (Search)\\tConsistency across services\\tProposed measures will enable consistency among the services. They state that he majority of large services will already have some elements of these proposals in place, however these proposals should help bring consistency across all applicable services. Automated content moderation (User to User) \\tKeyword detection \\tAgree with using human moderation as a means to support automation as fraud is too large to be dealt with through just human review. However, they do raise AI and learning models. The volume of fraudulent content on services is simply too large to reasonably expect services to rely solely on human moderation, or referrals from regulators, law enforcement and the like. As such, an element of automated moderation is a key part of the response. This is particularly so with the increase prevalence of AI and learning models. As described in other areas of the consultation, supporting this with human intervention and the concept of ‘trusted flaggers’ is likely to provide a more effective means of identifying and removing content, whilst assisting in the continued development of the learning models used by services. Automated content moderation (User to User) \\tKeyword detection \\tThe costs are justified\\tThey agree the costs are likely to be significant for some services, however they believe this is justified given the acknowledged scale and harms associated with online fraud. Evidence: National Trading Standards has supported the prosecution of a number of fraud and money laundering cases where the articles for use in the fraud were procured through online services. National Trading Standards has also undertaken disruption activities alongside other UK regulators where the articles for use were procured online (for example, in the sending of millions of unsolicited text messages using ‘SIM farms’). The criminal cases and associated disruption activity have involved tens of millions of pounds in harm and affected millions of UK consumers. Automated content moderation (Search) \\t\\tURL detection extend to fraud\\tThey would suggest that consideration be given to extending these proposals to other illegal harms, including fraud. User reporting and complaints (U2U and search) \\tDRC  Dedicated Reporting Channels \\tAgree with proposals and give further evidence \\tThey agree, in particular with the proposals to establish and maintain a dedicated reporting channel for trusted flaggers and to provide simple and consistent reporting services for the public. Evidence: National Trading already undertakes a range of activities to disrupt online harms directed at UK consumers. The activities are an important part ensuring UK consumers are protected from being exposed to the risk of a range of online frauds. In many of these cases, the arrangements by which we highlight content to online services is not covered by any formal and/or statutory process. It is often based on mutual trust in that National Trading Standards has consistently demonstrated it can accurately identify harmful content and therefore the online services we work with can be confident in removing such content. They believe this work demonstrates the key reasoning behind the concept of being designated as a ‘trusted flagger’ and these proposals would build upon existing ‘informal’ arrangements and further strengthen protections for UK consumers. Additionally, as set out in our response to Q10, there is an acknowledged chronic underreporting of online fraud. Providing simple and consistent means by which fraud (and other illegal harms) can be reported by the public may assist in driving up reporting levels and help to identify and mitigate harmful content more quickly. Terms of service and Publicly Available Statements\\tPublicly accessible information\\tAgree - publicly accessible information is key to building public confidence\\tThere is clear evidence of a strong consumer sentiment that “not enough is done” to tackle online fraud. They believe publicly accessible information is key to building public confidence and helping demonstrate that positive steps are being taken to tackle online fraud. Service design and user support (Search) \\tPredictive search \\tRelevant measure for fraud\\tThey have seen evidence (particularly in cases similar to those referred to in their response to Q49) where predictive search has yielded suggested searches that would likely lead to fraudulent content. As such, these proposed measures could help mitigate against consumers accessing such content. Cumulative Assessment  \\t\\tCriminals use large services because eof the reach \\tThey state that the proposals seem reasonable, however in their experience the risk of fraud (in particular) is low from small/micro services due to the propensity for those looking to commit fraud tending to use large services, principally due to their reach. It is therefore difficult to provide a definitive opinion in respect of these proposals. Cumulative Assessment  \\t\\tRisk of large services \\tThey state that large services tend to pose a much higher risk of exposing consumers to large-scale fraud and as such, the additional burdens are appropriately reflective of the scale of harm caused. Governance and accountability  \\tCosts \\tAgree that good governance and accountability is sufficiently important to justify these costs. The direct and indirect financial losses, personal impact and wider impact on the UK economy are significant. They agree that good governance and accountability is sufficiently important to justify these costs.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9912631511688232,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Governance and accountability  \\tServices in scope \\tAgree with the proposals. They state that in almost all the cases supported by the National Trading Standards eCrime Team, the route to consumers being defrauded has been via search or social media content. The proposals to include all ‘large’ and ‘multi-risk’ services in the enhanced governance and accountability measures would capture this risk. Governance and accountability  \\tEfficacy, costs and risks\\tIndependent auditing of services' measures. They believe it is important to explore in detail the future potential for independent auditing of services’ measures to ensure they are sufficiently detailed and robust. Approach to the Codes\\t\\tThey agree with Ofcom's statement “Effective content moderation systems or processes allow services to identify and remove illegal content swiftly, accurately and consistently.” (Volume 4 12. U2U content moderation (P.19)). It is important for animal protection groups to be included in the consultation process for the Codes of Practice. They comment that accountability and consistency are important, as is having confidence in the systems employed by services. Action for Primates (and others) have often found a lack of urgency, action and consistency when reporting illegal harmful to social media platforms which often fail to implement their own policies. They agree in the importance of Codes of Practice, and to keeping such codes under review. To ensure animal cruelty content receives the optimal amount of scrutiny when it comes to defining content that is harmful for children, they believe it is important for animal protection groups to be included in the consultation process for the Codes of Practice\\nUser reporting and complaints (U2U and search) \\t\\tThere is a significant ongoing communications challenge for the regulator, industry and wider stakeholders \\tOrganisations with an interest in online safety are not yet well informed about respective roles, unless they are already very closely involved and contributing to Ofcom’s work. There is a significant ongoing communications challenge and we look forward to seeing public understanding of the respective roles for Ofcom and services develop, especially among the most vulnerable and affected groups. Ofcom’s effectiveness in enforcing services to play their role in pre-empting and resolving harms will be to the fore in ensuring the regime works and Ofcom’s own resources are not overwhelmed by reports which should be directed to the services. Ofcom also has a critical role to play in proactively identifying emerging and evolving issues to ensure that the regime remains relevant and delivers its aims. User reporting and complaints (U2U and search) \\t\\tThe process by which other organisations and consumers can bring wider issues to Ofcom’s attention is less clear and would benefit from further definition and communication. Supercomplaints will also be an important means of bringing key issues to light. However, the process by which other organisations and consumers can bring wider issues to Ofcom’s attention is less clear and would benefit from further definition and communication. Will they, for instance, have ready access to a sufficiently large, diverse and resourced network of supercomplainants (including NI based organisations independent of government) or will there be more direct access to Ofcom? It is important that there is an ability for wider issues experienced at grass roots level to permeate in a timely manner, and we are mindful that there can be organisational, cultural and resource barriers to this. User reporting and complaints (U2U and search) \\t\\tAgree with the guidance which Ofcom has laid out, noting that this is a major shift in practice for many services. Suggest that Ofcom moves quickly to enforce these new requirements where necessary. It is hugely important that user reporting and complaints systems, as well as Terms of Service/Publicly Available Statements are easy to find, access and use so that issues can be raised and resolved quickly. Even large organisations find it difficult to get major platforms to engage with their complaints and reports of harms, unless they resort to legal action. We agree with the guidance which Ofcom has laid out, noting that this is a major shift in practice for many services.\": [{'label': 'POSITIVE',\n",
       "               'score': 0.9973247051239014,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'We therefore would suggest that Ofcom moves quickly to enforce these new requirements where necessary. Terms of service and Publicly Available Statements\\t\\tThe requirement for writing Terms of Service at the reading age comprehensible to the lowest age of user permitted to agree to them is extended to also take into account the range of people at any age who may benefit from an easy read version, including – but by no means limited to - adults with learning difficulties. Welcome the particular consideration given to children and people with disabilities in Ofcom’s guidance in this regard. However, we suggest that the requirement for writing Terms of Service at the reading age comprehensible to the lowest age of user permitted to agree to them is extended to also take into account the range of people at any age who may benefit from an easy read version, including – but by no means limited to - adults with learning difficulties. The broad benefits of ‘easy read’ explanations for users should also be a consideration for reporting/complaints arrangements. Approach to the Codes\\t\\tOfcom’s guidance could encourage services to consider whether any aspects of requirements that are not mandatory at their level could nonetheless be helpful in supporting good practice and a more systematic and potentially cost-effective way to protect against harms\\tOfcom’s guidance could encourage services to consider whether any aspects of requirements that are not mandatory at their level could nonetheless be helpful in supporting good practice and a more systematic and potentially cost-effective way to protect against harms. This might particularly apply to services which could expect to be redesignated as ‘large’ as their user base expands. Approach to the Codes\\t\\tInterested to understand more about Ofcom’s processes for providing ongoing guidance and alerts to service providers in relation to emerging and evolving online harms\\tInterested to understand more about Ofcom’s processes for providing ongoing guidance and alerts to service providers in relation to emerging and evolving online harms, so that good practice can be developed as early and consistently as possible. \" Default settings and user support (U2U)\"\\tSearch\\tSearch engines should adopt user support information \\t\" It is our view that Ofcom’s proposals should be further strengthened in places. Higher standards and robust responses against fraud are essential to end the misuse of digital services for criminal purposes and to empower victims. In this response, we will refer to ‘fraud’ in a wider sense that includes both fraud and the wider financial services offences\\nincluded on the face of the OSA. Our specific recommendations are set out in detail in the pages to follow, but they concentrate on three main themes. We recommend that:\\n• The proposals in the Codes for search engines to adopt systems and processes\\nhighlighting user support information should be extended to high-risk financial\\nsearches.\"\\nApproach to the Codes\\tCodes should be strengthened\\tCodes should be strengthened and future proofed\\t\" It is our view that Ofcom’s proposals should be further strengthened in places. Higher standards and robust responses against fraud are essential to end the misuse of digital services for criminal purposes and to empower victims. In this response, we will refer to ‘fraud’ in a wider sense that includes both fraud and the wider financial services offences\\nincluded on the face of the OSA. Our specific recommendations are set out in detail in the pages to follow, but they concentrate on three main themes. We recommend that:\\n• The Codes should be strengthened and future-proofed, setting a clear standard \\nfor compliance which cannot be exploited by services to do the bare minimum, or \\neven less than they currently do.\"\\nService design and user support (Search)\\tSearch features, functionalities, user support\\t\" all search engine services used by \\nconsumers to find financial services and financial products should be able to present fraud-prevention user support messaging\"\\t\"[in relation nto the recommendation of provision of suicide crisis prevention information for search - speak about how they agree with it, and provide some details of its relevance to Fraud]\\nThe FCA provides free and accessible help to consumers via our ScamSmart and \\nInvestSmart campaigns. This includes pointing those seeking to invest towards \\nchecking that the firm they are planning to give their money to is an authorised firm. Given the evidence of harm, we consider that all search engine services used by \\nconsumers to find financial services and financial products should be able to present \\nfraud-prevention user support messaging similarly to how user support messaging \\nis shown for e.g. self-harm prevention. [FCA continues to provide evidence as to the effectiveness of this and state they understand regulated services may be resistant to this.]\"\\nService design and user support (Search)\\tSafety by design\\tindexing policies and systems should be developed with a focus on ensuring that official entries (Government, statutory bodies, regulators) show first in the search list for potentially high-risk investments and for financial help content. \"We also consider that regulated services can do more with regard to the indexing of \\nsearch results in ways that are consistent with ‘safety by design’ principles. In the case of debt advice instance, help available to consumers from regulators (The \\nMoney and Pensions Service “MaPS”) and high-profile non-profit specialists (Citizens  Advice, StepChange) consistently appears below material related to fee charging debt \\nadvice firms. This could either because the commercial firm is paying for advertising to appear at the top, or as a result of search optimisation. However, this can cause real harm as these fee-charging firms at times can impersonate reputable not-for profit and charitable organisations to hook the searcher, and/or may make misleading claims implying that they are government endorsed and offering unsubstantiated claims of achievable debt trade-offs (StepChange). To be successful in minimising risk of harm and illegal activity targeting users, indexing \\npolicies and systems should be developed with a focus on ensuring that official entries (Government, statutory bodies, regulators) show first in the search list for potentially high-risk investments and for financial help content, to ensure that vulnerable users are able to identify official advice. Being able to access impartial official information is essential for consumers. We use paid search activity to advertise FCA official information on searches and our analysis on the performance of our adverts shows us that users are interested in material that can help them make informed decisions if/when this is presented to them at the right time. For example, on our InvestSmart campaign, on Phase 8 (January – February 2024) we saw a click through rate of over 10% - higher than industry benchmarks of between 6% and 8%. This illustrates not only the benefit of placement (top or near top of search listing), but also the importance of the message in the context of a  consumer journey for purchasing a financial product or service.\"\\nApproach to the Codes\\t\\tThe standards should, as far as possible, point to the specific outcomes which Ofcom are expecting\\tWe recommend that the standards should, as far as possible, point to the specific outcomes which Ofcom are expecting, to avoid allowing services to do the bare minimum or even less than they currently do to protect users. Also, we consider this would help future-proof the guidance and codes to accommodate developments in technology/best practice/evolution of online fraud without the need to redraft sections of the documents. We believe this is especially relevant around the standards for detection, reporting, review and removal of illegal content. We have included some examples of where we believe a more outcomes-based approach would be helpful in terms of timeframes and future-proofing/use of technology\\nApproach to the Codes\\t\\tWe recommend instead that guidance and recommended measures set an expectation that wherever issues are identified by regulated services or reported by trusted flaggers that these are responded to “as early as possible” or “in a timeframe agreed with the flagger”\\t\"In our experience, cooperation with tech platforms can change (and deteriorate) \\nextremely quickly, depending on a range of different factors. We therefore believe \\nthan in the interest of futureproofing, there is no need to set highly specific timeframes on any of the codes’ recommendations, e.g. such as the currently recommended reviews with trusted flaggers reporting channels every 2 years, or key word detection at least every 6 months. Such timeframes carry the risk of leaving issues unaddressed for too long before a review is required (and action taken) and ‘dropping the bar’ to the lowest possible effort a service can make, which in turn could undermine the benefit of the proposals. We recommend instead that guidance and recommended measures set an expectation that wherever issues are identified by regulated services or reported by trusted flaggers that these are responded to “as early as possible” or “in a timeframe agreed with the flagger”, particularly in the first years of the implementation of the  online safety regime. The guidance could be further strengthened with baselines for compliance which can align with the expectations in the Home Office Online Fraud Tech Charter (e.g. that reporting mechanisms for fraud are accessible in two clicks, and that removals for content and users take place ‘straight away’). [give further indication as to why they think it\\'s appropriate]\"\\nAutomated content moderation (User to User) \\t\\tRecommend measures other than keyword detection for fraud\\twe believe that Ofcom guidance can accommodate developments in technology and that the proposals in the consultation should be further strengthened – for instance, including expectations around the use by regulated services of machine learning and URL detection to tackle fraud in addition to keyword detection. Online services (particularly the largest regulated services) have access to extensive technological capital that they can leverage, and they have been following the progress of the legislation for years. Content moderation (User to User) \\t\\tSnap welcomes the broad approach Ofcom set out, recognising that a one-size fits all approach would not be appropriate and allowing for flexibility for services to establish effective systems in line with their platform design. Snap recommend that Ofcom considers the content moderation part of their response in conjunction with Snap’s submission on the Enforcement of Terms and Conditions dated 25 April 2023 under the VSP Regulation. Content moderation (User to User) \\tContent Moderation Systems. Snap agree with Ofcom\\'s recommendations that the systems / processes that services put in place must be designed in a way that ensures known illegal content is removed swiftly, but that \\'swift\\' removal should not be tightly defined and further clarity should be given on recommendations. \"Snap operate internal performance targets, which include the timeliness in which reports are\\nenforced or taken down (see below) and their Transparency Report provides median turnaround times per harm type. https://values.snap.com/en-GB/privacy/transparency Snap have concerns about the statement that systems should \\'make an illegal content judgement in relation to the content and, if it determines that the content is illegal content, swiftly take the content down\\', and would not recommend this as an appropriate course of action. They state that the implication that each piece of content would require legal analysis and judgement by services\\nraises human rights concerns (citing legislation in Germany and France). They are aligned to the option in which systems should \\'where the provider is satisfied that its terms and conditions for the service prohibit the types of illegal content defined in the Act which it has reason to suspect exist, consider whether the content is in breach of those terms of service and, if it is, swiftly take the content down\\'. However they would welcome operational clarification on the level of granularity required as part of the service\\'s terms and conditions. For example,\\nwould it suffice that the terms and conditions cover the relevant illegal harms by subject with a recognition that users must follow local laws, including UK laws? They state it would be inappropriate to cover each illegal harm in granular detail under the UK legislation given that these may not apply to every jurisdiction in which they operate. Snap recommend that in the context of content moderation, Terms of Service also remain sufficiently high-level regarding illegal content prohibited on the service to maintain overall consistency of approach.\"\\nContent moderation (User to User) \\tInternal Content Policies. Snap argue there should be stronger recognition by Ofcom in the consultation of the need to also assess the effectiveness of services\\' internal policies and guidance for moderators. Snap agree that service providers should have internal content policies specifying what content is or is notallowed on the platform, as well as how they should be operationalised and enforced (they state they have already demonstrated compliance here through VSP regulation requirements). They state that despite Ofcom’s suggestion that performance targets could mitigate the risk of stifling free speech, there is still a risk that services may over-enforce and remove legal content to hit time-based targets while erring on the side of caution, especially during times of acute pressure. Lastly, Snap agree that to mitigate these risks, it is important to quality assure the targets and ensure they remain effective. The level of content reinstations may also be used by services to assess effectiveness (although they argue this may be less applicable to Snapchat given the ephemeral nature of content). Content moderation (User to User) \\tPrioritisation\\tAgree that service providers should operate a content moderation system setting out certain factors to which they should have regard, but the factors articulated by Ofcom require clarification. \"Snap argue that whether virality should be a factor in prioritisation depends on the nature and design of the platform and this should be reflected in Ofcom\\'s final guidance (much lower risk on Snapchat). For example, Snapchat doesn\\'t offer tools that allow for quick and unvetted \\'reposting\\' or \\'resharing\\', arguing that this is a design choice to improve platform safety, despite the added engagement such tools could bring. They also have no open newsfeed or option to live-stream, making it suboptimal to broadcast UGC. Snap also agree that severity should be a factor for prioritisation, but that severity may not align with Ofcom\\'s priority illegal harms. For example, a harm may be considered “severe” or “priority” by Ofcom, but have almost no prevalence on the service. Lastly, Snap agree that signals (such as reporting / complaints from Trusted Flaggers) can be a clear indication on the likelihood that content is illegal and should be prioritised for removal. However, they are cautious about the weight Ofcom places on Trusted Flaggers. For example, Community Security Trust is a Trusted Flagger at Snap and since the\\nIsrael-Gaza conflict began, Snap have received one report of anti-semetic content from them, which was reviewed and removed. However, this did not indicate that hate crime or anti-semitism was prevalent on Snapchat and the need to therefore change internal processes. Based on other signals and platform design, Snap were confident that Snapchat does not provide an environment for harm, such as antisemitism to flourish. Snap suggest that Ofcom considers broader criteria in making this assessment.\"\\nContent moderation (User to User) \\tResourcing\\tPlanning ahead with regards to languages and external events make sense, however many evens such as wars and pandemics are unpredictable and in these circumstances Snap rely on playbooks and past learning. Snap agree that it is sensible to consider what languages are common in the regions in which they operate and to ensure language expertise as part of their moderation efforts. Also that they should plan ahead and anticipate demand for content moderation for certain external events such as elections. However, many external events are unpredictable and for these they rely on playbooks and past learnings. For example, to manage their response to the Israel-Gaza conflict, Snap used vendors for additional capacity (including on language) and this was flexed depending on the volumes and prevalence of content related to the conflict on Snapchat. Snap found that there were increases in content detection and removals during critical moments but that these were not sustained over a continued period of time and the levels were manageable. Snap note that the pull on resources not only impacts content moderation but business-wide resourcing, resulting in re-prioritisation of resources rather than new resourcing. For example, in relation to the conflict in Israel and Gaza, Snap set up a cross-functional Safety and Regulatory Governance Group to closely monitor the situation, share information and manage the crisis from all angles. This led to policy calibrations and product changes to support detection and moderation of associated content. Lastly, Snap warned Ofcom to be more concerned with quality than quantity and how platforms will require different moderation and resourcing levels. Content moderation (User to User) \\tProvision of training and materials to moderators. Snap largely agree with these proposals, however they feel Ofcom have underestimated the costs of training. Snap appreciate Ofcom\\'s observation that there\\'s no universal best practice for training content moderation teams. They state that what will be effective for each service depends on the nature of the platform, products offered and user demographic, etc. Snap noted that their VSP regulation submissions provide details on Snap\\'s training practices, which align with the recommendations made in this consultation. Snap also note that they already provide real-time guidance, advice and support to content moderators. However, they feel that the estimated costs provided by Ofcom omit the significant costs and efforts associated with providing training in a constant and iterative manner. Automated content moderation (User to User) \\t\\tBroadly agree with the reasoning and proposals, however they have concerns with the proposal on fraud keyword detection. On the whole Snap agree with the reasoning and proposals and to a large extent they reflect measures Snap already employ through CSAM image scanning and hash matching with PhotoDNA and Google CSAI, and proactive detection methods that include URL reputation service flags. All of these occur in Snapchat’s public communication spaces (and also in private communications such as scanning 1:1 chat media for CSAM). However, Snap believe the use of keyword detection or an ‘Abusive Language Detection’ (ALD) tool (as in Snap’s case) should be used more broadly to support service efforts to detect and moderate illegal content – not only fraud, and in particular, articles for use in frauds (i.e. stolen credentials). They refer back to comments in their Summary and Dedicated Fraud Reporting Channel parts of their response. Snap believe  Ofcom\\'s assessment that the effectiveness and accuracy of standard keyword detection is a reason it\\'s not being recommended, should be at the discretion of the service. They also note that while Ofcom\\'s research shows fraud is the most commonly experienced illegal harm, this won\\'t be true on every platform. Automated content moderation (User to User) \\t\\tClarification required on nuanced issues in fraud. Snap argue a service may consider itself to be at high or medium risk of certain types of fraud (e.g. romance scams) in its risk assessment but this risk may not be associated with articles of fraud (e.g. stolen credentials). It would therefore be unreasonable and disproportionate for services to adopt the keyword detection measure. They say it\\'s therefore unclear how Ofcom distinguishes these nuances in risk and more clarification is required. Snap  consider the risk of online fraud to be relatively low on Snapchat but  do use their ALD tool to detect keywords that may be associated with frauds seen on the platform (e.g. money making schemes). They regularly review keywords as new trends, fraud or solicitation patterns emerge. Automated content moderation (User to User) \\t\\tThis recommendation is aligned to Snap\\'s approach and is appropriate in striking a balance between advancing the safety of users while honouring their privacy rights. This recommendation is aligned to Snap\\'s approach and is appropriate in striking a balance between advancing the safety of users while honouring their privacy rights. User reporting and complaints (U2U and search) \\tComplaints\\tAgree with the majority of Ofcom\\'s recommendations in relation to complaints handling. Snap appreciate the flexibility to estalish the most appropriate and proportionate approach. They have updated their reporting and complaints system on Snapchat to ensure users can exercise their rights to raise an issue or appeal a decision, to comply with the DSA. The provide details on what happens when they recieve reports and complaints from users. User reporting and complaints (U2U and search) \\tIndicative timelines for complaints\\tSnap agree that complainants should receive an acknowledgement that their complaint / report has been received, however they have concerns on the requirement that an indicative timeline is provided to the complainant. Snap argue that they have internal goals and metrics regarding turnaround times for complaints, but it would be very hard to adopt a one-size fits all approach on a timeline for user complaints. They state that this would probably result, operationally, in users just being sent the longest possible time frame each time as Snap cannot assess ahead of time how complex the issue being reported is. Snap therefore suggest instead including turnaround time metrics in transparency reporting obligations (which they already do), or suggest that if a complaint has not been resolved within a reasonable time frame (as decided by the service provider), Snap provide the complainant with an update, including if they need more time to handle their case. https://values.snap.com/en-GB/privacy/transparency\\nUser reporting and complaints (U2U and search) \\tAppeals\\tAgreed with a number of Ofcom\\'s recommendations on appeals processes of content moderation and enforcement and called out ones they already do. \"Measures Snap already take that are recommended by Ofcom include: -Service providers deal with appeals of content moderation and takedown decisions promptly. -Service providers should set their own internal metrics and performance targets for appeal\\ntimeline and accuracy decisions and adhere to them. -Service providers notify users of their rights to bring proceedings for breach of contract if the useof proactive technologies has been used to takedown content or restrict access to content in  a way inconsistent or not contemplated by our terms of service. -Service providers establish a triage process for relevant complaints, with a responsible team\\nleading this process and ensuring that complaints reach the most relevant function or team\"\\nUser reporting and complaints (U2U and search) \\tPrioritisation of appeals\\tSnap agree with some criteria put forward on how service providers should prioritise appeals (prioritising based on severity and date / chronology), but have 2 main concerns. Concerns: 1) Whether the decision that the content was illegal content was made by proactive technology: this involves a moving target, as proactive technology in detecting illegal harm content is constantly improving and changing. Therefore, developing a prioritisation system for appeals based on the accuracy of proactive technology would be extremely difficult and require significant resources to constantly update it to real-time. 2) The service’s past error rate in making illegal content judgements of the type concerned: as above, this creates similar operational challenges and would require constant updating; diverting vital resources from working on the technology and/or moderation and appeals (recognising that Snap has finite resources as a smaller, challenger company). User reporting and complaints (U2U and search) \\tAction following appeal determination\\tSnap agree with the recommendation that if a service provider reverses a content moderation decision on the grounds the content was not legal then the provider should adjust the relevant meration guidance and its automated moderation tech to prevent errors in the future. Despite agreeing with this, Snap note that the ephemeral nature of content on Snapchat (it deletes by default after a short period of time) makes it hard to \\'restore\\' content that was erroneously taken down. They therefore urge Ofcom to consider platform differentiation as part of proposals to ensure follow-up action is appropriate. User reporting and complaints (U2U and search) \\tDedicated fraud reporting channel\\tSnap are concerned with the proposals recommended in this section and urge Ofcom to take a more proportionate approach to \\'maintain the integrity of the legislation\\'. \"Snap believe this part of the consultation places the full weight on one specific harm type: online fraud (outside of the codes of conduct stipulated by the Act). Noting Ofcom\\'s research on the increasing risk of online fraud, they don\\'t believe this justifies a diversion from the\\nlegal position and the prescriptive requirements to tackle online fraud - compelled further with the specific focus on financial fraud when online fraud is a complex issue and can encompass many different types. Snap urge Ofcom to take a more proportionate approach to maintain the integrity of the legislation. A Trusted Flagger programme or Dedicated Reporting Channel should be deployed as an effective system to help service providers tackle illegal content and understand any emerging threats on their platforms – not one that prioritises fraud, especially when it may be deemed as medium risk against other much higher-risk illegal harms. Snap determine the risk of financial fraud on Snapchat as relatively low (more details will be provided at a later date as part of Snap’s risk assessment under the Act). Snap are not clear based on the measures proposed whether their existing Trusted Flagger programme would meet Ofcom’s requirements or whether there would be an expectation to go further and create a dedicated route within their programme for fraud. They would strongly argue against this, as it does not align with the risk profile on Snapchat and would raise serious capacity concerns for the Trust & Safety team who lead on these relationships. Snap also disagree that all Trusted Flaggers should be onboarded to ensure parity on accessibility and engagement with platforms; rather this should be left at the discretion of the service provider. At Snap, they have a clear policy and process for onboarding and reviewing the status of  Trusted Flaggers. This is assessed on a case-by-case basis and on criteria including the organisation and their relative expertise; types of harm(s) they wish to report; volumes and risk/relevance on Snapchat. If Snap were expected to onboard all these organisations as Trusted Flaggers, this would also raise capacity concerns, as mentioned above, as well as increased operational costs. Snap do not believe this is proportionate against the risk of online fraud on Snapchat and should not be at the expense of focusing on those illegal harms which are high-risk. It would also be helpful to understand whether Ofcom’s proposed list of Trusted Flaggers is exhaustive or will be reviewed and added to\"\\nTerms of service and Publicly Available Statements\\tSubstance of Terms of Service\\tOn the whole, Snap support the guidance but believe that disclosures about a service\\'s safety strategy and methodology should be kept at a high level. They believe disclosures around a service’s safety strategy and methodology – including how we protect users and how the technology works – should be kept at a high-level for various reasons: 1) to avoid overwhelming the user; 2) to ensure content is at an age-appropriate reading level (Snapchat does not age-gate its content or information; it must all be appropriate for a 13+ audience); 3) to prevent potential violators from gaming their systems and evading technologies; and 4) to remain flexible and agile to changing factors, including real world events and user needs, so that adjustments can be made quickly as needed. Snap note the further steps they take, such as Community Guidelines, policy explainers and a dedicated safety centre, as well as in-app notifications and resources for users\\' awareness. Some of these provisions are in direct response to the DSA (e.g., policy explainers) and Snap asked if Ofcom could consider alignment to ensure a consistent approach to ToS. Terms of service and Publicly Available Statements\\tClarity and accessibility\\tSnap would like to understand why Ofcom took a more prescriptive approach on its recommendations here than the one proposed under the VSP regime. Snap already considers Ofcom\\'s recommendations that services provide clear and accessible provisions by considering the 4 factors laid out. However, they want to know why Ofcom took a more prescriptive approach on its recommendations here than the one proposed under the VSP regime. Snap understood that VSP learnings would be used to help inform the OS regime but they cannot see how these learnings have been considered in the consultation (the Content Moderation chapter is another  example where the VSP regime offered key findings that have not been reflected here). Snap also believe that Ofcom’s estimated costs for achieving this level of clarity and accessibility (£16,500 at the high end) are far too low. Snap would estimate that the actual cost of providing terms and provisions that conform to these standards, including thoughtful and compliant design, accessible copywriting, translation services, and design layout would actually cost services at a minimum $250,000-$500,000, not including maintenance costs. \" Default settings and user support (U2U)\"\\tDefault settings for child accounts proposal\\tSnap provide details of the default settings to protect children from harm that they support, but take issue with a number of other proposals. \"Snap agree that default settings for children should protect them from harm, especially grooming and child sexual exploitation and abuse (CSEA). Snap support a number of the measures proposed by Ofcom in Chapter 18 of Volume 4 of the consultation. This includes that child users should not be exposed in connection lists of other users, that children shouldn\\'t recieve DMs from people they aren\\'t connected with, and that user location info should be off by default. However they do not agree with the proposal that children should be removed from network expansion prompts. They cite their own research https://newsroom.snap.com/en-GB/new-research-from-norc-2024 to say that: ● Two-thirds of young people say direct messaging with family and close friends makes them feel extremely or very happy. ● Snapchat users report higher satisfaction with the quality of friendships and relationships\\nwith family than those who do not use Snapchat\\n● Over 90% of Snapchat users say they feel comfortable, happy and connected when using our\\nservice\"\\n\" Default settings and user support (U2U)\"\\tDefault settings for child accounts proposal\\tSnap cite Internet Matters\\' report on digital wellbeing to argue this evidence supports the need to make available responsible network expansion opportunities to teenage users by default. \"Snap cite Internet Matters recent Digital Wellbeing Index report for the UK which found that: ● There has been a rise in the positive developmental, emotional, and social experiences of\\nchildren online from 2022 to 2023. Two-thirds (65%) of children say spending time online\\nmakes them feel at least mostly happy. ● A large majority of children continue to agree that digital technology is key for keeping in\\ntouch with friends (82%). It is also clear that digital devices and online platforms are not just\\nabout games and videos; they are often about community, friendship, and support. This year, 60% of children say that being online makes them feel like they\\'re part of a group. https://www.internetmatters.org/hub/research/childrens-wellbeing-in-a-digital-world-index-report-2024/ Snap argue this evidence supports the need to make available responsible network expansion opportunities to teenage users by default - \\'Quick Add\\' in the case of Snapchat. They further cite the \\'loneliness epidemic\\' https://edition.cnn.com/2023/10/24/health/lonely-adults-gallup-poll-wellness/index.html which recognises that the loss of social connections can\\nhave an acute impact on an individual’s mental and/or physical health. \"\\n\" Default settings and user support (U2U)\"\\tDefault settings for child accounts proposal\\tSnap\\'s recommendations for the codes. Snap argue the Code should allow for older children (i.e. teenager users (13-17)) to be included in network expansion prompts for other users and be presented with network expansion prompts by default – but only where the service has limited these to cases of there being multiple mutual contacts or the contact is already in the users’ device (or OS account) address book. In the case of Snapchat, Quick Add is not powered by random recommender algorithms but rather on the basis of phone contacts or how many mutual friends a user has in common with the suggested friend, among other reliable indicators of potential real life connections between people, before the recommendation is made. For teens in particular, Snap apply heightened safeguards (such as having minimum mutual friends in common) before surfacing them as Quick Add suggestions to other users. Snap recognise the number of child sexual offences in the UK show this is a risk for a relatively small but growing number of children, with devastating consequences for victims. Snap say they have 0 tolerance of this, however, given the strong benefits of reasonable network expansion opportunities to the vast majority of teenage users they do not agree that a total prohibition on network expansion opportunities is the right measure to set in the Code at this time. They recommend a number of measures that services could take to provide reasonable and safe network expansion opportunities - for more details see p20-21. \" Default settings and user support (U2U)\"\\tSupport for child users\\tSnap believe further consideration should be given in the Code to striking a balance between providing info to users at the moment each choice is being made versus part of the user onboarding and education process when their account is created. \"● At the risk of creating an overly burdensome user experience with detailed information presented at the point of every triggering decision or event, there should be:\\ni.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9910231828689575,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'an option for the information provision to occur the first time a user takes the triggering\\naction/experiences the event or there has been a material change/update to the feature;\\nii. an option for users to dismiss future repeated displays due to redundancy, i.e. a “Don’t\\nShow Me Again” button; and\\niii. An option for an infrequent reminder, particularly if a user has not used the service for a\\nwhile. ● To ensure users have access to important information at all times, platforms should also present easy-to-access resources that summarise important information and provide further details for users\\' recourse where appropriate. On Snapchat these can be found via our Support Hub. ● There is a high likelihood that providing the amount of recommended information at the point of any of the listed scenarios in the consultation may result in visual (and even cognitive) overload for child users. Therefore, design flexibility and discretion should be explicitly afforded to services with regard to how such information is presented. This is particularly acute when services like Snapchat are predominantly mobile-app based and there are practical implications to display information on a small screen.\"\\n\" Default settings and user support (U2U)\"\\t\\tLocation sharing should be used. A child user who elects to change their default setting to share their precise location with another user should be provided with information regarding the potential risk of sharing that information with the selected user(s), as well as reminders: i. to only share location with friends that they know and trust; and ii.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9933544397354126,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'how to easily disable location sharing with users. Snap provide this information to  users of Snap Map. Recommender system testing (U2U) \\t\\tSnap disagree with the conclusion as to who the measures should apply to. \"Snap agree that services whose risk assessment indicates that they are medium or high risk for illegal content relating to priority offences should test for the prevalence of such content and log these safety metrics. However, they disagree with Ofcom’s conclusions as to who the measures would apply to. An irresponsible platform, or new platform, would have a significant competitive advantage over an existing platform if they can avoid the cost of deploying on platform testing. In Snap\\'s view, neither Option A, B nor C in paragraph 19.47 should be adopted by Ofcom. Instead, the key factor in determining whether user-to-user services should deploy illegal content testing should be based on whether the service has assessed that they are high or medium risk for at least one type of illegal harm identified in paragraph 19.53 of volume 4, or the extent that the risk is unknown. Snap believe this is proportionate as the costs and complexity of such testing are expected to be proportional to the size of the service and the volume of content being published, as well as the design of the service (i.e. the complexity of a recommender system and whether its design inherently reduces the volume of content needed to be tested). We agree that this measure should not apply to services whose risk assessment\\nindicates that users face a low risk of encountering harms relevant to recommender systems.\"\\nRecommender system testing (U2U) \\t\\t\"Snap strongly believe that all services offering public content spaces should  design them such that they\\nlimit the ability for unmoderated content to be shared widely.\"\\tSnapchat offers two main public content platforms – Stories and Spotlight — where Snapchatters can find public Stories and videos published by vetted media organisations, verified creators, and Snapchatters. In these sections of the app, they limit the ability for unmoderated content to be shared widely. They use proactive detection tools prior to publication and additional review processes to make sure this public content complies with our guidelines before it can get broadcast to a large audience. They argue they have proven that such proactive measures, combined with rapid reaction to reports of illegal content that evades proactive moderation, improves users safety. Therefore Snap see no reason why other services should not make similar proactive content moderation choices. Enhanced user control (U2U) \\t\\tSnap believe the Code should have greater flexibility in how the protection from other individuals is achieved because services are designed differently. \"While we broadly agree that services should provide the ability to block and mute individual\\naccounts, we have concerns in respect of the proposal to provide users an ability to block all\\nunconnected users accounts. In the case of Snapchat, the service is already designed to prevent\\nunconnected users (i.e. users who are not mutually accepted or in their contact book) from\\nexchanging chats and snaps, and viewing / replying to user stories. As expressed in various parts of this response, we do not agree that these requirements should only apply to large services. Blocking and muting functionality is a fundamental design decision for user-to-user services and we find it hard to believe that existing and new services do not consider this design choice when developing their service. Failure to apply this proposal to all services would allow irresponsible design to be contemplated at an early stage in a service’s lifecycle – the antithesis of the Act’s aims on safety-by-design – and will allow those services that do not provide appropriate protections to gain a competitive advantage through reduce protection and safety cost.\"\\nEnhanced user control (U2U) \\t\\tSnap believe the Code should have greater flexibility in how the protection from other individuals is achieved because services are designed differently. \"We agree with Ofcom’s proposal that services should allow users to disable comments on content, and that users be given an easy way to find and use a functionality to disable comments. However, similar to our comment above, we believe the Code should provide greater flexibility in how the desired outcome is achieved. In the case of Snapchat, by default, the user must manually approve any inbound comments before they can appear publicly. We believe this default achieves a stronger protection than that proposed in the Code and effectively prevents any users from commenting on content posted by that user.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9922486543655396,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'Also, as above, we believe the obligation to achieve this protection should apply to all services. We agree with Ofcom’s proposal for requiring services to have clear internal policies and better public\\ntransparency around notable user verification, including paid-for user verification schemes. As above, we believe this proposal should also apply to smaller services. We recognise that smaller services may be less likely to have a notable user verification or paid-for user verification scheme. However, where they do, the key question should be whether a service is subject to a relevant high or medium risk.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9915660619735718,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'While they may have lower reach, ‘smaller services’ can still be quite large in practice. Bad actors removed from larger services are likely to seek smaller services instead and the public is likely to see the most benefit when responsible practices are adopted across the industry.\"\\nEnhanced user control (U2U) \\t\\tSnap believe transparency requirements around how users can disable or block comments would be appropriate. We think that transparency requirements around how users can disable or block comments would be appropriate. However, we recommend that specific methods of transparency should be at the discretion of the services and not left to prescription within the Codes.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9887186288833618,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"Services understand how their users typically engage with their platform, and therefore ought to be best positioned to achieve user transparency in the most effective way. Enhanced user control (U2U) \\t\\tThere's a risk of misuse and exploitation of verification schemes to impersonate or deceive users and thereby expose them to illegal content can arise whether the scheme is voluntary or paid-for. Risk of misuse and exploitation of verification schemes to impersonate or deceive users and thereby expose them to illegal content can arise whether the scheme is voluntary or paid-for. In each case, the risk can best be mitigated by a thorough and rigorous set of processes that services apply when operating a verification scheme to achieve satisfaction that the user is credibly the notable person that they claim to be. User access to services (U2U) \\t\\tMainly agree with the recommendations of the consultation, namely that the service providers remove user accounts where they become aware of its use by a proscribed org. It should be noted that Snap, as a U.S.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9852100014686584,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             \"company, only adheres to the U.S. Department of State and United Nations Security Council’s proscribed list of terrorist organisations. It does not follow local laws in the jurisdictions where it operates in this regard (including the UK’s Terorrism Act 2000). The reasoning for this is two-fold: i) some Governments can designate organisations as terrorists for their own political purposes and not necessarily because the organisation promotes terrorism. Snap operates in many jurisdictions around the globe and needs a uniform policy in this regard; ii) we do not have to rely on a local designation to take action against terrorist organisations. Snap's own internal policies already prohibit hateful content, terrorism and extremism. This includes content that promotes or glorifies terrorist or violent extremist activity. Ofcom may wish to reflect on this clarification within their final guidance. User access to services (U2U) \\t\\tWe apply account blocks based on signals including Device ID; verified email address; verified phone number; and IP address of a violating user. We apply account blocks based on signals including Device ID; verified email address; verified phone number; and IP address of a violating user. User access to services (U2U) \\t\\tSnap provide a detailed account of the advantages and disadvantages of the different options they utilise, including any potential impact on other users. It is Snap’s practice to generally apply account blocks using a combination of these signals in order to give confidence that the violating user is blocked from returning to Snapchat whilst ensuring that legitimate users are not inadvertently blocked from the platform. This requires a case-specific analysis. Of the five signals listed above, the strongest signal in its own right is Device ID, which is an alpha-numeric identifier unique to a cellular device. Although Device ID has some weaknesses (noted below in more detail) when used to block a user, it has the highest likelihood of blocking the violating user without inadvertently blocking legitimate users. As outlined below, the other signals on their own are either not particularly effective or have the potential to negatively impact legitimate users. Because of this, Snap tends to use a combination of Device ID and one or more of the other signals to apply account blocks. User access to services (U2U) \\t\\tList the disadvantages of each signal. Disadvantages of each signal ● Blocking by Device ID may negatively impact legitimate users who share the same device with a violating user. Also, on Android, if the device is reset, we do not know if it is the same device (OEM Android devices (approximately 11 years old devices) there is a risk that the same device ID will stay with the device after the device changes hands and is reset). ● Blocking by phone number may negatively impact legitimate users who obtain a recycled phone number that previously belonged to a violating user who was blocked. It would also be valuable if telecommunications providers or Ofcom could provide access to a free database so companies - like Snap - could unblock devices when phone numbers are reassigned. Currently, we block a phone number for 90 days. We would appreciate guidance from Ofcom if the 90 day period is too short or long, and having access to the aforementioned database could further improve the phone number blocking experience. ● Blocking by email address is not very effective in that bad actors can easily obtain unlimited email addresses to open up accounts with. Therefore, blocking any one email address will not effectively prevent the violator from committing illegal activity. Currently, we block email addresses indefinitely. ● Blocking by IP address is not very effective by itself (as multiple users often share the same IP address or a violating user can use VPN connection to get around a blocked IP address), but can be used in conjunction with other factors (such as phone number or Device ID) to strengthen the case for blocking a user. User access to services (U2U) \\t\\tSnap argue large device Operating Systems (OS) or app stores should be playing a more significant role in providing appropriate infrastructure here.\": [{'label': 'NEGATIVE',\n",
       "               'score': 0.9909539818763733,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             '\"We would stress that this issue is not easily, efficiently or effectively tackled at an individual service level only. Large device Operating Systems (OS) or app store providers should be playing a more significant role in providing appropriate infrastructure so that services can manage blocks throughout the online ecosystem. For example: ● One flaw in service level mechanisms that block device ID is that the block does not persist if the user factory resets their device and is assigned a new device ID. Large device OS or app Store providers could provide a feature that ensures permanent device labels persist across factory resets. ● Another challenge is that there is currently no mechanism for services to learn when another service has applied a block. Large device OS or app store providers should develop an API that would allow services to i) register when the user account/device has been blocked by their service; and ii) check whether other services have blocked a user account/device. A mechanism like this would allow blocks to be applied throughout the ecosystem, preventing a blocked user/device easily jumping to the next service once they receive a block. ● Appropriate mechanisms should also be provided to ensure false positives could be challenged. While large device OS/app store providers have been responsive to queries in this regard, such proposals\\nhave not yet been made available to services. We would urge Ofcom to urgently consult on the\\nrequirements stated in the Act regarding the role of OS/app store providers to ensure more holistic and effective solutions can be sought.\"\\nUser access to services (U2U) \\t\\tThey should be blocked permanently in almost all cases. Notwithstanding an appeal being successful and overturning the decision that led to the block, we generally believe that any user who shares known CSAM should be blocked permanently. However, as discussed below, we believe that the ultimate decision to block a user can sometimes depend on context, and that service providers are best positioned to make those determinations. We recommend that any legislation ultimately promulgated by Ofcom recognise that there are some edge cases where some discretion may be appropriate. User access to services (U2U) \\t\\tBeyond proactive technologies, Snap recommend that any further guidelines by Ofcom on this subject are limited to scanning for known CSAM at this time (as defined as CSAM that resides in hash databases), as opposed to first-generation or novel CSAM\\t\"Snap and other platforms utilise hash matching technology, specifically PhotoDNA and Google CSAI, to proactively scan for CSAM. These two tools are only able to scan for known CSAM (as opposed to first generation/novel CSAM), via the Internet Watch Foundation’s database and the hashes are made available to various companies for this particular purpose. Therefore, when a user posts or shares an image that is picked up by our proactive automated screening system and flagged as CSAM, this means that the user’s image contains hashes (which are like a photo or video’s unique DNA or fingerprints) that are identical to those in the known CSAM database. The technology is well-established and continuously improving - we are confident that the risk of error here is low. For example, in 2023 we proactively detected and actioned 98% of the total child sexual exploitation and abuse violations reported, which is a 4% increase from the previous period - this reflects improvements in the technologies we use. In order to keep the errors low and to avoid situations where lawful content is incorrectly flagged as CSAM by proactive technology, we recommend that any further guidelines by Ofcom on this subject\\nare limited to scanning for known CSAM at this time (as defined as CSAM that resides in hash\\ndatabases), as opposed to first-generation or novel CSAM. Due to strict U.S.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.9902819395065308,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'federal regulations on sharing and distributing CSAM materials, service providers are still exploring viable tech solutions to engage in accurate and proactive screening for first-generation CSAM which also do not infringe the privacy rights of the user. Generally speaking, the vast majority of users who share CSAM on Snapchat are permanently blocked and reported to the National Center for Missing and Exploited Children (NCMEC) in line with US law. However, we agree with Ofcom that there may be times where a user is blocked for sharing something that is flagged as CSAM, but where a permanent block is not the most appropriate option. In these cases, context often matters and an understanding that CSAM can take various forms. We would welcome a reflection by Ofcom on edge cases and providing platforms with some discretion to\\nwork within as part of the final guidance.\"\\nGovernance and accountability  \\t\\t\"Snap would like the codes to clarify who it would envisage would be the person accountable to the governance body for compliance for the purposes of Section 3B and the staff who make decisions\\nfor the purposes of Section 3C. \"\\t\"Snap generally agreed with governance proposals laid out in Chapter 8, Volume 3: Sections 3A-3G. However they\\'d like further clarification on who the expected person accountable to the governance body would be. They believe the ‘person accountable to the\\ngovernance body for compliance’ is equivalent to the EU’s Head of Compliance role (and could be performed by the same individual); and (b) the ‘staff who make decisions’ are the most senior Product/Engineering and Operations managers responsible for deciding on and implementing the risk mitigation measures. They\\'d like to know if this understanding is correct. \"\\nGovernance and accountability  \\t\\tSnap don\\'t consider it appropriate to limit governance and accountability measures to large and multi-risk services. \"Snap argue that all U2U services should face the same gov and accountability measures, in accordance with their risk profile. They say that number of users (particularly mentioning 7 million as a large threshold), should not be the qualifying criteria. They quote evidence from the General Risk Factors (Annex 5): - \\'Low capacity or early-stage services may increase the likelihood of different illegal harms as they may have limited technical skills and financial resources to introduce effective risk management.\\'\\n-  \\'A fast-growing user base may negatively affect effective risk management, given the\\nincreased scale and sophistication of the moderation technologies and processes\\nrequired to keep track of a fast-growing user base (particularly since the sources of risk\\ncan change quickly as the user base develops).\\' They state that these risk factors particularly arise with small services, and that the regulatory burden created by the OSA could create a competitive advantage to companies that already have sizable user bases before having to put in place equivalent gov and accountability measures as a \\'large service\\'. \"\\nGovernance and accountability  \\tCosts of audits conducted by third-parties.': [{'label': 'NEGATIVE',\n",
       "               'score': 0.6623722314834595,\n",
       "               'class_name': 'governance and accountability'}],\n",
       "             'The cost of such a measure is very high. All VLOPs are subject to external audit under the DSA. They do not know yet the impact on efficacy and risks as the audits are just beginning, but the measure is very expensive. Snap have estimated that the DSA audit will cost ~2M in external audit fees and another 1.5M in internal fees ($3.5M). Snap argue the preparation for such an audit takes considerable time and resource away from frontline teams (T&S Ops and Law Enforcement Ops) which would be \\'better spent\\' on their primary functions. Governance and accountability  \\tRemuneration for senior managers for positive online safety outcomes. No first-hand evidence but note this has has been used in financial and other sectors. They have no first-hand evidence to provide on the efficacy, costs, and risks associated with explicitly tieing remuneration for senior managers to positive online safety outcomes. They note companies in other sectors have found this very hard to implement - https://legalbriefs.deloitte.com/post/102ie00/compliance-sanctions-and-adjustments-to-executive-compensation Snap note that such a measure could drastically impact collaboration, transparency and employee mental health. They cite a recent documentary which highlighted how UK water company executives are incentivised not to report sewage discharge issues for fear of damaging their remuneration - https://www.thetimes.co.uk/article/joe-lycett-how-i-tackled-britains-poo-problem-with-toilet-humour-llfmc2tx8 \\nGovernance and accountability  \\tRemuneration for senior managers for positive online safety outcomes. There are multiple ways that the measure could be abused. \"Snap asked that Ofcom consider how this measure might be abused. For example, when users complain about enforcement decisions, companies spend a lot of time reviewing the matter and checking that they have taken the right enforcement action. In some cases, even though it is clear the right enforcement action has been taken and this has been communicated, users can still feel very aggrieved despite being in the wrong. In a small number of these situations, this has led to users becoming bad actors who spend a lot of time looking for ways to ‘punish’ the company. In situations like this, a measure to tie remuneration could encourage abuse. It could create a perverse incentive for bad actors to try to generate negative online safety outcomes (for example, by flooding platforms with harmful content) because they know that this will directly impact the remuneration of the company managers responsible for correct decisions\\ntaken against them. Additionally, this measure could negatively impact privacy, freedom of information and expression as platforms will be incentivised to only allow the most plain or \\'bland\\' content on their platform to ensure positive OS outcomes. They argue that as many employees working on safety measures in U2U services are shareholders, they already have an incentive to keep the platform safe. \"\\nRoR\\t\\tWelcomes ofcosm approach to the ensuring the saftey of sex workers online\\tSupports Ofcoms wide use of litrature to regognise the distinction between sex work and sexual exploitation to ensure that measures do not inavertantly force sex workers into more harmful situations thus incresing the risk of exual exploiation. There is a need for Ofcom to engage with sex workers abut the implications of the OSA. Senior poice publicly noting that preventing sex workers being able to advertise onlone may push perpetrators to more closed spaces making it harder for law enforcement to detect the harm. Approach  to codes\\t\\tConcern Ofcom has set \\'the bar to low\\' for detecting harm and the expectation show be higher for Adult Service websites\\tThey support the use of the STIM but alongside robust and specialist human moderation and partnerships with NGOs to relect current sex worker practice and risk. ICJG \\t\\tConcern Ofcom has set \\'the bar to low\\' for detecting harm and the expectation show be higher for Adult Service websites\\tASW should be expected to take more of a proactive apprach to detecting harm that goes further than the current ICJG threshold. The use of \\'warning signs\\'  (like the STIM) unlikly to reach resonable grounds to inform that the threshold has been met unless a third party (law enforceent) has additional infomration. Online Safety Enforcement Guidance\\t\\tConcerns over the speed and efficiancy of Ofcoms business disruption measures. Moving perpetrators to less safeplatforms\\tThe nature of the ASW sector means that privacey and confidentiality are important to sex workers due to privacy risks and issues with stigma. If measures come into effect around AV viva street are concernd that through theire compliance with the codes it may drive sex workers to uses other services that are not complicit and in turn not as safe having the effec of increasing the harm. ': [{'label': 'POSITIVE',\n",
       "               'score': 0.9970593452453613,\n",
       "               'class_name': 'governance and accountability'}]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Put tagged chunks into jason file and store in tagged_paragraph folder\n",
    "tagged_output = tagged_par_folder + \"/overview_gov_class.json\"\n",
    "with open(tagged_output, \"w\") as tagged_par: \n",
    "    json.dump(res, tagged_par, indent = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. pull pdf\n",
    "pdf_pages = etl.get_pdf()\n",
    "# 2. split PDF into chunks\n",
    "splitter, chunks = etl.split_pdf(pdf_pages)\n",
    "# 3. Put chunks into json file and store in empty_paragraph folder\n",
    "dict_chunks = defaultdict(list)\n",
    "for par in chunks:\n",
    "    dict_chunks[par].append('')\n",
    "outfile = empty_par_folder + \"/overview_gov_class.json\"\n",
    "with open(outfile, \"w\") as outfile: \n",
    "    json.dump(dict_chunks, outfile, indent = 4)\n",
    "\n",
    "\n",
    "# 4. Get chunks from json file\n",
    "with open(outfile) as json_file:\n",
    "    strored_chunk = json.load(json_file)\n",
    "# 5. Tag chunk using classifier\n",
    "res = etl.tag_pdf(strored_chunk, topic=\"governance and accountability\")\n",
    "\n",
    "# 6. Put tagged chunks into jason file and store in tagged_paragraph folder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
